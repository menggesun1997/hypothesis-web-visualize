[
  {
    "paperId": "pub.1188503821",
    "doi": "10.1016/j.patter.2025.101260",
    "title": "Unleashing the potential of prompt engineering for large language models",
    "year": 2025,
    "citationCount": 34,
    "fieldCitationRatio": NaN,
    "abstract": "This review explores the role of prompt engineering in unleashing the capabilities of large language models (LLMs). Prompt engineering is the process of structuring inputs, and it has emerged as a crucial technique for maximizing the utility and accuracy of these models. Both foundational and advanced prompt engineering methodologies-including techniques such as self-consistency, chain of thought, and generated knowledge, which can significantly enhance the performance of models-are explored in this paper. Additionally, the prompt methods for vision language models (VLMs) are examined in detail. Prompt methods are evaluated with subjective and objective metrics, ensuring a robust analysis of their efficacy. Critical to this discussion is the role of prompt engineering in artificial intelligence (AI) security, particularly in terms of defending against adversarial attacks that exploit vulnerabilities in LLMs. Strategies for minimizing these risks and improving the robustness of models are thoroughly reviewed. Finally, we provide a perspective for future research and applications.",
    "reference_ids": [
      "pub.1173611067",
      "pub.1138840449",
      "pub.1163453782",
      "pub.1138840329",
      "pub.1184547889",
      "pub.1129756760",
      "pub.1163042759",
      "pub.1148391035",
      "pub.1163043523",
      "pub.1099239594",
      "pub.1164953222",
      "pub.1122290238",
      "pub.1091745067",
      "pub.1155707017",
      "pub.1159923121",
      "pub.1167704269",
      "pub.1119011509",
      "pub.1110957800",
      "pub.1169012331",
      "pub.1151003027",
      "pub.1166873105",
      "pub.1141488106",
      "pub.1186383719",
      "pub.1129756745",
      "pub.1158341488",
      "pub.1163041741",
      "pub.1158361897",
      "pub.1042471266",
      "pub.1163202410",
      "pub.1164652827",
      "pub.1163045629",
      "pub.1106836591",
      "pub.1166872523",
      "pub.1156284911",
      "pub.1119942002",
      "pub.1163045258",
      "pub.1166872601",
      "pub.1173668048",
      "pub.1181300513",
      "pub.1095689025",
      "pub.1118248807",
      "pub.1133175312",
      "pub.1158239195",
      "pub.1171209008",
      "pub.1166872963",
      "pub.1129757334",
      "pub.1160816829",
      "pub.1167214836",
      "pub.1174225436",
      "pub.1166872813",
      "pub.1169524205",
      "pub.1086020012",
      "pub.1142776495",
      "pub.1162828780",
      "pub.1144759870",
      "pub.1159713539",
      "pub.1099151247",
      "pub.1168149665",
      "pub.1134089846",
      "pub.1166874040",
      "pub.1174224985",
      "pub.1168623653",
      "pub.1181529909",
      "pub.1165740721",
      "pub.1168164334",
      "pub.1153159015",
      "pub.1181138665",
      "pub.1138840268",
      "pub.1148390689",
      "pub.1160112177",
      "pub.1162746839",
      "pub.1167121017",
      "pub.1139947391",
      "pub.1146213159",
      "pub.1164602558",
      "pub.1151542260",
      "pub.1174344528",
      "pub.1094434189",
      "pub.1166873008",
      "pub.1148392045",
      "pub.1166873409",
      "pub.1118846385",
      "pub.1182811106",
      "pub.1166874514",
      "pub.1181182965",
      "pub.1100517312",
      "pub.1166873572",
      "pub.1146529147",
      "pub.1143968638",
      "pub.1163041822",
      "pub.1167120181",
      "pub.1163202465",
      "pub.1153160608",
      "pub.1117659479",
      "pub.1173426407",
      "pub.1146568087",
      "pub.1100516895",
      "pub.1157451981",
      "pub.1142230135",
      "pub.1152843639",
      "pub.1166874234",
      "pub.1166326084",
      "pub.1174225012",
      "pub.1174225358",
      "pub.1069992014",
      "pub.1122836184",
      "pub.1166874093",
      "pub.1169306136",
      "pub.1119019253",
      "pub.1163042387",
      "pub.1168185210",
      "pub.1144345294",
      "pub.1004486097",
      "pub.1152457948",
      "pub.1181783493",
      "pub.1117659409",
      "pub.1142776637",
      "pub.1104321164",
      "pub.1166072902",
      "pub.1144813060",
      "pub.1148390668",
      "pub.1158690222",
      "pub.1099221859",
      "pub.1186384457",
      "pub.1162714876",
      "pub.1129912534",
      "pub.1163749577",
      "pub.1119380936",
      "pub.1104321260",
      "pub.1133177007",
      "pub.1144294277",
      "pub.1176108429",
      "pub.1138840503",
      "pub.1103872751",
      "pub.1085197870",
      "pub.1170135934",
      "pub.1137490222",
      "pub.1135710434",
      "pub.1099106180",
      "pub.1156285111",
      "pub.1182671428",
      "pub.1164950133",
      "pub.1091867527",
      "pub.1165107256",
      "pub.1148122507",
      "pub.1168571955",
      "pub.1169200607",
      "pub.1174210971",
      "pub.1163044055",
      "pub.1168589819",
      "pub.1169759590",
      "pub.1148955603",
      "pub.1151381237",
      "pub.1148955808",
      "pub.1146216268",
      "pub.1147575362",
      "pub.1046331338",
      "pub.1106022887",
      "pub.1172768010",
      "pub.1093185199",
      "pub.1117658905",
      "pub.1164085450",
      "pub.1149741329",
      "pub.1160112082",
      "pub.1167961737",
      "pub.1157850927",
      "pub.1137255428",
      "pub.1147270439",
      "pub.1148491215",
      "pub.1164165159",
      "pub.1167352304",
      "pub.1169950705",
      "pub.1174224873",
      "pub.1163043571",
      "pub.1172502541",
      "pub.1133174396",
      "pub.1158341539",
      "pub.1170033790",
      "pub.1144245142",
      "pub.1107278872",
      "pub.1137805304",
      "pub.1149741578",
      "pub.1105386676",
      "pub.1132407464",
      "pub.1158645207",
      "pub.1137338045",
      "pub.1133177134",
      "pub.1166873181",
      "pub.1186384176",
      "pub.1166873914",
      "pub.1129757067",
      "pub.1157651417",
      "pub.1182672043",
      "pub.1149883714",
      "pub.1140364067",
      "pub.1160562348",
      "pub.1166166707",
      "pub.1186383975",
      "pub.1152455541",
      "pub.1035636783",
      "pub.1129757433",
      "pub.1121050330",
      "pub.1167748832",
      "pub.1001058137",
      "pub.1168833282",
      "pub.1182812080",
      "pub.1174211627",
      "pub.1105443367",
      "pub.1136998164",
      "pub.1079258573",
      "pub.1135720228",
      "pub.1159923650",
      "pub.1152818767",
      "pub.1155419910",
      "pub.1133177071",
      "pub.1155675189",
      "pub.1139284689",
      "pub.1169887899",
      "pub.1148391651",
      "pub.1120590150",
      "pub.1118547396",
      "pub.1166872814",
      "pub.1163043683",
      "pub.1119017753",
      "pub.1158079085",
      "pub.1163544078",
      "pub.1182813830",
      "pub.1156443066",
      "pub.1123821190",
      "pub.1158341232",
      "pub.1166873752",
      "pub.1169030983",
      "pub.1132949057",
      "pub.1148390813",
      "pub.1148391071",
      "pub.1139748253",
      "pub.1172421242",
      "pub.1061548895",
      "pub.1099106113",
      "pub.1100516773",
      "pub.1144244999",
      "pub.1182671453",
      "pub.1099110501",
      "pub.1160816154",
      "pub.1182277194",
      "pub.1117658962"
    ],
    "concepts_scores": [
      {
        "concept": "vision-language models",
        "relevance": 0.708
      },
      {
        "concept": "language model",
        "relevance": 0.679
      },
      {
        "concept": "artificial intelligence",
        "relevance": 0.628
      },
      {
        "concept": "adversarial attacks",
        "relevance": 0.567
      },
      {
        "concept": "robustness of models",
        "relevance": 0.563
      },
      {
        "concept": "objective metrics",
        "relevance": 0.548
      },
      {
        "concept": "modelsâ€”are",
        "relevance": 0.445
      },
      {
        "concept": "language",
        "relevance": 0.423
      },
      {
        "concept": "prompt method",
        "relevance": 0.421
      },
      {
        "concept": "security",
        "relevance": 0.417
      },
      {
        "concept": "engineering",
        "relevance": 0.411
      },
      {
        "concept": "intelligence",
        "relevance": 0.409
      },
      {
        "concept": "robustness analysis",
        "relevance": 0.408
      },
      {
        "concept": "attacks",
        "relevance": 0.403
      },
      {
        "concept": "metrics",
        "relevance": 0.402
      },
      {
        "concept": "robustness",
        "relevance": 0.394
      },
      {
        "concept": "accuracy",
        "relevance": 0.379
      },
      {
        "concept": "model",
        "relevance": 0.378
      },
      {
        "concept": "technique",
        "relevance": 0.378
      },
      {
        "concept": "LLM",
        "relevance": 0.375
      },
      {
        "concept": "vision",
        "relevance": 0.375
      },
      {
        "concept": "method",
        "relevance": 0.372
      },
      {
        "concept": "input",
        "relevance": 0.37
      },
      {
        "concept": "capability",
        "relevance": 0.368
      },
      {
        "concept": "performance",
        "relevance": 0.365
      },
      {
        "concept": "applications",
        "relevance": 0.344
      },
      {
        "concept": "vulnerability",
        "relevance": 0.333
      },
      {
        "concept": "knowledge",
        "relevance": 0.319
      },
      {
        "concept": "utilization",
        "relevance": 0.3
      },
      {
        "concept": "research",
        "relevance": 0.296
      },
      {
        "concept": "process",
        "relevance": 0.288
      },
      {
        "concept": "perspective",
        "relevance": 0.273
      },
      {
        "concept": "self-consistently",
        "relevance": 0.248
      },
      {
        "concept": "chain",
        "relevance": 0.242
      },
      {
        "concept": "discussion",
        "relevance": 0.237
      },
      {
        "concept": "analysis",
        "relevance": 0.23
      },
      {
        "concept": "potential",
        "relevance": 0.2
      },
      {
        "concept": "review",
        "relevance": 0.184
      },
      {
        "concept": "risk",
        "relevance": 0.15
      },
      {
        "concept": "efficacy",
        "relevance": 0.149
      }
    ]
  },
  {
    "paperId": "pub.1165266909",
    "doi": "10.1109/tvcg.2023.3327153",
    "title": ": Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models",
    "year": 2023,
    "citationCount": 9,
    "fieldCitationRatio": NaN,
    "abstract": "Recently, large pretrained language models have achieved compelling performance on commonsense benchmarks. Nevertheless, it is unclear what commonsense knowledge the models learn and whether they solely exploit spurious patterns. Feature attributions are popular explainability techniques that identify important input concepts for model outputs. However, commonsense knowledge tends to be implicit and rarely explicitly presented in inputs. These methods cannot infer models' implicit reasoning over mentioned concepts. We present CommonsenseVIS, a visual explanatory system that utilizes external commonsense knowledge bases to contextualize model behavior for commonsense question-answering. Specifically, we extract relevant commonsense knowledge in inputs as references to align model behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations. Through a user study, we show that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of models' relational reasoning over concepts in different situations.",
    "reference_ids": [
      "pub.1141494094",
      "pub.1036565846",
      "pub.1123669031",
      "pub.1148652457",
      "pub.1038436955",
      "pub.1122290471",
      "pub.1122290260",
      "pub.1129120011",
      "pub.1133176775",
      "pub.1128567840",
      "pub.1107064439",
      "pub.1091437722",
      "pub.1133177131",
      "pub.1139947973",
      "pub.1150865981",
      "pub.1150867042",
      "pub.1156629825",
      "pub.1138840549",
      "pub.1133175114",
      "pub.1110893725",
      "pub.1121025064",
      "pub.1122290267",
      "pub.1129757101",
      "pub.1148955808",
      "pub.1154466106",
      "pub.1030320684",
      "pub.1118973948",
      "pub.1143949294",
      "pub.1121025079",
      "pub.1136047839",
      "pub.1139947580",
      "pub.1140714420",
      "pub.1128856545",
      "pub.1139948370",
      "pub.1163042396",
      "pub.1138840642",
      "pub.1133174700",
      "pub.1148814287",
      "pub.1133177134",
      "pub.1150865984",
      "pub.1133176860",
      "pub.1120932880",
      "pub.1133176846",
      "pub.1147478217",
      "pub.1120516167",
      "pub.1139948352",
      "pub.1121025044",
      "pub.1148391063",
      "pub.1149514000",
      "pub.1122290299",
      "pub.1154888254",
      "pub.1145893204",
      "pub.1145281753",
      "pub.1129756633",
      "pub.1124608030",
      "pub.1148390668",
      "pub.1122290687"
    ],
    "concepts_scores": [
      {
        "concept": "commonsense knowledge",
        "relevance": 0.78
      },
      {
        "concept": "language model",
        "relevance": 0.742
      },
      {
        "concept": "commonsense knowledge bases",
        "relevance": 0.703
      },
      {
        "concept": "relevant commonsense knowledge",
        "relevance": 0.703
      },
      {
        "concept": "multi-level visualization",
        "relevance": 0.699
      },
      {
        "concept": "natural language model",
        "relevance": 0.691
      },
      {
        "concept": "NLP experts",
        "relevance": 0.651
      },
      {
        "concept": "user study",
        "relevance": 0.648
      },
      {
        "concept": "question-answering",
        "relevance": 0.637
      },
      {
        "concept": "knowledge bases",
        "relevance": 0.636
      },
      {
        "concept": "explainability techniques",
        "relevance": 0.634
      },
      {
        "concept": "feature attributes",
        "relevance": 0.625
      },
      {
        "concept": "input concepts",
        "relevance": 0.62
      },
      {
        "concept": "model behavior",
        "relevance": 0.595
      },
      {
        "concept": "human knowledge",
        "relevance": 0.586
      },
      {
        "concept": "relational reasoning",
        "relevance": 0.578
      },
      {
        "concept": "commonsense",
        "relevance": 0.562
      },
      {
        "concept": "implicit reasoning",
        "relevance": 0.553
      },
      {
        "concept": "input",
        "relevance": 0.517
      },
      {
        "concept": "visualization",
        "relevance": 0.509
      },
      {
        "concept": "NLP",
        "relevance": 0.495
      },
      {
        "concept": "explainability",
        "relevance": 0.49
      },
      {
        "concept": "users",
        "relevance": 0.489
      },
      {
        "concept": "benchmarks",
        "relevance": 0.465
      },
      {
        "concept": "concept",
        "relevance": 0.453
      },
      {
        "concept": "knowledge",
        "relevance": 0.452
      },
      {
        "concept": "system",
        "relevance": 0.441
      },
      {
        "concept": "model",
        "relevance": 0.435
      },
      {
        "concept": "model output",
        "relevance": 0.42
      },
      {
        "concept": "language",
        "relevance": 0.419
      },
      {
        "concept": "performance",
        "relevance": 0.419
      },
      {
        "concept": "experts",
        "relevance": 0.417
      },
      {
        "concept": "reasons",
        "relevance": 0.415
      },
      {
        "concept": "attributes",
        "relevance": 0.405
      },
      {
        "concept": "features",
        "relevance": 0.403
      },
      {
        "concept": "implicit",
        "relevance": 0.402
      },
      {
        "concept": "output",
        "relevance": 0.399
      },
      {
        "concept": "technique",
        "relevance": 0.374
      },
      {
        "concept": "editing",
        "relevance": 0.37
      },
      {
        "concept": "method",
        "relevance": 0.368
      },
      {
        "concept": "situation",
        "relevance": 0.34
      },
      {
        "concept": "behavior",
        "relevance": 0.315
      },
      {
        "concept": "reference",
        "relevance": 0.308
      },
      {
        "concept": "patterns",
        "relevance": 0.286
      },
      {
        "concept": "explanatory system",
        "relevance": 0.286
      },
      {
        "concept": "relations",
        "relevance": 0.285
      },
      {
        "concept": "basis",
        "relevance": 0.261
      },
      {
        "concept": "model probe",
        "relevance": 0.24
      },
      {
        "concept": "study",
        "relevance": 0.19
      },
      {
        "concept": "probe",
        "relevance": 0.17
      }
    ]
  },
  {
    "paperId": "pub.1143813821",
    "doi": "10.1016/j.isci.2021.103581",
    "title": "CX-ToM: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models",
    "year": 2021,
    "citationCount": 42,
    "fieldCitationRatio": 13.3,
    "abstract": "We propose <i>CX-ToM</i>, short for counterfactual explanations with theory-of-mind, a new explainable AI (XAI) framework for explaining decisions made by a deep convolutional neural network (CNN). In contrast to the current methods in XAI that generate explanations as a single shot response, we pose explanation as an iterative communication process, i.e., dialogue between the machine and human user. More concretely, our CX-ToM framework generates a sequence of explanations in a dialogue by mediating the differences between the minds of the machine and human user. To do this, we use Theory of Mind (ToM) which helps us in explicitly modeling the human's intention, the machine's mind as inferred by the human, as well as human's mind as inferred by the machine. Moreover, most state-of-the-art XAI frameworks provide attention (or heat map) based explanations. In our work, we show that these attention-based explanations are not sufficient for increasing human trust in the underlying CNN model. In CX-ToM, we instead use counterfactual explanations called <i>fault-lines</i> which we define as follows: given an input image <i>I</i> for which a CNN classification model <i>M</i> predicts class <i>c</i> <sub><i>pred</i></sub> , a fault-line identifies the minimal semantic-level features (e.g., <i>stripes</i> on zebra), referred to as explainable concepts, that need to be added to or deleted from <i>I</i> to alter the classification category of <i>I</i> by <i>M</i> to another specified class <i>c</i> <sub><i>alt</i></sub> . Extensive experiments verify our hypotheses, demonstrating that our CX-ToM significantly outperforms the state-of-the-art XAI models.",
    "reference_ids": [
      "pub.1034441287",
      "pub.1061406210",
      "pub.1038257330",
      "pub.1143948896",
      "pub.1042661602",
      "pub.1031419499",
      "pub.1038436955",
      "pub.1038140272",
      "pub.1084847747",
      "pub.1148916310",
      "pub.1095108577",
      "pub.1100060361",
      "pub.1073406755",
      "pub.1094631222",
      "pub.1092723101",
      "pub.1085641906",
      "pub.1053740354",
      "pub.1127481136",
      "pub.1085257894",
      "pub.1047909927",
      "pub.1094986107",
      "pub.1009767488",
      "pub.1002371134",
      "pub.1108128499",
      "pub.1121025776",
      "pub.1042719664",
      "pub.1092349655",
      "pub.1042382244",
      "pub.1015416220",
      "pub.1046455188",
      "pub.1092081307",
      "pub.1148956109",
      "pub.1098767990",
      "pub.1084228380",
      "pub.1091133194",
      "pub.1099341205",
      "pub.1137841620",
      "pub.1101554124",
      "pub.1095839840",
      "pub.1143949287",
      "pub.1024156580",
      "pub.1090908833",
      "pub.1033178586",
      "pub.1032233097",
      "pub.1091274319",
      "pub.1030888849",
      "pub.1062854487",
      "pub.1093403985",
      "pub.1022356842",
      "pub.1000456826",
      "pub.1110721054",
      "pub.1094071039",
      "pub.1098652906",
      "pub.1091437722",
      "pub.1095686866",
      "pub.1129757203",
      "pub.1093359587",
      "pub.1125998694",
      "pub.1095837693",
      "pub.1043710268",
      "pub.1035331085",
      "pub.1107865769",
      "pub.1117658929",
      "pub.1125080320",
      "pub.1092081310",
      "pub.1008148086",
      "pub.1093270996",
      "pub.1127359543",
      "pub.1128855996",
      "pub.1107372673",
      "pub.1002087643",
      "pub.1123502849",
      "pub.1094796378",
      "pub.1100060663",
      "pub.1030645893",
      "pub.1068000466",
      "pub.1143949655",
      "pub.1093862204",
      "pub.1054867295",
      "pub.1121715269"
    ],
    "concepts_scores": [
      {
        "concept": "convolutional neural network",
        "relevance": 0.827
      },
      {
        "concept": "state-of-the-art",
        "relevance": 0.779
      },
      {
        "concept": "counterfactual explanations",
        "relevance": 0.755
      },
      {
        "concept": "human users",
        "relevance": 0.721
      },
      {
        "concept": "human trust",
        "relevance": 0.712
      },
      {
        "concept": "deep convolutional neural network",
        "relevance": 0.686
      },
      {
        "concept": "convolutional neural network model",
        "relevance": 0.681
      },
      {
        "concept": "increasing human trust",
        "relevance": 0.679
      },
      {
        "concept": "semantic-level features",
        "relevance": 0.675
      },
      {
        "concept": "image recognition model",
        "relevance": 0.667
      },
      {
        "concept": "iterative communication process",
        "relevance": 0.665
      },
      {
        "concept": "XAI framework",
        "relevance": 0.62
      },
      {
        "concept": "recognition model",
        "relevance": 0.615
      },
      {
        "concept": "XAI models",
        "relevance": 0.612
      },
      {
        "concept": "neural network",
        "relevance": 0.609
      },
      {
        "concept": "human intention",
        "relevance": 0.605
      },
      {
        "concept": "machine mind",
        "relevance": 0.582
      },
      {
        "concept": "XAI",
        "relevance": 0.563
      },
      {
        "concept": "machine",
        "relevance": 0.548
      },
      {
        "concept": "users",
        "relevance": 0.547
      },
      {
        "concept": "communication process",
        "relevance": 0.51
      },
      {
        "concept": "classification categories",
        "relevance": 0.509
      },
      {
        "concept": "framework",
        "relevance": 0.494
      },
      {
        "concept": "trust",
        "relevance": 0.467
      },
      {
        "concept": "network",
        "relevance": 0.453
      },
      {
        "concept": "theory-of-mind",
        "relevance": 0.437
      },
      {
        "concept": "AI",
        "relevance": 0.437
      },
      {
        "concept": "classification",
        "relevance": 0.437
      },
      {
        "concept": "human mind",
        "relevance": 0.434
      },
      {
        "concept": "input",
        "relevance": 0.41
      },
      {
        "concept": "model",
        "relevance": 0.409
      },
      {
        "concept": "images",
        "relevance": 0.401
      },
      {
        "concept": "features",
        "relevance": 0.389
      },
      {
        "concept": "decision",
        "relevance": 0.388
      },
      {
        "concept": "method",
        "relevance": 0.356
      },
      {
        "concept": "concept",
        "relevance": 0.354
      },
      {
        "concept": "experiments",
        "relevance": 0.338
      },
      {
        "concept": "intention",
        "relevance": 0.337
      },
      {
        "concept": "i.",
        "relevance": 0.332
      },
      {
        "concept": "attention",
        "relevance": 0.322
      },
      {
        "concept": "process",
        "relevance": 0.319
      },
      {
        "concept": "categories",
        "relevance": 0.311
      },
      {
        "concept": "dialogue",
        "relevance": 0.283
      },
      {
        "concept": "humans",
        "relevance": 0.274
      },
      {
        "concept": "sequence",
        "relevance": 0.271
      },
      {
        "concept": "theory",
        "relevance": 0.27
      },
      {
        "concept": "fault-lines",
        "relevance": 0.267
      },
      {
        "concept": "explanation",
        "relevance": 0.261
      },
      {
        "concept": "mindfulness",
        "relevance": 0.257
      },
      {
        "concept": "ToM",
        "relevance": 0.21
      },
      {
        "concept": "hypothesis",
        "relevance": 0.174
      },
      {
        "concept": "differences",
        "relevance": 0.141
      }
    ]
  },
  {
    "paperId": "pub.1169759586",
    "doi": "10.48550/arxiv.2403.08946",
    "title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era",
    "year": 2024,
    "citationCount": 5,
    "fieldCitationRatio": NaN,
    "abstract": "Explainable AI (XAI) refers to techniques that provide human-understandable\ninsights into the workings of AI models. Recently, the focus of XAI is being\nextended toward explaining Large Language Models (LLMs). This extension calls\nfor a significant transformation in the XAI methodologies for two reasons.\nFirst, many existing XAI methods cannot be directly applied to LLMs due to\ntheir complexity and advanced capabilities. Second, as LLMs are increasingly\ndeployed in diverse applications, the role of XAI shifts from merely opening\nthe ``black box'' to actively enhancing the productivity and applicability of\nLLMs in real-world settings. Meanwhile, the conversation and generation\nabilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we\nintroduce Usable XAI in the context of LLMs by analyzing (1) how XAI can\nexplain and improve LLM-based AI systems and (2) how XAI techniques can be\nimproved by using LLMs. We introduce 10 strategies, introducing the key\ntechniques for each and discussing their associated challenges. We also provide\ncase studies to demonstrate how to obtain and leverage explanations. The code\nused in this paper can be found at:\nhttps://github.com/JacksonWuxs/UsableXAI_LLM.",
    "reference_ids": NaN,
    "concepts_scores": [
      {
        "concept": "Explainable AI",
        "relevance": 0.611
      },
      {
        "concept": "XAI methods",
        "relevance": 0.609
      },
      {
        "concept": "language model",
        "relevance": 0.606
      },
      {
        "concept": "XAI techniques",
        "relevance": 0.604
      },
      {
        "concept": "XAI methodology",
        "relevance": 0.6
      },
      {
        "concept": "XAI",
        "relevance": 0.595
      },
      {
        "concept": "AI systems",
        "relevance": 0.593
      },
      {
        "concept": "key techniques",
        "relevance": 0.592
      },
      {
        "concept": "AI models",
        "relevance": 0.585
      },
      {
        "concept": "advanced capabilities",
        "relevance": 0.565
      },
      {
        "concept": "code",
        "relevance": 0.508
      },
      {
        "concept": "diverse applications",
        "relevance": 0.484
      },
      {
        "concept": "explainability",
        "relevance": 0.464
      },
      {
        "concept": "generation ability",
        "relevance": 0.446
      },
      {
        "concept": "LLM",
        "relevance": 0.439
      },
      {
        "concept": "technique",
        "relevance": 0.438
      },
      {
        "concept": "applications",
        "relevance": 0.432
      },
      {
        "concept": "capability",
        "relevance": 0.399
      },
      {
        "concept": "language",
        "relevance": 0.396
      },
      {
        "concept": "model",
        "relevance": 0.381
      },
      {
        "concept": "transformation",
        "relevance": 0.36
      },
      {
        "concept": "system",
        "relevance": 0.36
      },
      {
        "concept": "extension",
        "relevance": 0.356
      },
      {
        "concept": "strategies",
        "relevance": 0.35
      },
      {
        "concept": "method",
        "relevance": 0.348
      },
      {
        "concept": "methodology",
        "relevance": 0.344
      },
      {
        "concept": "reasons",
        "relevance": 0.339
      },
      {
        "concept": "challenges",
        "relevance": 0.339
      },
      {
        "concept": "ability",
        "relevance": 0.332
      },
      {
        "concept": "complex",
        "relevance": 0.33
      },
      {
        "concept": "context",
        "relevance": 0.327
      },
      {
        "concept": "era",
        "relevance": 0.313
      },
      {
        "concept": "generation",
        "relevance": 0.309
      },
      {
        "concept": "leverage",
        "relevance": 0.278
      },
      {
        "concept": "explanation",
        "relevance": 0.204
      },
      {
        "concept": "conversion",
        "relevance": 0.203
      },
      {
        "concept": "shift",
        "relevance": 0.192
      },
      {
        "concept": "production",
        "relevance": 0.186
      }
    ]
  },
  {
    "paperId": "pub.1167839102",
    "doi": "10.1109/tkde.2024.3352100",
    "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "year": 2024,
    "citationCount": 542,
    "fieldCitationRatio": NaN,
    "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
    "reference_ids": [
      "pub.1149741564",
      "pub.1138840598",
      "pub.1128537995",
      "pub.1134455524",
      "pub.1148955808",
      "pub.1122290605",
      "pub.1120460441",
      "pub.1138468582",
      "pub.1139948330",
      "pub.1095476742",
      "pub.1143949435",
      "pub.1162764128",
      "pub.1122291001",
      "pub.1153011719",
      "pub.1169640286",
      "pub.1150867042",
      "pub.1092091880",
      "pub.1120647276",
      "pub.1136702674",
      "pub.1153746454",
      "pub.1121025062",
      "pub.1141164798",
      "pub.1148390642",
      "pub.1163043966",
      "pub.1122290267",
      "pub.1137500968",
      "pub.1004335965",
      "pub.1117658869",
      "pub.1155357697",
      "pub.1163844428",
      "pub.1166874315",
      "pub.1166381395",
      "pub.1133175188",
      "pub.1133174427",
      "pub.1129756968",
      "pub.1153244550",
      "pub.1132275746",
      "pub.1100517046",
      "pub.1105386787",
      "pub.1133176815",
      "pub.1147477705",
      "pub.1149513928",
      "pub.1042802800",
      "pub.1160264295",
      "pub.1149741557",
      "pub.1166872967",
      "pub.1163044979",
      "pub.1083910180",
      "pub.1144245479",
      "pub.1121025190",
      "pub.1163045053",
      "pub.1089970582",
      "pub.1139420820",
      "pub.1137489839",
      "pub.1118169619",
      "pub.1148390745",
      "pub.1148391063",
      "pub.1148917041",
      "pub.1122290299",
      "pub.1152843639",
      "pub.1129119954",
      "pub.1127360241",
      "pub.1163990968",
      "pub.1047438228",
      "pub.1166872902",
      "pub.1139947941",
      "pub.1131074864",
      "pub.1138840642",
      "pub.1104321292",
      "pub.1143949042",
      "pub.1150867046",
      "pub.1169244023",
      "pub.1149214134",
      "pub.1119942011",
      "pub.1170011997",
      "pub.1124455510",
      "pub.1163041724",
      "pub.1133177107",
      "pub.1148390783",
      "pub.1120192474",
      "pub.1158126044",
      "pub.1028565626",
      "pub.1163041878",
      "pub.1121856159",
      "pub.1129120011",
      "pub.1131901523",
      "pub.1149214126",
      "pub.1121024926",
      "pub.1170136103",
      "pub.1002753319",
      "pub.1139948075",
      "pub.1129757011",
      "pub.1133174799",
      "pub.1150972844",
      "pub.1117658880",
      "pub.1137326917",
      "pub.1169640331",
      "pub.1129757334",
      "pub.1131726230",
      "pub.1129483373",
      "pub.1134455402",
      "pub.1163042342",
      "pub.1165082553",
      "pub.1160813262",
      "pub.1103644775",
      "pub.1115581636",
      "pub.1129388289",
      "pub.1148391571",
      "pub.1170643103",
      "pub.1164785899",
      "pub.1166872919",
      "pub.1163679939",
      "pub.1160800694",
      "pub.1149741229",
      "pub.1125558196",
      "pub.1148391555",
      "pub.1143949144",
      "pub.1150281685",
      "pub.1121024731",
      "pub.1112688930",
      "pub.1160714945",
      "pub.1137377483",
      "pub.1163453407",
      "pub.1141168559",
      "pub.1130798515",
      "pub.1120590131",
      "pub.1140364091",
      "pub.1151156015",
      "pub.1133176860",
      "pub.1109352359",
      "pub.1123318116",
      "pub.1120612831",
      "pub.1122290952",
      "pub.1129119692",
      "pub.1118769417",
      "pub.1133174384",
      "pub.1133174463",
      "pub.1138575203"
    ],
    "concepts_scores": [
      {
        "concept": "knowledge graph",
        "relevance": 0.782
      },
      {
        "concept": "language model",
        "relevance": 0.72
      },
      {
        "concept": "field of natural language processing",
        "relevance": 0.7
      },
      {
        "concept": "natural language processing",
        "relevance": 0.669
      },
      {
        "concept": "KG tasks",
        "relevance": 0.632
      },
      {
        "concept": "black-box models",
        "relevance": 0.631
      },
      {
        "concept": "inference phase",
        "relevance": 0.628
      },
      {
        "concept": "pre-training",
        "relevance": 0.609
      },
      {
        "concept": "language processing",
        "relevance": 0.608
      },
      {
        "concept": "artificial intelligence",
        "relevance": 0.603
      },
      {
        "concept": "external knowledge",
        "relevance": 0.594
      },
      {
        "concept": "bidirectional reasoning",
        "relevance": 0.593
      },
      {
        "concept": "research directions",
        "relevance": 0.547
      },
      {
        "concept": "general framework",
        "relevance": 0.545
      },
      {
        "concept": "graph",
        "relevance": 0.515
      },
      {
        "concept": "ChatGPT",
        "relevance": 0.489
      },
      {
        "concept": "Wikipedia",
        "relevance": 0.487
      },
      {
        "concept": "inference",
        "relevance": 0.487
      },
      {
        "concept": "framework",
        "relevance": 0.472
      },
      {
        "concept": "language",
        "relevance": 0.47
      },
      {
        "concept": "embedding",
        "relevance": 0.466
      },
      {
        "concept": "intelligence",
        "relevance": 0.455
      },
      {
        "concept": "LLM",
        "relevance": 0.451
      },
      {
        "concept": "knowledge",
        "relevance": 0.443
      },
      {
        "concept": "task",
        "relevance": 0.443
      },
      {
        "concept": "model",
        "relevance": 0.411
      },
      {
        "concept": "emergency ability",
        "relevance": 0.409
      },
      {
        "concept": "roadmap",
        "relevance": 0.407
      },
      {
        "concept": "huapu",
        "relevance": 0.401
      },
      {
        "concept": "answers",
        "relevance": 0.38
      },
      {
        "concept": "construction",
        "relevance": 0.361
      },
      {
        "concept": "generalizability",
        "relevance": 0.359
      },
      {
        "concept": "method",
        "relevance": 0.357
      },
      {
        "concept": "reasons",
        "relevance": 0.348
      },
      {
        "concept": "research",
        "relevance": 0.33
      },
      {
        "concept": "data",
        "relevance": 0.323
      },
      {
        "concept": "process",
        "relevance": 0.32
      },
      {
        "concept": "generation",
        "relevance": 0.317
      },
      {
        "concept": "completion",
        "relevance": 0.309
      },
      {
        "concept": "direction",
        "relevance": 0.302
      },
      {
        "concept": "ability",
        "relevance": 0.295
      },
      {
        "concept": "field",
        "relevance": 0.292
      },
      {
        "concept": "understanding",
        "relevance": 0.285
      },
      {
        "concept": "interpretation",
        "relevance": 0.276
      },
      {
        "concept": "nature",
        "relevance": 0.263
      },
      {
        "concept": "enhanced understanding",
        "relevance": 0.251
      },
      {
        "concept": "phase",
        "relevance": 0.21
      },
      {
        "concept": "wave",
        "relevance": 0.202
      }
    ]
  },
  {
    "paperId": "pub.1173707867",
    "doi": "10.1145/3626772.3657819",
    "title": "Steering Large Language Models for Cross-lingual Information Retrieval",
    "year": 2024,
    "citationCount": 3,
    "fieldCitationRatio": NaN,
    "abstract": "In today's digital age, accessing information across language barriers poses a significant challenge, with conventional search systems often struggling to interpret and retrieve multilingual content accurately. Addressing this issue, our study introduces a novel integration of applying Large Language Models (LLMs) as Cross-lingual Readers in information retrieval systems, specifically targeting the complexities of cross-lingual information retrieval (CLIR). We present an innovative approach: Activation Steered Multilingual Retrieval (ASMR) that employs \"steering activations''-a method to adjust and direct the LLM's focus-enhancing its ability to understand user queries and generate accurate, language-coherent responses. ASMR adeptly combines a Multilingual Dense Passage Retrieval (mDPR) system with an LLM, overcoming the limitations of traditional search engines in handling diverse linguistic inputs. This approach is particularly effective in managing the nuances and intricacies inherent in various languages. Rigorous testing on established benchmarks such as XOR-TyDi QA, and MKQA demonstrates that ASMR not only meets but surpasses existing standards in CLIR, achieving state-of-the-art performance. The results of our research hold significant implications for understanding the inherent features of how LLMs understand and generate natural languages, offering an attempt towards more inclusive, effective, and linguistically diverse information access on a global scale.",
    "reference_ids": [
      "pub.1136694254",
      "pub.1149741188",
      "pub.1134455520",
      "pub.1144245399",
      "pub.1120096978",
      "pub.1166874287",
      "pub.1174225115",
      "pub.1121025044",
      "pub.1163041720",
      "pub.1186384530",
      "pub.1163044933",
      "pub.1157546777",
      "pub.1129670497",
      "pub.1133177198",
      "pub.1149741509",
      "pub.1167960992",
      "pub.1139621819",
      "pub.1143949272",
      "pub.1139621840",
      "pub.1155645581",
      "pub.1163245717",
      "pub.1143949236",
      "pub.1143224037",
      "pub.1151927530",
      "pub.1149214795",
      "pub.1166872914",
      "pub.1149741477",
      "pub.1129670253",
      "pub.1163044962",
      "pub.1160811767",
      "pub.1163044107",
      "pub.1141073631",
      "pub.1141625582",
      "pub.1160801866",
      "pub.1163043978",
      "pub.1163990927",
      "pub.1146097203"
    ],
    "concepts_scores": [
      {
        "concept": "cross-lingual information retrieval",
        "relevance": 0.759
      },
      {
        "concept": "information retrieval",
        "relevance": 0.681
      },
      {
        "concept": "language model",
        "relevance": 0.675
      },
      {
        "concept": "state-of-the-art performance",
        "relevance": 0.657
      },
      {
        "concept": "dense passage retriever",
        "relevance": 0.639
      },
      {
        "concept": "information retrieval systems",
        "relevance": 0.636
      },
      {
        "concept": "traditional search engines",
        "relevance": 0.636
      },
      {
        "concept": "user queries",
        "relevance": 0.591
      },
      {
        "concept": "multilingual retrieval",
        "relevance": 0.59
      },
      {
        "concept": "passage retrieval",
        "relevance": 0.588
      },
      {
        "concept": "retrieval system",
        "relevance": 0.585
      },
      {
        "concept": "today's digital age",
        "relevance": 0.58
      },
      {
        "concept": "multilingual content",
        "relevance": 0.578
      },
      {
        "concept": "natural language",
        "relevance": 0.573
      },
      {
        "concept": "information access",
        "relevance": 0.568
      },
      {
        "concept": "search engines",
        "relevance": 0.563
      },
      {
        "concept": "retrieval",
        "relevance": 0.528
      },
      {
        "concept": "digital age",
        "relevance": 0.48
      },
      {
        "concept": "language",
        "relevance": 0.471
      },
      {
        "concept": "query",
        "relevance": 0.458
      },
      {
        "concept": "linguistic input",
        "relevance": 0.457
      },
      {
        "concept": "users",
        "relevance": 0.445
      },
      {
        "concept": "information",
        "relevance": 0.444
      },
      {
        "concept": "Rigorous testing",
        "relevance": 0.438
      },
      {
        "concept": "language barriers",
        "relevance": 0.43
      },
      {
        "concept": "benchmarks",
        "relevance": 0.423
      },
      {
        "concept": "LLM",
        "relevance": 0.418
      },
      {
        "concept": "innovative approach",
        "relevance": 0.39
      },
      {
        "concept": "input",
        "relevance": 0.387
      },
      {
        "concept": "performance",
        "relevance": 0.381
      },
      {
        "concept": "access",
        "relevance": 0.378
      },
      {
        "concept": "model",
        "relevance": 0.367
      },
      {
        "concept": "features",
        "relevance": 0.367
      },
      {
        "concept": "multilingualism",
        "relevance": 0.36
      },
      {
        "concept": "linguistics",
        "relevance": 0.36
      },
      {
        "concept": "today",
        "relevance": 0.351
      },
      {
        "concept": "system",
        "relevance": 0.347
      },
      {
        "concept": "engineering",
        "relevance": 0.347
      },
      {
        "concept": "readers",
        "relevance": 0.344
      },
      {
        "concept": "issues",
        "relevance": 0.342
      },
      {
        "concept": "method",
        "relevance": 0.335
      },
      {
        "concept": "integration",
        "relevance": 0.333
      },
      {
        "concept": "nuances",
        "relevance": 0.326
      },
      {
        "concept": "complex",
        "relevance": 0.317
      },
      {
        "concept": "research",
        "relevance": 0.309
      },
      {
        "concept": "standards",
        "relevance": 0.307
      },
      {
        "concept": "results",
        "relevance": 0.292
      },
      {
        "concept": "focus enhancement",
        "relevance": 0.279
      },
      {
        "concept": "limitations",
        "relevance": 0.273
      },
      {
        "concept": "attempt",
        "relevance": 0.27
      },
      {
        "concept": "global scale",
        "relevance": 0.258
      },
      {
        "concept": "content",
        "relevance": 0.244
      },
      {
        "concept": "test",
        "relevance": 0.238
      },
      {
        "concept": "scale",
        "relevance": 0.221
      },
      {
        "concept": "approach",
        "relevance": 0.202
      },
      {
        "concept": "study",
        "relevance": 0.199
      },
      {
        "concept": "age",
        "relevance": 0.17
      },
      {
        "concept": "barriers",
        "relevance": 0.16
      },
      {
        "concept": "activity",
        "relevance": 0.156
      },
      {
        "concept": "response",
        "relevance": 0.156
      }
    ]
  },
  {
    "paperId": "pub.1159861812",
    "doi": "10.48550/arxiv.2306.08302",
    "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "year": 2023,
    "citationCount": 17,
    "fieldCitationRatio": 10.33,
    "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.",
    "reference_ids": NaN,
    "concepts_scores": [
      {
        "concept": "knowledge graph",
        "relevance": 0.782
      },
      {
        "concept": "language model",
        "relevance": 0.721
      },
      {
        "concept": "field of natural language processing",
        "relevance": 0.701
      },
      {
        "concept": "natural language processing",
        "relevance": 0.67
      },
      {
        "concept": "black-box models",
        "relevance": 0.632
      },
      {
        "concept": "KG tasks",
        "relevance": 0.632
      },
      {
        "concept": "inference phase",
        "relevance": 0.628
      },
      {
        "concept": "language processing",
        "relevance": 0.609
      },
      {
        "concept": "pre-training",
        "relevance": 0.609
      },
      {
        "concept": "artificial intelligence",
        "relevance": 0.604
      },
      {
        "concept": "external knowledge",
        "relevance": 0.595
      },
      {
        "concept": "research directions",
        "relevance": 0.547
      },
      {
        "concept": "general framework",
        "relevance": 0.545
      },
      {
        "concept": "graph",
        "relevance": 0.515
      },
      {
        "concept": "ChatGPT",
        "relevance": 0.49
      },
      {
        "concept": "inference",
        "relevance": 0.488
      },
      {
        "concept": "Wikipedia",
        "relevance": 0.487
      },
      {
        "concept": "framework",
        "relevance": 0.473
      },
      {
        "concept": "language",
        "relevance": 0.471
      },
      {
        "concept": "embedding",
        "relevance": 0.466
      },
      {
        "concept": "intelligence",
        "relevance": 0.456
      },
      {
        "concept": "LLM",
        "relevance": 0.451
      },
      {
        "concept": "knowledge",
        "relevance": 0.443
      },
      {
        "concept": "task",
        "relevance": 0.443
      },
      {
        "concept": "model",
        "relevance": 0.418
      },
      {
        "concept": "roadmap",
        "relevance": 0.407
      },
      {
        "concept": "huapu",
        "relevance": 0.401
      },
      {
        "concept": "answers",
        "relevance": 0.38
      },
      {
        "concept": "generalizability",
        "relevance": 0.359
      },
      {
        "concept": "method",
        "relevance": 0.358
      },
      {
        "concept": "research",
        "relevance": 0.33
      },
      {
        "concept": "data",
        "relevance": 0.323
      },
      {
        "concept": "process",
        "relevance": 0.32
      },
      {
        "concept": "generation",
        "relevance": 0.317
      },
      {
        "concept": "construction",
        "relevance": 0.312
      },
      {
        "concept": "completion",
        "relevance": 0.31
      },
      {
        "concept": "direction",
        "relevance": 0.302
      },
      {
        "concept": "ability",
        "relevance": 0.295
      },
      {
        "concept": "field",
        "relevance": 0.292
      },
      {
        "concept": "article",
        "relevance": 0.286
      },
      {
        "concept": "understanding",
        "relevance": 0.285
      },
      {
        "concept": "interpretation",
        "relevance": 0.276
      },
      {
        "concept": "nature",
        "relevance": 0.264
      },
      {
        "concept": "enhanced understanding",
        "relevance": 0.251
      },
      {
        "concept": "phase",
        "relevance": 0.21
      }
    ]
  },
  {
    "paperId": "pub.1174819836",
    "doi": "10.32604/cmc.2024.052618",
    "title": "Evolution and Prospects of Foundation Models: From Large Language Models to Large Multimodal Models",
    "year": 2024,
    "citationCount": 21,
    "fieldCitationRatio": NaN,
    "abstract": "Since the 1950s, when the Turing Test was introduced, there has been notable progress in machine language intelligence. Language modeling, crucial for AI development, has evolved from statistical to neural models over the last two decades. Recently, transformer-based Pre-trained Language Models (PLM) have excelled in Natural Language Processing (NLP) tasks by leveraging large-scale training corpora. Increasing the scale of these models enhances performance significantly, introducing abilities like context learning that smaller models lack. The advancement in Large Language Models, exemplified by the development of ChatGPT, has made significant impacts both academically and industrially, capturing widespread societal interest. This survey provides an overview of the development and prospects from Large Language Models (LLM) to Large Multimodal Models (LMM). It first discusses the contributions and technological advancements of LLMs in the field of natural language processing, especially in text generation and language understanding. Then, it turns to the discussion of LMMs, which integrates various data modalities such as text, images, and sound, demonstrating advanced capabilities in understanding and generating cross-modal content, paving new pathways for the adaptability and flexibility of AI systems. Finally, the survey highlights the prospects of LMMs in terms of technological development and application potential, while also pointing out challenges in data integration, cross-modal understanding accuracy, providing a comprehensive perspective on the latest developments in this field.",
    "reference_ids": [
      "pub.1156602823",
      "pub.1110720726",
      "pub.1143949669",
      "pub.1170273443",
      "pub.1160606682",
      "pub.1167572368",
      "pub.1166872692",
      "pub.1144390589",
      "pub.1166324069",
      "pub.1166872239",
      "pub.1135038688",
      "pub.1182671986",
      "pub.1121024877",
      "pub.1152417736",
      "pub.1166873019",
      "pub.1124552899",
      "pub.1139840091",
      "pub.1163044072",
      "pub.1124815816",
      "pub.1159810749",
      "pub.1115977877",
      "pub.1152675833",
      "pub.1099110758",
      "pub.1160142391",
      "pub.1003682073",
      "pub.1160336928",
      "pub.1163029334",
      "pub.1121331236",
      "pub.1148390526",
      "pub.1122290276",
      "pub.1095851605",
      "pub.1172143333",
      "pub.1156060460",
      "pub.1157198221",
      "pub.1160264295",
      "pub.1164617614",
      "pub.1110957806",
      "pub.1149915839",
      "pub.1182277194",
      "pub.1163045663",
      "pub.1095713388",
      "pub.1045321436",
      "pub.1117659565",
      "pub.1160406490",
      "pub.1158631756",
      "pub.1170804306",
      "pub.1017917055",
      "pub.1167666341",
      "pub.1158010904",
      "pub.1163753091",
      "pub.1156069812",
      "pub.1165313796",
      "pub.1153017743",
      "pub.1168616712",
      "pub.1129757334",
      "pub.1151380124",
      "pub.1132323123",
      "pub.1167053794",
      "pub.1093780738",
      "pub.1157266976",
      "pub.1163045367",
      "pub.1149622280",
      "pub.1145901168",
      "pub.1143949018",
      "pub.1110034687",
      "pub.1042471266",
      "pub.1117659708",
      "pub.1151381211",
      "pub.1174225510",
      "pub.1129912995",
      "pub.1150980194",
      "pub.1166872119",
      "pub.1169564276",
      "pub.1166874426",
      "pub.1166872883",
      "pub.1169306136",
      "pub.1166873237",
      "pub.1166971826",
      "pub.1068001274",
      "pub.1061180209",
      "pub.1150177514",
      "pub.1158547550",
      "pub.1163453782",
      "pub.1044800349",
      "pub.1142369420",
      "pub.1164826588",
      "pub.1099106053",
      "pub.1138871340",
      "pub.1093662010",
      "pub.1181929347",
      "pub.1138337506",
      "pub.1151380637",
      "pub.1163041872",
      "pub.1095689025",
      "pub.1170136121",
      "pub.1132270121",
      "pub.1166873275",
      "pub.1163749188",
      "pub.1169044209",
      "pub.1151380786",
      "pub.1155673755",
      "pub.1166872435",
      "pub.1160732169",
      "pub.1164375237",
      "pub.1185997578",
      "pub.1085642448",
      "pub.1148391290",
      "pub.1163043706",
      "pub.1142214649",
      "pub.1117658887",
      "pub.1166972832",
      "pub.1166873227",
      "pub.1163044349",
      "pub.1083549046",
      "pub.1158361897",
      "pub.1148390689",
      "pub.1009767488",
      "pub.1153500306",
      "pub.1160635088",
      "pub.1164514327",
      "pub.1185030116",
      "pub.1095181999",
      "pub.1100060688",
      "pub.1151380649",
      "pub.1151381197",
      "pub.1123988164",
      "pub.1173246914",
      "pub.1099239469"
    ],
    "concepts_scores": [
      {
        "concept": "natural language processing",
        "relevance": 0.745
      },
      {
        "concept": "pre-trained language models",
        "relevance": 0.735
      },
      {
        "concept": "language model",
        "relevance": 0.709
      },
      {
        "concept": "language processing",
        "relevance": 0.645
      },
      {
        "concept": "field of natural language processing",
        "relevance": 0.641
      },
      {
        "concept": "multimodal model",
        "relevance": 0.587
      },
      {
        "concept": "training corpus",
        "relevance": 0.574
      },
      {
        "concept": "text generation",
        "relevance": 0.567
      },
      {
        "concept": "language understanding",
        "relevance": 0.566
      },
      {
        "concept": "neural model",
        "relevance": 0.561
      },
      {
        "concept": "AI systems",
        "relevance": 0.557
      },
      {
        "concept": "data integration",
        "relevance": 0.553
      },
      {
        "concept": "Turing test",
        "relevance": 0.55
      },
      {
        "concept": "data modalities",
        "relevance": 0.549
      },
      {
        "concept": "AI development",
        "relevance": 0.54
      },
      {
        "concept": "language intelligence",
        "relevance": 0.536
      },
      {
        "concept": "language",
        "relevance": 0.521
      },
      {
        "concept": "small models",
        "relevance": 0.498
      },
      {
        "concept": "data",
        "relevance": 0.468
      },
      {
        "concept": "ChatGPT",
        "relevance": 0.448
      },
      {
        "concept": "technological advances",
        "relevance": 0.433
      },
      {
        "concept": "technological development",
        "relevance": 0.423
      },
      {
        "concept": "intelligence",
        "relevance": 0.417
      },
      {
        "concept": "corpus",
        "relevance": 0.414
      },
      {
        "concept": "text",
        "relevance": 0.414
      },
      {
        "concept": "machine",
        "relevance": 0.408
      },
      {
        "concept": "foundation model",
        "relevance": 0.393
      },
      {
        "concept": "accuracy",
        "relevance": 0.387
      },
      {
        "concept": "model",
        "relevance": 0.386
      },
      {
        "concept": "LLM",
        "relevance": 0.382
      },
      {
        "concept": "societal interests",
        "relevance": 0.376
      },
      {
        "concept": "capability",
        "relevance": 0.375
      },
      {
        "concept": "comprehensive perspective",
        "relevance": 0.374
      },
      {
        "concept": "performance",
        "relevance": 0.372
      },
      {
        "concept": "images",
        "relevance": 0.369
      },
      {
        "concept": "Turing",
        "relevance": 0.362
      },
      {
        "concept": "sound",
        "relevance": 0.362
      },
      {
        "concept": "flexibility",
        "relevance": 0.358
      },
      {
        "concept": "context",
        "relevance": 0.356
      },
      {
        "concept": "discussion",
        "relevance": 0.353
      },
      {
        "concept": "applications",
        "relevance": 0.351
      },
      {
        "concept": "perspective",
        "relevance": 0.347
      },
      {
        "concept": "process",
        "relevance": 0.339
      },
      {
        "concept": "system",
        "relevance": 0.339
      },
      {
        "concept": "understanding",
        "relevance": 0.335
      },
      {
        "concept": "adaptation",
        "relevance": 0.329
      },
      {
        "concept": "integration",
        "relevance": 0.325
      },
      {
        "concept": "interest",
        "relevance": 0.324
      },
      {
        "concept": "application potential",
        "relevance": 0.321
      },
      {
        "concept": "development",
        "relevance": 0.317
      },
      {
        "concept": "overview",
        "relevance": 0.3
      },
      {
        "concept": "nature",
        "relevance": 0.297
      },
      {
        "concept": "survey",
        "relevance": 0.294
      },
      {
        "concept": "decades",
        "relevance": 0.292
      },
      {
        "concept": "LMM",
        "relevance": 0.292
      },
      {
        "concept": "ability",
        "relevance": 0.29
      },
      {
        "concept": "generation",
        "relevance": 0.29
      },
      {
        "concept": "content",
        "relevance": 0.288
      },
      {
        "concept": "field",
        "relevance": 0.268
      },
      {
        "concept": "contribution",
        "relevance": 0.264
      },
      {
        "concept": "modalities",
        "relevance": 0.256
      },
      {
        "concept": "significant impact",
        "relevance": 0.255
      },
      {
        "concept": "evolution",
        "relevance": 0.242
      },
      {
        "concept": "test",
        "relevance": 0.232
      },
      {
        "concept": "prospects",
        "relevance": 0.226
      },
      {
        "concept": "impact",
        "relevance": 0.226
      },
      {
        "concept": "potential",
        "relevance": 0.199
      },
      {
        "concept": "progression",
        "relevance": 0.184
      },
      {
        "concept": "pathway",
        "relevance": 0.179
      }
    ]
  }
]