{
  "papers": [
    {
      "paperId": "pub.1169225310",
      "doi": "10.3390/bioengineering11030219",
      "title": "A Comprehensive Review on Synergy of Multi-Modal Data and AI Technologies in Medical Diagnosis",
      "year": 2024,
      "citationCount": 59,
      "fieldCitationRatio": NaN,
      "abstract": "Disease diagnosis represents a critical and arduous endeavor within the medical field. Artificial intelligence (AI) techniques, spanning from machine learning and deep learning to large model paradigms, stand poised to significantly augment physicians in rendering more evidence-based decisions, thus presenting a pioneering solution for clinical practice. Traditionally, the amalgamation of diverse medical data modalities (e.g., image, text, speech, genetic data, physiological signals) is imperative to facilitate a comprehensive disease analysis, a topic of burgeoning interest among both researchers and clinicians in recent times. Hence, there exists a pressing need to synthesize the latest strides in multi-modal data and AI technologies in the realm of medical diagnosis. In this paper, we narrow our focus to five specific disorders (Alzheimer's disease, breast cancer, depression, heart disease, epilepsy), elucidating advanced endeavors in their diagnosis and treatment through the lens of artificial intelligence. Our survey not only delineates detailed diagnostic methodologies across varying modalities but also underscores commonly utilized public datasets, the intricacies of feature engineering, prevalent classification models, and envisaged challenges for future endeavors. In essence, our research endeavors to contribute to the advancement of diagnostic methodologies, furnishing invaluable insights for clinical decision making.",
      "reference_ids": [
        "pub.1126961929",
        "pub.1043695946",
        "pub.1130399778",
        "pub.1141232130",
        "pub.1153950534",
        "pub.1121050345",
        "pub.1147210691",
        "pub.1112393257",
        "pub.1163485944",
        "pub.1020815383",
        "pub.1113541265",
        "pub.1128590377",
        "pub.1104047828",
        "pub.1005297170",
        "pub.1140322253",
        "pub.1061219233",
        "pub.1099745818",
        "pub.1122780174",
        "pub.1181105167",
        "pub.1107638404",
        "pub.1140492182",
        "pub.1155035437",
        "pub.1079397789",
        "pub.1135880889",
        "pub.1131883141",
        "pub.1079398950",
        "pub.1052153484",
        "pub.1123348532",
        "pub.1085346149",
        "pub.1165294971",
        "pub.1032570273",
        "pub.1095811486",
        "pub.1110857777",
        "pub.1146318431",
        "pub.1064200609",
        "pub.1138844575",
        "pub.1160103012",
        "pub.1124917200",
        "pub.1038140272",
        "pub.1137943749",
        "pub.1157817432",
        "pub.1129821401",
        "pub.1094404483",
        "pub.1123449746",
        "pub.1137715598",
        "pub.1113579026",
        "pub.1033000174",
        "pub.1154740753",
        "pub.1174107909",
        "pub.1103814455",
        "pub.1151332162",
        "pub.1152886822",
        "pub.1148729079",
        "pub.1018797993",
        "pub.1148355954",
        "pub.1107215324",
        "pub.1163645105",
        "pub.1024991403",
        "pub.1126083889",
        "pub.1117031947",
        "pub.1110275505",
        "pub.1151267964",
        "pub.1106431811",
        "pub.1132688052",
        "pub.1120141135",
        "pub.1155477977",
        "pub.1140720334",
        "pub.1145496447",
        "pub.1129811735",
        "pub.1093336180",
        "pub.1155813437",
        "pub.1164502497",
        "pub.1163618331",
        "pub.1148224244",
        "pub.1146301626",
        "pub.1154392516",
        "pub.1135848261",
        "pub.1034114636",
        "pub.1122199776",
        "pub.1106070150",
        "pub.1125900401",
        "pub.1092867125",
        "pub.1140271201",
        "pub.1112445024",
        "pub.1157837749",
        "pub.1153989491",
        "pub.1152004400",
        "pub.1107898723",
        "pub.1144735400",
        "pub.1023540354",
        "pub.1021962820",
        "pub.1124277552",
        "pub.1164005647",
        "pub.1128189753",
        "pub.1106269425",
        "pub.1113713193",
        "pub.1121563623",
        "pub.1156461211",
        "pub.1138043437",
        "pub.1052939783",
        "pub.1146098489",
        "pub.1133252928",
        "pub.1122146079",
        "pub.1138359875",
        "pub.1024163546",
        "pub.1145437736",
        "pub.1014607335",
        "pub.1147312929",
        "pub.1156445645",
        "pub.1127648900",
        "pub.1100479707",
        "pub.1092334727",
        "pub.1121096015",
        "pub.1145824348",
        "pub.1104262505",
        "pub.1029462199",
        "pub.1120329849",
        "pub.1163451091",
        "pub.1122231080",
        "pub.1032050855",
        "pub.1135163861",
        "pub.1143964260",
        "pub.1107514555",
        "pub.1112077146",
        "pub.1135898563",
        "pub.1164038380",
        "pub.1108054472",
        "pub.1141157191",
        "pub.1149272856",
        "pub.1017822839",
        "pub.1124297242",
        "pub.1136265952",
        "pub.1144934002",
        "pub.1091035488",
        "pub.1026934516",
        "pub.1006483704",
        "pub.1144386621",
        "pub.1016704632",
        "pub.1128290157",
        "pub.1140661464",
        "pub.1163919958",
        "pub.1165544296",
        "pub.1164164358",
        "pub.1084746731",
        "pub.1160635088",
        "pub.1159361320",
        "pub.1123468592",
        "pub.1163441028",
        "pub.1106929210",
        "pub.1146159351",
        "pub.1138849323",
        "pub.1130018712",
        "pub.1120878418",
        "pub.1105807105",
        "pub.1009531603",
        "pub.1121025784",
        "pub.1060727666",
        "pub.1137383951",
        "pub.1033632930"
      ],
      "concepts_scores": [
        {
          "concept": "artificial intelligence",
          "relevance": 0.653
        },
        {
          "concept": "AI technology",
          "relevance": 0.622
        },
        {
          "concept": "multi-modal data",
          "relevance": 0.593
        },
        {
          "concept": "multi-modal data",
          "relevance": 0.593
        },
        {
          "concept": "medical diagnosis",
          "relevance": 0.583
        },
        {
          "concept": "public datasets",
          "relevance": 0.552
        },
        {
          "concept": "feature engineering",
          "relevance": 0.552
        },
        {
          "concept": "deep learning",
          "relevance": 0.549
        },
        {
          "concept": "machine learning",
          "relevance": 0.539
        },
        {
          "concept": "classification model",
          "relevance": 0.538
        },
        {
          "concept": "data modalities",
          "relevance": 0.534
        },
        {
          "concept": "modeling paradigm",
          "relevance": 0.512
        },
        {
          "concept": "disease analysis",
          "relevance": 0.485
        },
        {
          "concept": "medical field",
          "relevance": 0.478
        },
        {
          "concept": "intelligence",
          "relevance": 0.47
        },
        {
          "concept": "decision making",
          "relevance": 0.451
        },
        {
          "concept": "disease diagnosis",
          "relevance": 0.449
        },
        {
          "concept": "learning",
          "relevance": 0.449
        },
        {
          "concept": "evidence-based decisions",
          "relevance": 0.425
        },
        {
          "concept": "clinical decision making",
          "relevance": 0.425
        },
        {
          "concept": "technology",
          "relevance": 0.417
        },
        {
          "concept": "dataset",
          "relevance": 0.411
        },
        {
          "concept": "machine",
          "relevance": 0.396
        },
        {
          "concept": "data",
          "relevance": 0.393
        },
        {
          "concept": "classification",
          "relevance": 0.391
        },
        {
          "concept": "clinical practice",
          "relevance": 0.38
        },
        {
          "concept": "research endeavors",
          "relevance": 0.379
        },
        {
          "concept": "diagnostic methodology",
          "relevance": 0.372
        },
        {
          "concept": "methodology",
          "relevance": 0.364
        },
        {
          "concept": "paradigm",
          "relevance": 0.358
        },
        {
          "concept": "diagnosis",
          "relevance": 0.352
        },
        {
          "concept": "model",
          "relevance": 0.348
        },
        {
          "concept": "features",
          "relevance": 0.348
        },
        {
          "concept": "decision",
          "relevance": 0.347
        },
        {
          "concept": "research",
          "relevance": 0.34
        },
        {
          "concept": "making",
          "relevance": 0.339
        },
        {
          "concept": "endeavors",
          "relevance": 0.336
        },
        {
          "concept": "comprehensive review",
          "relevance": 0.33
        },
        {
          "concept": "engineering",
          "relevance": 0.329
        },
        {
          "concept": "modalities",
          "relevance": 0.319
        },
        {
          "concept": "physicians",
          "relevance": 0.318
        },
        {
          "concept": "solution",
          "relevance": 0.31
        },
        {
          "concept": "clinicians",
          "relevance": 0.305
        },
        {
          "concept": "medication",
          "relevance": 0.304
        },
        {
          "concept": "advances",
          "relevance": 0.296
        },
        {
          "concept": "survey",
          "relevance": 0.288
        },
        {
          "concept": "data",
          "relevance": 0.287
        },
        {
          "concept": "disease",
          "relevance": 0.277
        },
        {
          "concept": "practice",
          "relevance": 0.275
        },
        {
          "concept": "treatment",
          "relevance": 0.272
        },
        {
          "concept": "comprehension",
          "relevance": 0.27
        },
        {
          "concept": "disorders",
          "relevance": 0.27
        },
        {
          "concept": "field",
          "relevance": 0.26
        },
        {
          "concept": "review",
          "relevance": 0.256
        },
        {
          "concept": "synergy",
          "relevance": 0.244
        },
        {
          "concept": "amalgam",
          "relevance": 0.23
        },
        {
          "concept": "analysis",
          "relevance": 0.228
        },
        {
          "concept": "lens",
          "relevance": 0.21
        }
      ]
    },
    {
      "paperId": "pub.1160635088",
      "doi": "10.1038/s41586-023-06291-2",
      "title": "Large language models encode clinical knowledge",
      "year": 2023,
      "citationCount": 1807,
      "fieldCitationRatio": 1143.54,
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.",
      "reference_ids": [
        "pub.1131794605",
        "pub.1143948984",
        "pub.1145333867",
        "pub.1147625763",
        "pub.1060343143",
        "pub.1019551002",
        "pub.1169290908",
        "pub.1146478063",
        "pub.1124308279",
        "pub.1148115417",
        "pub.1144859682",
        "pub.1031369500",
        "pub.1122290276",
        "pub.1133174582",
        "pub.1163041639",
        "pub.1140052367",
        "pub.1130673697",
        "pub.1104506335",
        "pub.1127261664",
        "pub.1141942664",
        "pub.1148283402",
        "pub.1121658381",
        "pub.1148150423",
        "pub.1148556704",
        "pub.1144537306",
        "pub.1148116107",
        "pub.1149151305",
        "pub.1120882528",
        "pub.1118803161",
        "pub.1006217895",
        "pub.1118818544",
        "pub.1139947391",
        "pub.1149439682",
        "pub.1140466886",
        "pub.1146892344",
        "pub.1142230135",
        "pub.1137337965",
        "pub.1121015527",
        "pub.1145122135",
        "pub.1149547072",
        "pub.1128562573",
        "pub.1134427256",
        "pub.1129565202",
        "pub.1148187428",
        "pub.1122290595",
        "pub.1099239594",
        "pub.1152022267",
        "pub.1124301819",
        "pub.1137821262",
        "pub.1162970625",
        "pub.1138572951",
        "pub.1139758893",
        "pub.1140910420",
        "pub.1134323565",
        "pub.1143749058",
        "pub.1145942961",
        "pub.1146726542",
        "pub.1119262691",
        "pub.1136509890",
        "pub.1146094553",
        "pub.1111334730",
        "pub.1117658904",
        "pub.1152637972",
        "pub.1143544709",
        "pub.1148728946",
        "pub.1163991073",
        "pub.1144119443",
        "pub.1146909638",
        "pub.1137024208",
        "pub.1141035588",
        "pub.1148815213",
        "pub.1151655603",
        "pub.1149600393",
        "pub.1152983179",
        "pub.1127688687",
        "pub.1151332162",
        "pub.1096025530",
        "pub.1119464119",
        "pub.1084253366",
        "pub.1152818808",
        "pub.1011491073",
        "pub.1146909620",
        "pub.1137740499",
        "pub.1122290388",
        "pub.1142420217",
        "pub.1151003027",
        "pub.1151765911",
        "pub.1152020866",
        "pub.1143336252",
        "pub.1100517312",
        "pub.1117925618",
        "pub.1163045658",
        "pub.1125798967",
        "pub.1139648327",
        "pub.1152818783"
      ],
      "concepts_scores": [
        {
          "concept": "prompt tuning",
          "relevance": 0.716
        },
        {
          "concept": "human evaluation",
          "relevance": 0.708
        },
        {
          "concept": "state-of-the-art accuracy",
          "relevance": 0.69
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.665
        },
        {
          "concept": "evaluation framework",
          "relevance": 0.634
        },
        {
          "concept": "answering dataset",
          "relevance": 0.618
        },
        {
          "concept": "language model",
          "relevance": 0.612
        },
        {
          "concept": "consumer queries",
          "relevance": 0.606
        },
        {
          "concept": "automated evaluation",
          "relevance": 0.573
        },
        {
          "concept": "impressive capability",
          "relevance": 0.555
        },
        {
          "concept": "dataset",
          "relevance": 0.553
        },
        {
          "concept": "knowledge of models",
          "relevance": 0.545
        },
        {
          "concept": "model answers",
          "relevance": 0.53
        },
        {
          "concept": "medical questions",
          "relevance": 0.496
        },
        {
          "concept": "MedQA",
          "relevance": 0.489
        },
        {
          "concept": "accuracy",
          "relevance": 0.481
        },
        {
          "concept": "query",
          "relevance": 0.48
        },
        {
          "concept": "clinical knowledge",
          "relevance": 0.466
        },
        {
          "concept": "framework",
          "relevance": 0.465
        },
        {
          "concept": "knowledge recall",
          "relevance": 0.465
        },
        {
          "concept": "language",
          "relevance": 0.463
        },
        {
          "concept": "benchmarks",
          "relevance": 0.444
        },
        {
          "concept": "LLM",
          "relevance": 0.439
        },
        {
          "concept": "applications",
          "relevance": 0.436
        },
        {
          "concept": "evaluation",
          "relevance": 0.436
        },
        {
          "concept": "tuning",
          "relevance": 0.436
        },
        {
          "concept": "model",
          "relevance": 0.414
        },
        {
          "concept": "recall",
          "relevance": 0.411
        },
        {
          "concept": "model scale",
          "relevance": 0.409
        },
        {
          "concept": "multiple axes",
          "relevance": 0.407
        },
        {
          "concept": "knowledge",
          "relevance": 0.404
        },
        {
          "concept": "capability",
          "relevance": 0.403
        },
        {
          "concept": "reasons",
          "relevance": 0.396
        },
        {
          "concept": "instruction",
          "relevance": 0.384
        },
        {
          "concept": "domain",
          "relevance": 0.379
        },
        {
          "concept": "answers",
          "relevance": 0.374
        },
        {
          "concept": "comprehension",
          "relevance": 0.369
        },
        {
          "concept": "exemplars",
          "relevance": 0.363
        },
        {
          "concept": "method",
          "relevance": 0.352
        },
        {
          "concept": "factuality",
          "relevance": 0.344
        },
        {
          "concept": "professional medicine",
          "relevance": 0.335
        },
        {
          "concept": "limitations",
          "relevance": 0.331
        },
        {
          "concept": "utilization",
          "relevance": 0.329
        },
        {
          "concept": "research",
          "relevance": 0.324
        },
        {
          "concept": "consumers",
          "relevance": 0.314
        },
        {
          "concept": "model1",
          "relevance": 0.305
        },
        {
          "concept": "strategies",
          "relevance": 0.305
        },
        {
          "concept": "variants",
          "relevance": 0.295
        },
        {
          "concept": "gap",
          "relevance": 0.277
        },
        {
          "concept": "questions",
          "relevance": 0.274
        },
        {
          "concept": "development",
          "relevance": 0.273
        },
        {
          "concept": "combination",
          "relevance": 0.268
        },
        {
          "concept": "method development",
          "relevance": 0.263
        },
        {
          "concept": "clinical application",
          "relevance": 0.25
        },
        {
          "concept": "harm",
          "relevance": 0.243
        },
        {
          "concept": "scale",
          "relevance": 0.232
        },
        {
          "concept": "potential utility",
          "relevance": 0.224
        },
        {
          "concept": "medicine",
          "relevance": 0.222
        },
        {
          "concept": "approach",
          "relevance": 0.189
        },
        {
          "concept": "clinicians",
          "relevance": 0.187
        },
        {
          "concept": "bar",
          "relevance": 0.174
        },
        {
          "concept": "axis",
          "relevance": 0.155
        }
      ]
    },
    {
      "paperId": "pub.1151332162",
      "doi": "10.1093/bib/bbac409",
      "title": "BioGPT: generative pre-trained transformer for biomedical text generation and mining",
      "year": 2022,
      "citationCount": 630,
      "fieldCitationRatio": 164.36,
      "abstract": "Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.",
      "reference_ids": [
        "pub.1039633073",
        "pub.1129119978",
        "pub.1121024723",
        "pub.1163041639",
        "pub.1133174796",
        "pub.1118169851",
        "pub.1138840042",
        "pub.1099113707",
        "pub.1125947346",
        "pub.1122290276",
        "pub.1045847617",
        "pub.1099113598",
        "pub.1133174687",
        "pub.1121025784",
        "pub.1141942664",
        "pub.1117659374",
        "pub.1151003027",
        "pub.1148391030",
        "pub.1117660148",
        "pub.1133177283",
        "pub.1129756704",
        "pub.1144245013",
        "pub.1134455314",
        "pub.1139947391",
        "pub.1120882528",
        "pub.1105386706",
        "pub.1019551002",
        "pub.1148391303",
        "pub.1151667611",
        "pub.1129120019",
        "pub.1129756783",
        "pub.1143948902",
        "pub.1133177065",
        "pub.1163042704",
        "pub.1129757334",
        "pub.1122290388",
        "pub.1032444382",
        "pub.1007810086"
      ],
      "concepts_scores": [
        {
          "concept": "language model",
          "relevance": 0.716
        },
        {
          "concept": "text generation",
          "relevance": 0.679
        },
        {
          "concept": "biomedical domain",
          "relevance": 0.67
        },
        {
          "concept": "natural language processing tasks",
          "relevance": 0.66
        },
        {
          "concept": "pre-trained language models",
          "relevance": 0.656
        },
        {
          "concept": "relation extraction task",
          "relevance": 0.645
        },
        {
          "concept": "language processing tasks",
          "relevance": 0.64
        },
        {
          "concept": "biomedical natural language processing tasks",
          "relevance": 0.638
        },
        {
          "concept": "transformer language models",
          "relevance": 0.637
        },
        {
          "concept": "pre-trained transformers",
          "relevance": 0.635
        },
        {
          "concept": "natural language domain",
          "relevance": 0.635
        },
        {
          "concept": "biomedical literature",
          "relevance": 0.629
        },
        {
          "concept": "fluent descriptions",
          "relevance": 0.599
        },
        {
          "concept": "extraction task",
          "relevance": 0.593
        },
        {
          "concept": "F1 score",
          "relevance": 0.584
        },
        {
          "concept": "processing tasks",
          "relevance": 0.574
        },
        {
          "concept": "biomedical tasks",
          "relevance": 0.558
        },
        {
          "concept": "biomedical terms",
          "relevance": 0.555
        },
        {
          "concept": "task",
          "relevance": 0.519
        },
        {
          "concept": "language domains",
          "relevance": 0.513
        },
        {
          "concept": "PubMedQA",
          "relevance": 0.471
        },
        {
          "concept": "BC5CDR",
          "relevance": 0.469
        },
        {
          "concept": "BERT",
          "relevance": 0.463
        },
        {
          "concept": "BioBERT",
          "relevance": 0.463
        },
        {
          "concept": "PubMedBERT",
          "relevance": 0.462
        },
        {
          "concept": "domain",
          "relevance": 0.451
        },
        {
          "concept": "case study",
          "relevance": 0.445
        },
        {
          "concept": "increasing attention",
          "relevance": 0.412
        },
        {
          "concept": "mining",
          "relevance": 0.404
        },
        {
          "concept": "accuracy",
          "relevance": 0.4
        },
        {
          "concept": "model",
          "relevance": 0.398
        },
        {
          "concept": "text",
          "relevance": 0.386
        },
        {
          "concept": "language",
          "relevance": 0.385
        },
        {
          "concept": "transformation",
          "relevance": 0.35
        },
        {
          "concept": "generation",
          "relevance": 0.347
        },
        {
          "concept": "success",
          "relevance": 0.341
        },
        {
          "concept": "description",
          "relevance": 0.317
        },
        {
          "concept": "i.",
          "relevance": 0.316
        },
        {
          "concept": "literature",
          "relevance": 0.309
        },
        {
          "concept": "attention",
          "relevance": 0.306
        },
        {
          "concept": "term",
          "relevance": 0.278
        },
        {
          "concept": "records",
          "relevance": 0.271
        },
        {
          "concept": "lack",
          "relevance": 0.257
        },
        {
          "concept": "branches",
          "relevance": 0.253
        },
        {
          "concept": "cases",
          "relevance": 0.243
        },
        {
          "concept": "GPT",
          "relevance": 0.23
        },
        {
          "concept": "study",
          "relevance": 0.212
        },
        {
          "concept": "scores",
          "relevance": 0.207
        },
        {
          "concept": "ddI",
          "relevance": 0.176
        }
      ]
    },
    {
      "paperId": "pub.1141942664",
      "doi": "10.1145/3458754",
      "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
      "year": 2021,
      "citationCount": 1471,
      "fieldCitationRatio": 464.23,
      "abstract": " Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at https://aka.ms/BLURB . ",
      "reference_ids": [
        "pub.1061662795",
        "pub.1121024871",
        "pub.1007810086",
        "pub.1039633073",
        "pub.1031899717",
        "pub.1117659358",
        "pub.1099110523",
        "pub.1092391112",
        "pub.1121025784",
        "pub.1021581372",
        "pub.1045847617",
        "pub.1084853449",
        "pub.1099113598",
        "pub.1101601100",
        "pub.1051365551",
        "pub.1005860045",
        "pub.1114069796",
        "pub.1117659208",
        "pub.1120882528",
        "pub.1041411233",
        "pub.1038140272",
        "pub.1118170219",
        "pub.1115678138",
        "pub.1022762554",
        "pub.1091206594",
        "pub.1099105884",
        "pub.1050483976",
        "pub.1090718443"
      ],
      "concepts_scores": [
        {
          "concept": "natural language processing",
          "relevance": 0.82
        },
        {
          "concept": "language model",
          "relevance": 0.762
        },
        {
          "concept": "language processing",
          "relevance": 0.698
        },
        {
          "concept": "task-specific fine-tuning",
          "relevance": 0.693
        },
        {
          "concept": "biomedical NLP",
          "relevance": 0.69
        },
        {
          "concept": "biomedical natural language processing",
          "relevance": 0.679
        },
        {
          "concept": "general domain corpora",
          "relevance": 0.675
        },
        {
          "concept": "biomedical NLP tasks",
          "relevance": 0.675
        },
        {
          "concept": "neural language models",
          "relevance": 0.671
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.669
        },
        {
          "concept": "Pretrained language models",
          "relevance": 0.668
        },
        {
          "concept": "language model pretraining",
          "relevance": 0.662
        },
        {
          "concept": "task-specific models",
          "relevance": 0.66
        },
        {
          "concept": "NLP benchmarks",
          "relevance": 0.626
        },
        {
          "concept": "unlabeled text",
          "relevance": 0.625
        },
        {
          "concept": "domain corpus",
          "relevance": 0.624
        },
        {
          "concept": "NLP tasks",
          "relevance": 0.623
        },
        {
          "concept": "entity recognition",
          "relevance": 0.622
        },
        {
          "concept": "BERT model",
          "relevance": 0.619
        },
        {
          "concept": "model pretraining",
          "relevance": 0.617
        },
        {
          "concept": "tagging scheme",
          "relevance": 0.598
        },
        {
          "concept": "fine-tuning",
          "relevance": 0.573
        },
        {
          "concept": "pretraining",
          "relevance": 0.569
        },
        {
          "concept": "BERT",
          "relevance": 0.56
        },
        {
          "concept": "substantial gains",
          "relevance": 0.525
        },
        {
          "concept": "benchmarks",
          "relevance": 0.517
        },
        {
          "concept": "https://aka",
          "relevance": 0.493
        },
        {
          "concept": "impressive gains",
          "relevance": 0.492
        },
        {
          "concept": "leaderboard",
          "relevance": 0.48
        },
        {
          "concept": "newswire",
          "relevance": 0.471
        },
        {
          "concept": "dataset",
          "relevance": 0.457
        },
        {
          "concept": "Web",
          "relevance": 0.448
        },
        {
          "concept": "task",
          "relevance": 0.438
        },
        {
          "concept": "scheme",
          "relevance": 0.438
        },
        {
          "concept": "scratch results",
          "relevance": 0.432
        },
        {
          "concept": "recognition",
          "relevance": 0.421
        },
        {
          "concept": "model",
          "relevance": 0.418
        },
        {
          "concept": "corpus",
          "relevance": 0.415
        },
        {
          "concept": "text",
          "relevance": 0.403
        },
        {
          "concept": "gain",
          "relevance": 0.392
        },
        {
          "concept": "scratch",
          "relevance": 0.389
        },
        {
          "concept": "Biomedical",
          "relevance": 0.385
        },
        {
          "concept": "domain",
          "relevance": 0.381
        },
        {
          "concept": "process",
          "relevance": 0.367
        },
        {
          "concept": "entities",
          "relevance": 0.365
        },
        {
          "concept": "results",
          "relevance": 0.357
        },
        {
          "concept": "evaluation",
          "relevance": 0.355
        },
        {
          "concept": "blurb",
          "relevance": 0.344
        },
        {
          "concept": "experiments",
          "relevance": 0.336
        },
        {
          "concept": "research",
          "relevance": 0.326
        },
        {
          "concept": "efforts",
          "relevance": 0.325
        },
        {
          "concept": "board",
          "relevance": 0.313
        },
        {
          "concept": "choice",
          "relevance": 0.273
        },
        {
          "concept": "practice",
          "relevance": 0.265
        },
        {
          "concept": "biomedicine",
          "relevance": 0.264
        },
        {
          "concept": "community",
          "relevance": 0.262
        },
        {
          "concept": "investigation",
          "relevance": 0.168
        }
      ]
    },
    {
      "paperId": "pub.1129120019",
      "doi": "10.24963/ijcai.2020/561",
      "title": "A Relation-Specific Attention Network for Joint Entity and Relation Extraction",
      "year": 2020,
      "citationCount": 117,
      "fieldCitationRatio": 26.67,
      "abstract": "Joint extraction of entities and relations is an important task in natural language processing (NLP), which aims to capture all relational triplets from plain texts. This is a big challenge due to some of the triplets extracted from one sentence may have overlapping entities. Most existing methods perform entity recognition followed by relation detection between every possible entity pairs, which usually suffers from numerous redundant operations. In this paper, we propose a relation-specific attention network (RSAN) to handle the issue. Our RSAN utilizes relation-aware attention mechanism to construct specific sentence representations for each relation, and then performs sequence labeling to extract its corresponding head and tail entities. Experiments on two public datasets show that our model can effectively extract overlapping triplets and achieve state-of-the-art performance.",
      "reference_ids": NaN,
      "concepts_scores": [
        {
          "concept": "natural language processing",
          "relevance": 0.755
        },
        {
          "concept": "state-of-the-art performance",
          "relevance": 0.683
        },
        {
          "concept": "joint extraction of entities",
          "relevance": 0.682
        },
        {
          "concept": "relation-aware attention mechanism",
          "relevance": 0.682
        },
        {
          "concept": "extraction of entities",
          "relevance": 0.662
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.658
        },
        {
          "concept": "tail entities",
          "relevance": 0.616
        },
        {
          "concept": "entity pairs",
          "relevance": 0.615
        },
        {
          "concept": "relation extraction",
          "relevance": 0.612
        },
        {
          "concept": "entity recognition",
          "relevance": 0.612
        },
        {
          "concept": "sentence representation",
          "relevance": 0.61
        },
        {
          "concept": "sequence labeling",
          "relevance": 0.609
        },
        {
          "concept": "attention mechanism",
          "relevance": 0.605
        },
        {
          "concept": "public datasets",
          "relevance": 0.604
        },
        {
          "concept": "relational triplets",
          "relevance": 0.595
        },
        {
          "concept": "joint extraction",
          "relevance": 0.594
        },
        {
          "concept": "language processing",
          "relevance": 0.593
        },
        {
          "concept": "attention network",
          "relevance": 0.578
        },
        {
          "concept": "sentences",
          "relevance": 0.461
        },
        {
          "concept": "RSAN",
          "relevance": 0.452
        },
        {
          "concept": "dataset",
          "relevance": 0.45
        },
        {
          "concept": "entities",
          "relevance": 0.447
        },
        {
          "concept": "network",
          "relevance": 0.444
        },
        {
          "concept": "task",
          "relevance": 0.431
        },
        {
          "concept": "representation",
          "relevance": 0.421
        },
        {
          "concept": "recognition",
          "relevance": 0.414
        },
        {
          "concept": "text",
          "relevance": 0.397
        },
        {
          "concept": "performance",
          "relevance": 0.396
        },
        {
          "concept": "detection",
          "relevance": 0.381
        },
        {
          "concept": "labeling",
          "relevance": 0.381
        },
        {
          "concept": "operation",
          "relevance": 0.362
        },
        {
          "concept": "issues",
          "relevance": 0.356
        },
        {
          "concept": "method",
          "relevance": 0.348
        },
        {
          "concept": "experiments",
          "relevance": 0.33
        },
        {
          "concept": "model",
          "relevance": 0.329
        },
        {
          "concept": "relations",
          "relevance": 0.328
        },
        {
          "concept": "process",
          "relevance": 0.312
        },
        {
          "concept": "extraction",
          "relevance": 0.31
        },
        {
          "concept": "pairs",
          "relevance": 0.307
        },
        {
          "concept": "sequence",
          "relevance": 0.266
        },
        {
          "concept": "triplet",
          "relevance": 0.261
        },
        {
          "concept": "head",
          "relevance": 0.256
        },
        {
          "concept": "mechanism",
          "relevance": 0.231
        },
        {
          "concept": "tail",
          "relevance": 0.187
        }
      ]
    },
    {
      "paperId": "pub.1152983179",
      "doi": "10.1016/j.aiopen.2022.11.003",
      "title": "PTR: Prompt Tuning with Rules for Text Classification",
      "year": 2022,
      "citationCount": 302,
      "fieldCitationRatio": 119.17,
      "abstract": "Recently, prompt tuning has been widely applied to stimulate the rich knowledge in pre-trained language models (PLMs) to serve NLP tasks. Although prompt tuning has achieved promising results on some few-class classification tasks, such as sentiment classification and natural language inference, manually designing prompts is cumbersome. Meanwhile, generating prompts automatically is also difficult and time-consuming. Therefore, obtaining effective prompts for complex many-class classification tasks still remains a challenge. In this paper, we propose to encode the prior knowledge of a classification task into rules, then design sub-prompts according to the rules, and finally combine the sub-prompts to handle the task. We name this Prompt Tuning method with Rules “PTR”. Compared with existing prompt-based methods, PTR achieves a good trade-off between effectiveness and efficiency in building prompts. We conduct experiments on three many-class classification tasks, including relation classification, entity typing, and intent classification. The results show that PTR outperforms both vanilla and prompt tuning baselines, indicating the effectiveness of utilizing rules for prompt tuning. The source code of PTR is available at https://github.com/thunlp/PTR.",
      "reference_ids": [
        "pub.1012146698",
        "pub.1100516689",
        "pub.1121024871",
        "pub.1139947274",
        "pub.1139947391",
        "pub.1129757730",
        "pub.1122290267",
        "pub.1133177283",
        "pub.1148391057",
        "pub.1122290126",
        "pub.1133177060",
        "pub.1133174526",
        "pub.1122290022",
        "pub.1129756642",
        "pub.1143948874",
        "pub.1041355599",
        "pub.1150866022",
        "pub.1122290729",
        "pub.1149741611",
        "pub.1142776451",
        "pub.1148390501",
        "pub.1129756711",
        "pub.1133174687",
        "pub.1096024932",
        "pub.1143948984",
        "pub.1133177171",
        "pub.1122290148",
        "pub.1151003027",
        "pub.1134455702",
        "pub.1131074864",
        "pub.1129756622",
        "pub.1147477782",
        "pub.1117658890",
        "pub.1139947326",
        "pub.1121024948",
        "pub.1148391028",
        "pub.1125558196"
      ],
      "concepts_scores": [
        {
          "concept": "classification task",
          "relevance": 0.774
        },
        {
          "concept": "prompt tuning",
          "relevance": 0.728
        },
        {
          "concept": "prompt-based method",
          "relevance": 0.682
        },
        {
          "concept": "natural language inference",
          "relevance": 0.679
        },
        {
          "concept": "sentiment classification",
          "relevance": 0.631
        },
        {
          "concept": "text classification",
          "relevance": 0.629
        },
        {
          "concept": "NLP tasks",
          "relevance": 0.629
        },
        {
          "concept": "language inference",
          "relevance": 0.629
        },
        {
          "concept": "intent classification",
          "relevance": 0.628
        },
        {
          "concept": "entity types",
          "relevance": 0.623
        },
        {
          "concept": "language model",
          "relevance": 0.622
        },
        {
          "concept": "task",
          "relevance": 0.553
        },
        {
          "concept": "classification",
          "relevance": 0.547
        },
        {
          "concept": "time-consuming",
          "relevance": 0.531
        },
        {
          "concept": "rules",
          "relevance": 0.509
        },
        {
          "concept": "NLP",
          "relevance": 0.481
        },
        {
          "concept": "effective prompts",
          "relevance": 0.478
        },
        {
          "concept": "tuning",
          "relevance": 0.466
        },
        {
          "concept": "vanilla",
          "relevance": 0.451
        },
        {
          "concept": "code",
          "relevance": 0.45
        },
        {
          "concept": "sentiment",
          "relevance": 0.444
        },
        {
          "concept": "inference",
          "relevance": 0.421
        },
        {
          "concept": "method",
          "relevance": 0.414
        },
        {
          "concept": "knowledge",
          "relevance": 0.411
        },
        {
          "concept": "text",
          "relevance": 0.407
        },
        {
          "concept": "language",
          "relevance": 0.406
        },
        {
          "concept": "manually",
          "relevance": 0.398
        },
        {
          "concept": "PLM",
          "relevance": 0.392
        },
        {
          "concept": "prompts",
          "relevance": 0.379
        },
        {
          "concept": "entities",
          "relevance": 0.369
        },
        {
          "concept": "results",
          "relevance": 0.36
        },
        {
          "concept": "efficiency",
          "relevance": 0.354
        },
        {
          "concept": "experiments",
          "relevance": 0.339
        },
        {
          "concept": "model",
          "relevance": 0.338
        },
        {
          "concept": "intention",
          "relevance": 0.338
        },
        {
          "concept": "baseline",
          "relevance": 0.251
        },
        {
          "concept": "PTR",
          "relevance": 0.247
        },
        {
          "concept": "effect",
          "relevance": 0.235
        },
        {
          "concept": "type",
          "relevance": 0.213
        }
      ]
    },
    {
      "paperId": "pub.1147477782",
      "doi": "10.1145/3485447.3511998",
      "title": "KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction",
      "year": 2022,
      "citationCount": 262,
      "fieldCitationRatio": 102.72,
      "abstract": "Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in GitHub1 for reproducibility.",
      "reference_ids": [
        "pub.1138840374",
        "pub.1122315743",
        "pub.1122290022",
        "pub.1139947538",
        "pub.1134455317",
        "pub.1127360386",
        "pub.1137805304",
        "pub.1138569947",
        "pub.1148391035",
        "pub.1099106181",
        "pub.1117658890",
        "pub.1134455702",
        "pub.1140364023",
        "pub.1122290039",
        "pub.1134455455",
        "pub.1120647268",
        "pub.1128856587",
        "pub.1129756711",
        "pub.1127360416",
        "pub.1100516689",
        "pub.1125558196",
        "pub.1138571099",
        "pub.1117658766",
        "pub.1139947422",
        "pub.1139947326",
        "pub.1118169675",
        "pub.1129119957",
        "pub.1133177107",
        "pub.1139947391",
        "pub.1142776451",
        "pub.1134455786",
        "pub.1163042908",
        "pub.1139947379",
        "pub.1140412574",
        "pub.1133177283",
        "pub.1121024616",
        "pub.1150866060",
        "pub.1035437891",
        "pub.1121024871",
        "pub.1099113707",
        "pub.1129757046",
        "pub.1150866067",
        "pub.1117659160",
        "pub.1143948984"
      ],
      "concepts_scores": [
        {
          "concept": "prompt-tuning",
          "relevance": 0.756
        },
        {
          "concept": "classification task",
          "relevance": 0.712
        },
        {
          "concept": "few-shot classification tasks",
          "relevance": 0.688
        },
        {
          "concept": "language modeling problem",
          "relevance": 0.67
        },
        {
          "concept": "relation extraction",
          "relevance": 0.618
        },
        {
          "concept": "prompt template",
          "relevance": 0.617
        },
        {
          "concept": "domain expertise",
          "relevance": 0.604
        },
        {
          "concept": "text pieces",
          "relevance": 0.601
        },
        {
          "concept": "label words",
          "relevance": 0.599
        },
        {
          "concept": "latent knowledge",
          "relevance": 0.595
        },
        {
          "concept": "core idea",
          "relevance": 0.577
        },
        {
          "concept": "typed words",
          "relevance": 0.575
        },
        {
          "concept": "experimental results",
          "relevance": 0.547
        },
        {
          "concept": "dataset",
          "relevance": 0.526
        },
        {
          "concept": "model problem",
          "relevance": 0.506
        },
        {
          "concept": "task",
          "relevance": 0.504
        },
        {
          "concept": "GitHub1",
          "relevance": 0.486
        },
        {
          "concept": "optimization",
          "relevance": 0.479
        },
        {
          "concept": "labeling",
          "relevance": 0.476
        },
        {
          "concept": "words",
          "relevance": 0.453
        },
        {
          "concept": "code",
          "relevance": 0.442
        },
        {
          "concept": "structural constraints",
          "relevance": 0.438
        },
        {
          "concept": "classification",
          "relevance": 0.432
        },
        {
          "concept": "representation",
          "relevance": 0.425
        },
        {
          "concept": "knowledge",
          "relevance": 0.424
        },
        {
          "concept": "input",
          "relevance": 0.405
        },
        {
          "concept": "language",
          "relevance": 0.399
        },
        {
          "concept": "constraints",
          "relevance": 0.399
        },
        {
          "concept": "extraction",
          "relevance": 0.38
        },
        {
          "concept": "domain",
          "relevance": 0.378
        },
        {
          "concept": "sets",
          "relevance": 0.362
        },
        {
          "concept": "results",
          "relevance": 0.354
        },
        {
          "concept": "expertise",
          "relevance": 0.35
        },
        {
          "concept": "synergistic optimization",
          "relevance": 0.336
        },
        {
          "concept": "template",
          "relevance": 0.33
        },
        {
          "concept": "ideas",
          "relevance": 0.319
        },
        {
          "concept": "construction",
          "relevance": 0.307
        },
        {
          "concept": "low-resource settings",
          "relevance": 0.302
        },
        {
          "concept": "core",
          "relevance": 0.292
        },
        {
          "concept": "pieces",
          "relevance": 0.292
        },
        {
          "concept": "relations",
          "relevance": 0.271
        },
        {
          "concept": "reproducibility",
          "relevance": 0.215
        },
        {
          "concept": "effect",
          "relevance": 0.199
        },
        {
          "concept": "problem",
          "relevance": 0.172
        },
        {
          "concept": "approach",
          "relevance": 0.153
        }
      ]
    },
    {
      "paperId": "pub.1150866022",
      "doi": "10.1609/aaai.v35i15.17631",
      "title": "Re-TACRED: Addressing Shortcomings of the TACRED Dataset",
      "year": 2021,
      "citationCount": 54,
      "fieldCitationRatio": 17.1,
      "abstract": "TACRED is one of the largest and most widely used sentence-level relation extraction datasets. Proposed models that are evaluated using this dataset consistently set new state-of-the-art performance. However, they still exhibit large error rates despite leveraging external knowledge and unsupervised pretraining on large text corpora. A recent study suggested that this may be due to poor dataset quality. The study observed that over 50% of the most challenging sentences from the development and test sets are incorrectly labeled and account for an average drop of 8% f1-score in model performance. However, this study was limited to a small biased sample of 5k (out of a total of 106k) sentences, substantially restricting the generalizability and broader implications of its findings. In this paper, we address these shortcomings by: (i) performing a comprehensive study over the whole TACRED dataset, (ii) proposing an improved crowdsourcing strategy and deploying it to re-annotate the whole dataset, and (iii) performing a thorough analysis to understand how correcting the TACRED annotations affects previously published results. After verification, we observed that 23.9% of TACRED labels are incorrect. Moreover, evaluating several models on our revised dataset yields an average f1-score improvement of 14.3% and helps uncover significant relationships between the different models (rather than simply offsetting or scaling their scores by a constant factor). Finally, aside from our analysis we also release Re-TACRED, a new completely re-annotated version of the TACRED dataset that can be used to perform reliable evaluation of relation extraction models.",
      "reference_ids": NaN,
      "concepts_scores": [
        {
          "concept": "TACRED dataset",
          "relevance": 0.769
        },
        {
          "concept": "state-of-the-art performance",
          "relevance": 0.701
        },
        {
          "concept": "average F1 score improvement",
          "relevance": 0.7
        },
        {
          "concept": "F1 score improvement",
          "relevance": 0.679
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.676
        },
        {
          "concept": "leverage external knowledge",
          "relevance": 0.671
        },
        {
          "concept": "unsupervised pretraining",
          "relevance": 0.628
        },
        {
          "concept": "extraction dataset",
          "relevance": 0.621
        },
        {
          "concept": "text corpus",
          "relevance": 0.616
        },
        {
          "concept": "error rate",
          "relevance": 0.604
        },
        {
          "concept": "dataset quality",
          "relevance": 0.603
        },
        {
          "concept": "extraction model",
          "relevance": 0.599
        },
        {
          "concept": "external knowledge",
          "relevance": 0.594
        },
        {
          "concept": "crowdsourcing strategy",
          "relevance": 0.588
        },
        {
          "concept": "test set",
          "relevance": 0.584
        },
        {
          "concept": "evaluate several models",
          "relevance": 0.584
        },
        {
          "concept": "dataset",
          "relevance": 0.577
        },
        {
          "concept": "TACR",
          "relevance": 0.571
        },
        {
          "concept": "proposed models",
          "relevance": 0.535
        },
        {
          "concept": "model performance",
          "relevance": 0.511
        },
        {
          "concept": "biased sampling",
          "relevance": 0.489
        },
        {
          "concept": "sentences",
          "relevance": 0.473
        },
        {
          "concept": "performance",
          "relevance": 0.471
        },
        {
          "concept": "re-annotation",
          "relevance": 0.471
        },
        {
          "concept": "pretraining",
          "relevance": 0.464
        },
        {
          "concept": "annotation",
          "relevance": 0.451
        },
        {
          "concept": "verification",
          "relevance": 0.434
        },
        {
          "concept": "model",
          "relevance": 0.421
        },
        {
          "concept": "corpus",
          "relevance": 0.419
        },
        {
          "concept": "text",
          "relevance": 0.407
        },
        {
          "concept": "error",
          "relevance": 0.405
        },
        {
          "concept": "Several models",
          "relevance": 0.397
        },
        {
          "concept": "shortcomings",
          "relevance": 0.391
        },
        {
          "concept": "labeling",
          "relevance": 0.391
        },
        {
          "concept": "version",
          "relevance": 0.372
        },
        {
          "concept": "sets",
          "relevance": 0.368
        },
        {
          "concept": "comprehensive study",
          "relevance": 0.36
        },
        {
          "concept": "generalizability",
          "relevance": 0.359
        },
        {
          "concept": "evaluation",
          "relevance": 0.358
        },
        {
          "concept": "knowledge",
          "relevance": 0.355
        },
        {
          "concept": "published results",
          "relevance": 0.35
        },
        {
          "concept": "quality",
          "relevance": 0.348
        },
        {
          "concept": "improvement",
          "relevance": 0.325
        },
        {
          "concept": "extraction",
          "relevance": 0.318
        },
        {
          "concept": "results",
          "relevance": 0.311
        },
        {
          "concept": "strategies",
          "relevance": 0.31
        },
        {
          "concept": "analysis",
          "relevance": 0.297
        },
        {
          "concept": "development",
          "relevance": 0.278
        },
        {
          "concept": "average drop",
          "relevance": 0.26
        },
        {
          "concept": "test",
          "relevance": 0.253
        },
        {
          "concept": "rate",
          "relevance": 0.236
        },
        {
          "concept": "study",
          "relevance": 0.224
        },
        {
          "concept": "relationship",
          "relevance": 0.222
        },
        {
          "concept": "samples",
          "relevance": 0.193
        },
        {
          "concept": "drop",
          "relevance": 0.183
        },
        {
          "concept": "findings",
          "relevance": 0.165
        }
      ]
    },
    {
      "paperId": "pub.1160103012",
      "doi": "10.7759/cureus.40895",
      "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
      "year": 2023,
      "citationCount": 344,
      "fieldCitationRatio": NaN,
      "abstract": "Objective The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. Methods We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. Results The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Conclusion Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.",
      "reference_ids": [
        "pub.1156219911",
        "pub.1020892502",
        "pub.1157904515",
        "pub.1040339913",
        "pub.1152237077",
        "pub.1125946347",
        "pub.1153408759",
        "pub.1155222253",
        "pub.1156602823",
        "pub.1157341573"
      ],
      "concepts_scores": [
        {
          "concept": "Meta AI",
          "relevance": 0.693
        },
        {
          "concept": "language model",
          "relevance": 0.682
        },
        {
          "concept": "medical domain knowledge",
          "relevance": 0.645
        },
        {
          "concept": "specialized language models",
          "relevance": 0.645
        },
        {
          "concept": "information retrieval mechanism",
          "relevance": 0.639
        },
        {
          "concept": "model fine-tuning",
          "relevance": 0.633
        },
        {
          "concept": "online medical consultation platforms",
          "relevance": 0.599
        },
        {
          "concept": "information retrieval",
          "relevance": 0.594
        },
        {
          "concept": "domain knowledge",
          "relevance": 0.589
        },
        {
          "concept": "privacy concerns",
          "relevance": 0.587
        },
        {
          "concept": "real-time information",
          "relevance": 0.574
        },
        {
          "concept": "retrieval mechanism",
          "relevance": 0.568
        },
        {
          "concept": "low error tolerance",
          "relevance": 0.562
        },
        {
          "concept": "error tolerance",
          "relevance": 0.544
        },
        {
          "concept": "offline sources",
          "relevance": 0.536
        },
        {
          "concept": "enhanced accuracy",
          "relevance": 0.519
        },
        {
          "concept": "online sources",
          "relevance": 0.515
        },
        {
          "concept": "medical field",
          "relevance": 0.509
        },
        {
          "concept": "consultation platform",
          "relevance": 0.499
        },
        {
          "concept": "model's ability",
          "relevance": 0.487
        },
        {
          "concept": "model refinement",
          "relevance": 0.475
        },
        {
          "concept": "language",
          "relevance": 0.468
        },
        {
          "concept": "medical knowledge",
          "relevance": 0.466
        },
        {
          "concept": "medical databases",
          "relevance": 0.465
        },
        {
          "concept": "ChatGPT",
          "relevance": 0.463
        },
        {
          "concept": "accuracy",
          "relevance": 0.463
        },
        {
          "concept": "patient-doctor interaction",
          "relevance": 0.462
        },
        {
          "concept": "Wikipedia",
          "relevance": 0.461
        },
        {
          "concept": "privacy",
          "relevance": 0.458
        },
        {
          "concept": "information",
          "relevance": 0.449
        },
        {
          "concept": "dataset",
          "relevance": 0.437
        },
        {
          "concept": "patient's doctor",
          "relevance": 0.437
        },
        {
          "concept": "retrieval",
          "relevance": 0.431
        },
        {
          "concept": "patient needs",
          "relevance": 0.405
        },
        {
          "concept": "platform",
          "relevance": 0.404
        },
        {
          "concept": "patient inquiries",
          "relevance": 0.404
        },
        {
          "concept": "model",
          "relevance": 0.4
        },
        {
          "concept": "LLM",
          "relevance": 0.395
        },
        {
          "concept": "medical advice",
          "relevance": 0.394
        },
        {
          "concept": "knowledge",
          "relevance": 0.389
        },
        {
          "concept": "database",
          "relevance": 0.375
        },
        {
          "concept": "accurate advice",
          "relevance": 0.361
        },
        {
          "concept": "advice",
          "relevance": 0.358
        },
        {
          "concept": "improvement",
          "relevance": 0.357
        },
        {
          "concept": "significant advances",
          "relevance": 0.347
        },
        {
          "concept": "medication",
          "relevance": 0.343
        },
        {
          "concept": "refinement",
          "relevance": 0.334
        },
        {
          "concept": "source",
          "relevance": 0.318
        },
        {
          "concept": "patients",
          "relevance": 0.318
        },
        {
          "concept": "advances",
          "relevance": 0.315
        },
        {
          "concept": "research",
          "relevance": 0.312
        },
        {
          "concept": "data",
          "relevance": 0.306
        },
        {
          "concept": "needs",
          "relevance": 0.306
        },
        {
          "concept": "ability",
          "relevance": 0.279
        },
        {
          "concept": "field",
          "relevance": 0.277
        },
        {
          "concept": "limitations",
          "relevance": 0.275
        },
        {
          "concept": "dialogue",
          "relevance": 0.269
        },
        {
          "concept": "concerns",
          "relevance": 0.263
        },
        {
          "concept": "enhancement",
          "relevance": 0.263
        },
        {
          "concept": "interaction",
          "relevance": 0.237
        },
        {
          "concept": "tolerance",
          "relevance": 0.225
        },
        {
          "concept": "mechanism",
          "relevance": 0.224
        },
        {
          "concept": "llamas",
          "relevance": 0.221
        },
        {
          "concept": "response",
          "relevance": 0.221
        },
        {
          "concept": "inquiry",
          "relevance": 0.213
        },
        {
          "concept": "conversion",
          "relevance": 0.198
        }
      ]
    },
    {
      "paperId": "pub.1155222253",
      "doi": "10.2196/45312",
      "title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
      "year": 2023,
      "citationCount": 1624,
      "fieldCitationRatio": 2494.88,
      "abstract": "BACKGROUND: Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input.\nOBJECTIVE: This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as to analyze responses for user interpretability.\nMETHODS: We used 2 sets of multiple-choice questions to evaluate ChatGPT's performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT's performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question.\nRESULTS: Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT's answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively.\nCONCLUSIONS: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT's capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.",
      "reference_ids": [
        "pub.1127990889",
        "pub.1139648327",
        "pub.1069777578",
        "pub.1118769417",
        "pub.1032164422",
        "pub.1130454004",
        "pub.1140942024",
        "pub.1044588416",
        "pub.1131360000",
        "pub.1148391311",
        "pub.1119351642",
        "pub.1121015527",
        "pub.1070970326",
        "pub.1151118998",
        "pub.1121015251",
        "pub.1008947203",
        "pub.1140213204",
        "pub.1046621535",
        "pub.1122687461",
        "pub.1146341073",
        "pub.1099604658"
      ],
      "concepts_scores": [
        {
          "concept": "natural language processing models",
          "relevance": 0.723
        },
        {
          "concept": "language processing models",
          "relevance": 0.697
        },
        {
          "concept": "language model",
          "relevance": 0.661
        },
        {
          "concept": "GPT-3",
          "relevance": 0.639
        },
        {
          "concept": "Generative Pre-trained Transformer",
          "relevance": 0.633
        },
        {
          "concept": "response to user input",
          "relevance": 0.63
        },
        {
          "concept": "presence of information",
          "relevance": 0.628
        },
        {
          "concept": "medical question answering",
          "relevance": 0.621
        },
        {
          "concept": "pre-trained transformers",
          "relevance": 0.616
        },
        {
          "concept": "process model",
          "relevance": 0.593
        },
        {
          "concept": "data sets",
          "relevance": 0.579
        },
        {
          "concept": "question answering",
          "relevance": 0.578
        },
        {
          "concept": "answer selection",
          "relevance": 0.57
        },
        {
          "concept": "user input",
          "relevance": 0.567
        },
        {
          "concept": "text output",
          "relevance": 0.564
        },
        {
          "concept": "user base",
          "relevance": 0.562
        },
        {
          "concept": "ChatGPT",
          "relevance": 0.561
        },
        {
          "concept": "user interpretation",
          "relevance": 0.551
        },
        {
          "concept": "qualitative metrics",
          "relevance": 0.543
        },
        {
          "concept": "internal information",
          "relevance": 0.524
        },
        {
          "concept": "question difficulty",
          "relevance": 0.518
        },
        {
          "concept": "medical education tool",
          "relevance": 0.512
        },
        {
          "concept": "users",
          "relevance": 0.505
        },
        {
          "concept": "informal contexts",
          "relevance": 0.481
        },
        {
          "concept": "question bank",
          "relevance": 0.476
        },
        {
          "concept": "performance",
          "relevance": 0.465
        },
        {
          "concept": "logical justification",
          "relevance": 0.464
        },
        {
          "concept": "information",
          "relevance": 0.464
        },
        {
          "concept": "language",
          "relevance": 0.432
        },
        {
          "concept": "answers",
          "relevance": 0.431
        },
        {
          "concept": "incorrect answers",
          "relevance": 0.426
        },
        {
          "concept": "output",
          "relevance": 0.411
        },
        {
          "concept": "metrics",
          "relevance": 0.411
        },
        {
          "concept": "educational tool",
          "relevance": 0.407
        },
        {
          "concept": "task",
          "relevance": 0.406
        },
        {
          "concept": "learning",
          "relevance": 0.4
        },
        {
          "concept": "logic",
          "relevance": 0.397
        },
        {
          "concept": "AMBOSS",
          "relevance": 0.393
        },
        {
          "concept": "sets",
          "relevance": 0.391
        },
        {
          "concept": "model",
          "relevance": 0.387
        },
        {
          "concept": "accuracy",
          "relevance": 0.387
        },
        {
          "concept": "multiple-choice questions",
          "relevance": 0.383
        },
        {
          "concept": "input",
          "relevance": 0.378
        },
        {
          "concept": "text",
          "relevance": 0.374
        },
        {
          "concept": "data",
          "relevance": 0.37
        },
        {
          "concept": "random chance",
          "relevance": 0.358
        },
        {
          "concept": "ChAT",
          "relevance": 0.352
        },
        {
          "concept": "United States Medical Licensing Examination",
          "relevance": 0.349
        },
        {
          "concept": "transformation",
          "relevance": 0.339
        },
        {
          "concept": "tools",
          "relevance": 0.339
        },
        {
          "concept": "Medical Licensing Examination",
          "relevance": 0.325
        },
        {
          "concept": "potential applications",
          "relevance": 0.322
        },
        {
          "concept": "base",
          "relevance": 0.314
        },
        {
          "concept": "steps",
          "relevance": 0.312
        },
        {
          "concept": "selection",
          "relevance": 0.31
        },
        {
          "concept": "context",
          "relevance": 0.308
        },
        {
          "concept": "threshold",
          "relevance": 0.302
        },
        {
          "concept": "improvement",
          "relevance": 0.299
        },
        {
          "concept": "Licensing Examination",
          "relevance": 0.298
        },
        {
          "concept": "questions",
          "relevance": 0.296
        },
        {
          "concept": "medical students",
          "relevance": 0.295
        },
        {
          "concept": "difficulties",
          "relevance": 0.293
        },
        {
          "concept": "justification",
          "relevance": 0.29
        },
        {
          "concept": "students",
          "relevance": 0.278
        },
        {
          "concept": "National Board",
          "relevance": 0.276
        },
        {
          "concept": "medical education",
          "relevance": 0.272
        },
        {
          "concept": "third-year medical students",
          "relevance": 0.272
        },
        {
          "concept": "banks",
          "relevance": 0.259
        },
        {
          "concept": "units",
          "relevance": 0.254
        },
        {
          "concept": "interpretation",
          "relevance": 0.254
        },
        {
          "concept": "average",
          "relevance": 0.252
        },
        {
          "concept": "equivalence",
          "relevance": 0.246
        },
        {
          "concept": "chance",
          "relevance": 0.244
        },
        {
          "concept": "capacity",
          "relevance": 0.237
        },
        {
          "concept": "cases",
          "relevance": 0.236
        },
        {
          "concept": "education",
          "relevance": 0.234
        },
        {
          "concept": "National",
          "relevance": 0.224
        },
        {
          "concept": "exam",
          "relevance": 0.221
        },
        {
          "concept": "NBME",
          "relevance": 0.213
        },
        {
          "concept": "examination",
          "relevance": 0.201
        },
        {
          "concept": "medication",
          "relevance": 0.182
        },
        {
          "concept": "response",
          "relevance": 0.176
        },
        {
          "concept": "presence",
          "relevance": 0.17
        },
        {
          "concept": "study",
          "relevance": 0.169
        },
        {
          "concept": "decrease",
          "relevance": 0.152
        }
      ]
    },
    {
      "paperId": "pub.1118769417",
      "doi": "10.48550/arxiv.1706.03762",
      "title": "Attention Is All You Need",
      "year": 2017,
      "citationCount": 17916,
      "fieldCitationRatio": 3611.65,
      "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
      "reference_ids": NaN,
      "concepts_scores": [
        {
          "concept": "BLEU score",
          "relevance": 0.685
        },
        {
          "concept": "state-of-the-art BLEU scores",
          "relevance": 0.667
        },
        {
          "concept": "machine translation tasks",
          "relevance": 0.64
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.635
        },
        {
          "concept": "encoder-decoder configuration",
          "relevance": 0.634
        },
        {
          "concept": "limited training data",
          "relevance": 0.631
        },
        {
          "concept": "network architecture",
          "relevance": 0.585
        },
        {
          "concept": "attention mechanism",
          "relevance": 0.584
        },
        {
          "concept": "training data",
          "relevance": 0.581
        },
        {
          "concept": "translation tasks",
          "relevance": 0.569
        },
        {
          "concept": "training costs",
          "relevance": 0.556
        },
        {
          "concept": "BLEU",
          "relevance": 0.536
        },
        {
          "concept": "transduction model",
          "relevance": 0.498
        },
        {
          "concept": "WMT",
          "relevance": 0.49
        },
        {
          "concept": "task",
          "relevance": 0.482
        },
        {
          "concept": "GPU",
          "relevance": 0.458
        },
        {
          "concept": "English constituencies",
          "relevance": 0.458
        },
        {
          "concept": "convolution",
          "relevance": 0.446
        },
        {
          "concept": "encoding",
          "relevance": 0.443
        },
        {
          "concept": "training",
          "relevance": 0.439
        },
        {
          "concept": "architecture",
          "relevance": 0.431
        },
        {
          "concept": "network",
          "relevance": 0.428
        },
        {
          "concept": "machine",
          "relevance": 0.419
        },
        {
          "concept": "transformation",
          "relevance": 0.402
        },
        {
          "concept": "model",
          "relevance": 0.397
        },
        {
          "concept": "ensemble",
          "relevance": 0.372
        },
        {
          "concept": "attention",
          "relevance": 0.352
        },
        {
          "concept": "cost",
          "relevance": 0.347
        },
        {
          "concept": "quality",
          "relevance": 0.327
        },
        {
          "concept": "experiments",
          "relevance": 0.319
        },
        {
          "concept": "configuration",
          "relevance": 0.306
        },
        {
          "concept": "data",
          "relevance": 0.304
        },
        {
          "concept": "results",
          "relevance": 0.293
        },
        {
          "concept": "literature",
          "relevance": 0.268
        },
        {
          "concept": "mechanism",
          "relevance": 0.258
        },
        {
          "concept": "constituents",
          "relevance": 0.222
        },
        {
          "concept": "scores",
          "relevance": 0.205
        },
        {
          "concept": "recurrence",
          "relevance": 0.154
        },
        {
          "concept": "fraction",
          "relevance": 0.14
        },
        {
          "concept": "days",
          "relevance": 0.135
        }
      ]
    },
    {
      "paperId": "pub.1070970326",
      "doi": "10.3102/00346543069001021",
      "title": "Effects of Small-Group Learning on Undergraduates in Science, Mathematics, Engineering, and Technology: A Meta-Analysis",
      "year": 1999,
      "citationCount": 1248,
      "fieldCitationRatio": NaN,
      "abstract": "Recent calls for instructional innovation in undergraduate science, mathematics, engineering, and technology (SMET) courses and programs highlight the need for a solid foundation of education research at the undergraduate level on which to base policy and practice. We report herein the results of a meta-analysis that integrates research on undergraduate SMET education since 1980. The meta-analysis demonstrates that various forms of small-group learning are effective in promoting greater academic achievement, more favorable attitudes toward learning, and increased persistence through SMET courses and programs. The magnitude of the effects reported in this study exceeds most findings in comparable reviews of research on educational innovations and supports more widespread implementation of small-group learning in undergraduate SMET.",
      "reference_ids": [
        "pub.1082782018",
        "pub.1070668093",
        "pub.1030768904",
        "pub.1006014437",
        "pub.1023896998",
        "pub.1062544930",
        "pub.1081930110",
        "pub.1031398866",
        "pub.1030877007",
        "pub.1070970248",
        "pub.1003798269",
        "pub.1005699063",
        "pub.1049787029",
        "pub.1070970288",
        "pub.1051553965",
        "pub.1028998850",
        "pub.1046640872",
        "pub.1070970226",
        "pub.1070966554",
        "pub.1038198563",
        "pub.1063768367",
        "pub.1062559987",
        "pub.1062650800",
        "pub.1001522768",
        "pub.1046157649",
        "pub.1000424423",
        "pub.1130485210",
        "pub.1040807702",
        "pub.1006901594",
        "pub.1008407883",
        "pub.1041764428",
        "pub.1022014671",
        "pub.1046949145",
        "pub.1041358499",
        "pub.1027663168",
        "pub.1109737965",
        "pub.1055459605",
        "pub.1015049222",
        "pub.1005877156"
      ],
      "concepts_scores": [
        {
          "concept": "small group learning",
          "relevance": 0.875
        },
        {
          "concept": "effectiveness of small group learning",
          "relevance": 0.753
        },
        {
          "concept": "instructional innovation",
          "relevance": 0.673
        },
        {
          "concept": "educational research",
          "relevance": 0.672
        },
        {
          "concept": "review of research",
          "relevance": 0.672
        },
        {
          "concept": "educational innovation",
          "relevance": 0.671
        },
        {
          "concept": "academic achievement",
          "relevance": 0.67
        },
        {
          "concept": "undergraduate science",
          "relevance": 0.668
        },
        {
          "concept": "undergraduate level",
          "relevance": 0.642
        },
        {
          "concept": "learning",
          "relevance": 0.631
        },
        {
          "concept": "mathematics",
          "relevance": 0.607
        },
        {
          "concept": "education",
          "relevance": 0.597
        },
        {
          "concept": "undergraduate",
          "relevance": 0.58
        },
        {
          "concept": "favorable attitudes",
          "relevance": 0.578
        },
        {
          "concept": "increased persistence",
          "relevance": 0.564
        },
        {
          "concept": "course",
          "relevance": 0.547
        },
        {
          "concept": "science",
          "relevance": 0.54
        },
        {
          "concept": "research",
          "relevance": 0.523
        },
        {
          "concept": "achievement",
          "relevance": 0.509
        },
        {
          "concept": "program",
          "relevance": 0.507
        },
        {
          "concept": "innovation",
          "relevance": 0.5
        },
        {
          "concept": "Smets",
          "relevance": 0.481
        },
        {
          "concept": "attitudes",
          "relevance": 0.468
        },
        {
          "concept": "widespread implementation",
          "relevance": 0.459
        },
        {
          "concept": "practice",
          "relevance": 0.454
        },
        {
          "concept": "technology",
          "relevance": 0.443
        },
        {
          "concept": "policy",
          "relevance": 0.436
        },
        {
          "concept": "base policy",
          "relevance": 0.42
        },
        {
          "concept": "meta-analysis",
          "relevance": 0.4
        },
        {
          "concept": "findings",
          "relevance": 0.388
        },
        {
          "concept": "engineering",
          "relevance": 0.387
        },
        {
          "concept": "study",
          "relevance": 0.33
        },
        {
          "concept": "persistence",
          "relevance": 0.328
        },
        {
          "concept": "levels",
          "relevance": 0.301
        },
        {
          "concept": "results",
          "relevance": 0.275
        },
        {
          "concept": "review",
          "relevance": 0.271
        },
        {
          "concept": "effect",
          "relevance": 0.267
        },
        {
          "concept": "magnitude",
          "relevance": 0.186
        }
      ]
    },
    {
      "paperId": "pub.1156602823",
      "doi": "10.1056/nejmsr2214184",
      "title": "Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine",
      "year": 2023,
      "citationCount": 1204,
      "fieldCitationRatio": NaN,
      "abstract": "GPT-4, a General AI Chatbot for Medicine Chatbots are computer programs with which one can have a conversation. In this article, the authors describe how the GPT-4 chatbot, which has been given a g...",
      "reference_ids": [
        "pub.1155270525",
        "pub.1150999098",
        "pub.1134322180",
        "pub.1145086372",
        "pub.1136777420",
        "pub.1100133641"
      ],
      "concepts_scores": [
        {
          "concept": "AI chatbots",
          "relevance": 0.555
        },
        {
          "concept": "chatbot",
          "relevance": 0.472
        },
        {
          "concept": "computer program",
          "relevance": 0.417
        },
        {
          "concept": "computer",
          "relevance": 0.359
        },
        {
          "concept": "generalization",
          "relevance": 0.313
        },
        {
          "concept": "program",
          "relevance": 0.276
        },
        {
          "concept": "article",
          "relevance": 0.273
        },
        {
          "concept": "authors",
          "relevance": 0.264
        },
        {
          "concept": "benefits",
          "relevance": 0.241
        },
        {
          "concept": "medicine",
          "relevance": 0.238
        },
        {
          "concept": "limitations",
          "relevance": 0.23
        },
        {
          "concept": "risk",
          "relevance": 0.208
        },
        {
          "concept": "conversion",
          "relevance": 0.191
        }
      ]
    },
    {
      "paperId": "pub.1155270525",
      "doi": "10.1371/journal.pdig.0000198",
      "title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models",
      "year": 2023,
      "citationCount": 2693,
      "fieldCitationRatio": 2145.24,
      "abstract": "We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.",
      "reference_ids": [
        "pub.1121025018",
        "pub.1100900092",
        "pub.1130542575",
        "pub.1117041738",
        "pub.1117939920",
        "pub.1136656239",
        "pub.1092352256",
        "pub.1127685771",
        "pub.1121129078",
        "pub.1045049575",
        "pub.1151451023",
        "pub.1113525484",
        "pub.1126027253",
        "pub.1139648327",
        "pub.1116881202",
        "pub.1169290908",
        "pub.1112585067",
        "pub.1033178586",
        "pub.1078412683",
        "pub.1129260878",
        "pub.1147955703",
        "pub.1093497718"
      ],
      "concepts_scores": [
        {
          "concept": "language model",
          "relevance": 0.682
        },
        {
          "concept": "ChatGPT",
          "relevance": 0.546
        },
        {
          "concept": "United States Medical Licensing Exam",
          "relevance": 0.536
        },
        {
          "concept": "medical education",
          "relevance": 0.476
        },
        {
          "concept": "language",
          "relevance": 0.445
        },
        {
          "concept": "clinical decision-making",
          "relevance": 0.428
        },
        {
          "concept": "Medical Licensing Exam",
          "relevance": 0.427
        },
        {
          "concept": "performance",
          "relevance": 0.424
        },
        {
          "concept": "decision-making",
          "relevance": 0.412
        },
        {
          "concept": "level of concordance",
          "relevance": 0.405
        },
        {
          "concept": "licensing exam",
          "relevance": 0.394
        },
        {
          "concept": "specialized training",
          "relevance": 0.392
        },
        {
          "concept": "model",
          "relevance": 0.37
        },
        {
          "concept": "training",
          "relevance": 0.363
        },
        {
          "concept": "education",
          "relevance": 0.34
        },
        {
          "concept": "exam",
          "relevance": 0.339
        },
        {
          "concept": "reinforcement",
          "relevance": 0.324
        },
        {
          "concept": "threshold",
          "relevance": 0.296
        },
        {
          "concept": "results",
          "relevance": 0.28
        },
        {
          "concept": "concordance",
          "relevance": 0.263
        },
        {
          "concept": "units",
          "relevance": 0.247
        },
        {
          "concept": "levels",
          "relevance": 0.216
        },
        {
          "concept": "potential",
          "relevance": 0.196
        }
      ]
    },
    {
      "paperId": "pub.1100133641",
      "doi": "10.1109/access.2017.2788044",
      "title": "Deep Learning Applications in Medical Image Analysis",
      "year": 2017,
      "citationCount": 1277,
      "fieldCitationRatio": 257.43,
      "abstract": "The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.",
      "reference_ids": [
        "pub.1085642448",
        "pub.1061696713",
        "pub.1061696712",
        "pub.1061798360",
        "pub.1061696680",
        "pub.1028715170",
        "pub.1061696710",
        "pub.1009611012",
        "pub.1023638731",
        "pub.1015376043",
        "pub.1044365820",
        "pub.1061696692",
        "pub.1085753273",
        "pub.1095401413",
        "pub.1006555326",
        "pub.1094552856",
        "pub.1061744581",
        "pub.1008345178",
        "pub.1090383886",
        "pub.1061696721",
        "pub.1095735220",
        "pub.1008016534",
        "pub.1018633844",
        "pub.1010719422",
        "pub.1086144283",
        "pub.1093359587",
        "pub.1017257287",
        "pub.1061696572",
        "pub.1092089127",
        "pub.1085595827",
        "pub.1094291017",
        "pub.1039374598",
        "pub.1061696747",
        "pub.1046426641",
        "pub.1029133537",
        "pub.1095848734",
        "pub.1009551752",
        "pub.1091427714",
        "pub.1090579939",
        "pub.1061170381",
        "pub.1034209570",
        "pub.1061696734",
        "pub.1091589576",
        "pub.1030890551",
        "pub.1084908686",
        "pub.1037811822",
        "pub.1031059013",
        "pub.1095714739",
        "pub.1093838265",
        "pub.1023070025",
        "pub.1095641255",
        "pub.1084718850",
        "pub.1014968475",
        "pub.1091574901",
        "pub.1061696716",
        "pub.1027043520",
        "pub.1084918352",
        "pub.1006217895",
        "pub.1028121501",
        "pub.1013664571",
        "pub.1078913727",
        "pub.1009469917",
        "pub.1085742825",
        "pub.1084921376",
        "pub.1084228312",
        "pub.1018367015",
        "pub.1090662020",
        "pub.1095321586",
        "pub.1091930620",
        "pub.1061744373",
        "pub.1061696434",
        "pub.1009767488",
        "pub.1099192756",
        "pub.1086506677",
        "pub.1061696701",
        "pub.1100060688",
        "pub.1005938513",
        "pub.1034980694",
        "pub.1031091687",
        "pub.1061696607",
        "pub.1093656300",
        "pub.1017774818",
        "pub.1074217286",
        "pub.1004707137",
        "pub.1061696689",
        "pub.1002436929",
        "pub.1061696703"
      ],
      "concepts_scores": [
        {
          "concept": "medical image analysis",
          "relevance": 0.792
        },
        {
          "concept": "machine learning algorithms",
          "relevance": 0.772
        },
        {
          "concept": "learning algorithms",
          "relevance": 0.728
        },
        {
          "concept": "era of medical big data",
          "relevance": 0.721
        },
        {
          "concept": "application of medical image classification",
          "relevance": 0.721
        },
        {
          "concept": "success of machine learning algorithms",
          "relevance": 0.716
        },
        {
          "concept": "hand-crafting of features",
          "relevance": 0.714
        },
        {
          "concept": "medical image classification",
          "relevance": 0.692
        },
        {
          "concept": "image recognition tasks",
          "relevance": 0.691
        },
        {
          "concept": "convolutional neural network",
          "relevance": 0.684
        },
        {
          "concept": "deep learning applications",
          "relevance": 0.683
        },
        {
          "concept": "medical big data",
          "relevance": 0.679
        },
        {
          "concept": "image classification",
          "relevance": 0.635
        },
        {
          "concept": "hand-crafted",
          "relevance": 0.631
        },
        {
          "concept": "learning applications",
          "relevance": 0.624
        },
        {
          "concept": "neural network",
          "relevance": 0.624
        },
        {
          "concept": "machine learning",
          "relevance": 0.618
        },
        {
          "concept": "big data",
          "relevance": 0.617
        },
        {
          "concept": "recognition task",
          "relevance": 0.611
        },
        {
          "concept": "image analysis",
          "relevance": 0.587
        },
        {
          "concept": "research area",
          "relevance": 0.566
        },
        {
          "concept": "algorithm",
          "relevance": 0.548
        },
        {
          "concept": "hierarchical relationships",
          "relevance": 0.541
        },
        {
          "concept": "electronic medical records",
          "relevance": 0.529
        },
        {
          "concept": "machine",
          "relevance": 0.526
        },
        {
          "concept": "research obstacles",
          "relevance": 0.499
        },
        {
          "concept": "Deep",
          "relevance": 0.479
        },
        {
          "concept": "images",
          "relevance": 0.476
        },
        {
          "concept": "network",
          "relevance": 0.465
        },
        {
          "concept": "applications",
          "relevance": 0.452
        },
        {
          "concept": "task",
          "relevance": 0.451
        },
        {
          "concept": "classification",
          "relevance": 0.448
        },
        {
          "concept": "learning",
          "relevance": 0.444
        },
        {
          "concept": "obstacles",
          "relevance": 0.416
        },
        {
          "concept": "features",
          "relevance": 0.399
        },
        {
          "concept": "detection",
          "relevance": 0.399
        },
        {
          "concept": "segments",
          "relevance": 0.391
        },
        {
          "concept": "registration",
          "relevance": 0.39
        },
        {
          "concept": "research",
          "relevance": 0.389
        },
        {
          "concept": "increased use",
          "relevance": 0.382
        },
        {
          "concept": "data",
          "relevance": 0.381
        },
        {
          "concept": "clinical aspects",
          "relevance": 0.373
        },
        {
          "concept": "medical records",
          "relevance": 0.37
        },
        {
          "concept": "diagnostic imaging",
          "relevance": 0.368
        },
        {
          "concept": "localization",
          "relevance": 0.347
        },
        {
          "concept": "era",
          "relevance": 0.328
        },
        {
          "concept": "aspects",
          "relevance": 0.319
        },
        {
          "concept": "success",
          "relevance": 0.318
        },
        {
          "concept": "time",
          "relevance": 0.315
        },
        {
          "concept": "direction",
          "relevance": 0.308
        },
        {
          "concept": "analysis",
          "relevance": 0.302
        },
        {
          "concept": "field",
          "relevance": 0.298
        },
        {
          "concept": "records",
          "relevance": 0.291
        },
        {
          "concept": "medication",
          "relevance": 0.286
        },
        {
          "concept": "area",
          "relevance": 0.264
        },
        {
          "concept": "trends",
          "relevance": 0.259
        },
        {
          "concept": "use",
          "relevance": 0.258
        },
        {
          "concept": "review",
          "relevance": 0.253
        },
        {
          "concept": "years",
          "relevance": 0.248
        },
        {
          "concept": "relationship",
          "relevance": 0.226
        }
      ]
    }
  ],
  "evolution_links": [
    {
      "source": "pub.1169225310",
      "target": "pub.1160635088",
      "source_title": "A Comprehensive Review on Synergy of Multi-Modal Data and AI Technologies in Medical Diagnosis",
      "target_title": "Large language models encode clinical knowledge"
    },
    {
      "source": "pub.1160635088",
      "target": "pub.1151332162",
      "source_title": "Large language models encode clinical knowledge",
      "target_title": "BioGPT: generative pre-trained transformer for biomedical text generation and mining"
    },
    {
      "source": "pub.1151332162",
      "target": "pub.1141942664",
      "source_title": "BioGPT: generative pre-trained transformer for biomedical text generation and mining",
      "target_title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"
    },
    {
      "source": "pub.1151332162",
      "target": "pub.1129120019",
      "source_title": "BioGPT: generative pre-trained transformer for biomedical text generation and mining",
      "target_title": "A Relation-Specific Attention Network for Joint Entity and Relation Extraction"
    },
    {
      "source": "pub.1160635088",
      "target": "pub.1152983179",
      "source_title": "Large language models encode clinical knowledge",
      "target_title": "PTR: Prompt Tuning with Rules for Text Classification"
    },
    {
      "source": "pub.1152983179",
      "target": "pub.1147477782",
      "source_title": "PTR: Prompt Tuning with Rules for Text Classification",
      "target_title": "KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction"
    },
    {
      "source": "pub.1152983179",
      "target": "pub.1150866022",
      "source_title": "PTR: Prompt Tuning with Rules for Text Classification",
      "target_title": "Re-TACRED: Addressing Shortcomings of the TACRED Dataset"
    },
    {
      "source": "pub.1169225310",
      "target": "pub.1160103012",
      "source_title": "A Comprehensive Review on Synergy of Multi-Modal Data and AI Technologies in Medical Diagnosis",
      "target_title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge"
    },
    {
      "source": "pub.1160103012",
      "target": "pub.1155222253",
      "source_title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
      "target_title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment"
    },
    {
      "source": "pub.1155222253",
      "target": "pub.1118769417",
      "source_title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
      "target_title": "Attention Is All You Need"
    },
    {
      "source": "pub.1155222253",
      "target": "pub.1070970326",
      "source_title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
      "target_title": "Effects of Small-Group Learning on Undergraduates in Science, Mathematics, Engineering, and Technology: A Meta-Analysis"
    },
    {
      "source": "pub.1160103012",
      "target": "pub.1156602823",
      "source_title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
      "target_title": "Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine"
    },
    {
      "source": "pub.1156602823",
      "target": "pub.1155270525",
      "source_title": "Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine",
      "target_title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models"
    },
    {
      "source": "pub.1156602823",
      "target": "pub.1100133641",
      "source_title": "Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine",
      "target_title": "Deep Learning Applications in Medical Image Analysis"
    }
  ]
}