{
  "papers": [
    {
      "paperId": "pub.1182518197",
      "doi": "10.1145/3703155",
      "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
      "year": 2025,
      "citationCount": 495,
      "fieldCitationRatio": NaN,
      "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
      "reference_ids": [
        "pub.1186383944",
        "pub.1148391290",
        "pub.1152843639",
        "pub.1163042122",
        "pub.1163045360",
        "pub.1149741351",
        "pub.1069895470",
        "pub.1122290267",
        "pub.1129756666",
        "pub.1142776490",
        "pub.1163991186",
        "pub.1141969457",
        "pub.1143949294",
        "pub.1160173550",
        "pub.1166873088",
        "pub.1144245142",
        "pub.1129757053",
        "pub.1166872435",
        "pub.1143949341",
        "pub.1166873227",
        "pub.1174225536",
        "pub.1099239594",
        "pub.1163045642",
        "pub.1138840568",
        "pub.1138840270",
        "pub.1166874022",
        "pub.1182671435",
        "pub.1117658905",
        "pub.1166874100",
        "pub.1133176931",
        "pub.1166874407",
        "pub.1182811361",
        "pub.1139947296",
        "pub.1135710434",
        "pub.1181666485",
        "pub.1154765531",
        "pub.1173700042",
        "pub.1163043980",
        "pub.1129757334",
        "pub.1168349615",
        "pub.1129756915",
        "pub.1167020587",
        "pub.1143949401",
        "pub.1163044873",
        "pub.1166874060",
        "pub.1119910017",
        "pub.1133175338",
        "pub.1166872382",
        "pub.1163043966",
        "pub.1174225490",
        "pub.1163043978",
        "pub.1142532386",
        "pub.1163045688",
        "pub.1121024805",
        "pub.1163991067",
        "pub.1166873752",
        "pub.1133174910",
        "pub.1168956513",
        "pub.1163991170",
        "pub.1121025172",
        "pub.1166872883",
        "pub.1166872814",
        "pub.1169640331",
        "pub.1109447476",
        "pub.1163044371",
        "pub.1139947961",
        "pub.1133177310",
        "pub.1133177268",
        "pub.1148391003",
        "pub.1163991041",
        "pub.1138840259",
        "pub.1133174463",
        "pub.1166873617",
        "pub.1148391063",
        "pub.1110304822",
        "pub.1121024938",
        "pub.1129756745",
        "pub.1171925604",
        "pub.1182671352",
        "pub.1166873128",
        "pub.1166874260",
        "pub.1182811606",
        "pub.1163043716",
        "pub.1186384386",
        "pub.1148390672",
        "pub.1163041656",
        "pub.1062842461",
        "pub.1163991400",
        "pub.1186383308",
        "pub.1163044326",
        "pub.1163990971",
        "pub.1166873181",
        "pub.1148391033",
        "pub.1166874087",
        "pub.1163045344",
        "pub.1148391058",
        "pub.1166072902",
        "pub.1122290348",
        "pub.1148391788",
        "pub.1175836875",
        "pub.1149741095",
        "pub.1166872914",
        "pub.1166874504",
        "pub.1182671916",
        "pub.1145454307",
        "pub.1182671976",
        "pub.1182813264",
        "pub.1143948840",
        "pub.1163202398",
        "pub.1166872908",
        "pub.1148391706",
        "pub.1143949301",
        "pub.1137573792",
        "pub.1129757057",
        "pub.1163041936",
        "pub.1166872510",
        "pub.1182811698",
        "pub.1186384413",
        "pub.1126002716",
        "pub.1143948900",
        "pub.1142776491",
        "pub.1167104767",
        "pub.1117659409",
        "pub.1123988166",
        "pub.1182813135",
        "pub.1133176850",
        "pub.1175835259",
        "pub.1172706643"
      ],
      "concepts_scores": [
        {
          "concept": "natural language processing",
          "relevance": 0.754
        },
        {
          "concept": "information retrieval",
          "relevance": 0.707
        },
        {
          "concept": "language model",
          "relevance": 0.701
        },
        {
          "concept": "vision-language models",
          "relevance": 0.662
        },
        {
          "concept": "task-specific models",
          "relevance": 0.649
        },
        {
          "concept": "IR systems",
          "relevance": 0.601
        },
        {
          "concept": "language processing",
          "relevance": 0.592
        },
        {
          "concept": "detection method",
          "relevance": 0.556
        },
        {
          "concept": "information acquisition",
          "relevance": 0.538
        },
        {
          "concept": "research directions",
          "relevance": 0.532
        },
        {
          "concept": "representative methodologies",
          "relevance": 0.523
        },
        {
          "concept": "knowledge boundaries",
          "relevance": 0.513
        },
        {
          "concept": "language",
          "relevance": 0.458
        },
        {
          "concept": "taxonomy",
          "relevance": 0.447
        },
        {
          "concept": "retrieval",
          "relevance": 0.443
        },
        {
          "concept": "LLM",
          "relevance": 0.439
        },
        {
          "concept": "benchmarks",
          "relevance": 0.439
        },
        {
          "concept": "paradigm shift",
          "relevance": 0.419
        },
        {
          "concept": "comprehensive overview",
          "relevance": 0.414
        },
        {
          "concept": "model",
          "relevance": 0.407
        },
        {
          "concept": "information",
          "relevance": 0.398
        },
        {
          "concept": "paradigm",
          "relevance": 0.392
        },
        {
          "concept": "attributes",
          "relevance": 0.382
        },
        {
          "concept": "open questions",
          "relevance": 0.376
        },
        {
          "concept": "research",
          "relevance": 0.371
        },
        {
          "concept": "overview",
          "relevance": 0.369
        },
        {
          "concept": "intensive research",
          "relevance": 0.363
        },
        {
          "concept": "system",
          "relevance": 0.36
        },
        {
          "concept": "reliability",
          "relevance": 0.356
        },
        {
          "concept": "acquisition",
          "relevance": 0.35
        },
        {
          "concept": "method",
          "relevance": 0.348
        },
        {
          "concept": "knowledge",
          "relevance": 0.345
        },
        {
          "concept": "methodology",
          "relevance": 0.344
        },
        {
          "concept": "hallucinations",
          "relevance": 0.343
        },
        {
          "concept": "challenges",
          "relevance": 0.339
        },
        {
          "concept": "era",
          "relevance": 0.313
        },
        {
          "concept": "principles",
          "relevance": 0.312
        },
        {
          "concept": "process",
          "relevance": 0.312
        },
        {
          "concept": "direction",
          "relevance": 0.294
        },
        {
          "concept": "limitations",
          "relevance": 0.283
        },
        {
          "concept": "survey",
          "relevance": 0.274
        },
        {
          "concept": "boundaries",
          "relevance": 0.271
        },
        {
          "concept": "concerns",
          "relevance": 0.27
        },
        {
          "concept": "questions",
          "relevance": 0.253
        },
        {
          "concept": "content",
          "relevance": 0.247
        },
        {
          "concept": "emergency",
          "relevance": 0.242
        },
        {
          "concept": "divergence",
          "relevance": 0.237
        },
        {
          "concept": "phenomenon",
          "relevance": 0.225
        },
        {
          "concept": "urgency",
          "relevance": 0.214
        },
        {
          "concept": "shift",
          "relevance": 0.197
        },
        {
          "concept": "factors",
          "relevance": 0.185
        }
      ]
    },
    {
      "paperId": "pub.1173700042",
      "doi": "10.1145/3626772.3657834",
      "title": "The Power of Noise: Redefining Retrieval for RAG Systems",
      "year": 2024,
      "citationCount": 81,
      "fieldCitationRatio": 0.0,
      "abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first comprehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages included in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever's highest-scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to 35%. These results highlight the need to investigate the appropriate strategies when integrating retrieval with LLMs, thereby laying the groundwork for future research in this area.",
      "reference_ids": [
        "pub.1117659354",
        "pub.1068001284",
        "pub.1149311485",
        "pub.1160811602",
        "pub.1166265880",
        "pub.1166873295",
        "pub.1139621687",
        "pub.1148391832",
        "pub.1168999548",
        "pub.1148391035",
        "pub.1120096978",
        "pub.1100517190",
        "pub.1166072902",
        "pub.1129670253"
      ],
      "concepts_scores": [
        {
          "concept": "information retrieval",
          "relevance": 0.692
        },
        {
          "concept": "random documents",
          "relevance": 0.603
        },
        {
          "concept": "power of noise",
          "relevance": 0.598
        },
        {
          "concept": "enterprise setting",
          "relevance": 0.594
        },
        {
          "concept": "language model",
          "relevance": 0.593
        },
        {
          "concept": "IR systems",
          "relevance": 0.588
        },
        {
          "concept": "AI solutions",
          "relevance": 0.583
        },
        {
          "concept": "retrieval component",
          "relevance": 0.577
        },
        {
          "concept": "retrieval strategy",
          "relevance": 0.547
        },
        {
          "concept": "research community",
          "relevance": 0.545
        },
        {
          "concept": "retrieval",
          "relevance": 0.541
        },
        {
          "concept": "documents",
          "relevance": 0.475
        },
        {
          "concept": "query",
          "relevance": 0.465
        },
        {
          "concept": "relevant passages",
          "relevance": 0.446
        },
        {
          "concept": "system",
          "relevance": 0.436
        },
        {
          "concept": "LLM",
          "relevance": 0.426
        },
        {
          "concept": "increasing attention",
          "relevance": 0.415
        },
        {
          "concept": "noise",
          "relevance": 0.411
        },
        {
          "concept": "accuracy",
          "relevance": 0.403
        },
        {
          "concept": "enterprises",
          "relevance": 0.395
        },
        {
          "concept": "knowledge",
          "relevance": 0.392
        },
        {
          "concept": "information",
          "relevance": 0.39
        },
        {
          "concept": "language",
          "relevance": 0.387
        },
        {
          "concept": "solution",
          "relevance": 0.384
        },
        {
          "concept": "domain",
          "relevance": 0.367
        },
        {
          "concept": "research",
          "relevance": 0.364
        },
        {
          "concept": "sets",
          "relevance": 0.351
        },
        {
          "concept": "strategies",
          "relevance": 0.342
        },
        {
          "concept": "method",
          "relevance": 0.341
        },
        {
          "concept": "prompts",
          "relevance": 0.335
        },
        {
          "concept": "power",
          "relevance": 0.325
        },
        {
          "concept": "model",
          "relevance": 0.322
        },
        {
          "concept": "context",
          "relevance": 0.32
        },
        {
          "concept": "attention",
          "relevance": 0.308
        },
        {
          "concept": "generation",
          "relevance": 0.302
        },
        {
          "concept": "results",
          "relevance": 0.297
        },
        {
          "concept": "multiple factors",
          "relevance": 0.29
        },
        {
          "concept": "position",
          "relevance": 0.277
        },
        {
          "concept": "relevance",
          "relevance": 0.266
        },
        {
          "concept": "community",
          "relevance": 0.25
        },
        {
          "concept": "passage",
          "relevance": 0.248
        },
        {
          "concept": "area",
          "relevance": 0.246
        },
        {
          "concept": "analysis",
          "relevance": 0.244
        },
        {
          "concept": "systematic examination",
          "relevance": 0.23
        },
        {
          "concept": "findings",
          "relevance": 0.2
        },
        {
          "concept": "effect",
          "relevance": 0.193
        },
        {
          "concept": "factors",
          "relevance": 0.181
        },
        {
          "concept": "examination",
          "relevance": 0.148
        }
      ]
    },
    {
      "paperId": "pub.1120096978",
      "doi": "10.1162/tacl_a_00276",
      "title": "Natural Questions: A Benchmark for Question Answering Research",
      "year": 2019,
      "citationCount": 849,
      "fieldCitationRatio": 185.65,
      "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
      "reference_ids": [
        "pub.1099106215",
        "pub.1096025508",
        "pub.1044999643",
        "pub.1117659823",
        "pub.1096025530",
        "pub.1099113580",
        "pub.1104321191",
        "pub.1117658905",
        "pub.1099106053",
        "pub.1098653816",
        "pub.1115977877",
        "pub.1098653813",
        "pub.1117659405",
        "pub.1106022887",
        "pub.1098653837",
        "pub.1117659708",
        "pub.1100516767",
        "pub.1117658887",
        "pub.1117658906",
        "pub.1100516900",
        "pub.1099239594"
      ],
      "concepts_scores": [
        {
          "concept": "question answering system",
          "relevance": 0.687
        },
        {
          "concept": "human upper bound",
          "relevance": 0.672
        },
        {
          "concept": "aggregate queries",
          "relevance": 0.64
        },
        {
          "concept": "training examples",
          "relevance": 0.636
        },
        {
          "concept": "answering system",
          "relevance": 0.636
        },
        {
          "concept": "annotation task",
          "relevance": 0.634
        },
        {
          "concept": "question corpus",
          "relevance": 0.633
        },
        {
          "concept": "Wikipedia pages",
          "relevance": 0.626
        },
        {
          "concept": "search engines",
          "relevance": 0.608
        },
        {
          "concept": "competitive methods",
          "relevance": 0.586
        },
        {
          "concept": "Google search engine",
          "relevance": 0.583
        },
        {
          "concept": "robustness metrics",
          "relevance": 0.572
        },
        {
          "concept": "annotation",
          "relevance": 0.569
        },
        {
          "concept": "baseline results",
          "relevance": 0.545
        },
        {
          "concept": "upper bound",
          "relevance": 0.538
        },
        {
          "concept": "public release",
          "relevance": 0.528
        },
        {
          "concept": "metrics",
          "relevance": 0.525
        },
        {
          "concept": "test data",
          "relevance": 0.52
        },
        {
          "concept": "pages",
          "relevance": 0.517
        },
        {
          "concept": "development data",
          "relevance": 0.515
        },
        {
          "concept": "query",
          "relevance": 0.494
        },
        {
          "concept": "Wikipedia",
          "relevance": 0.493
        },
        {
          "concept": "benchmarks",
          "relevance": 0.457
        },
        {
          "concept": "task",
          "relevance": 0.448
        },
        {
          "concept": "examples",
          "relevance": 0.44
        },
        {
          "concept": "corpus",
          "relevance": 0.425
        },
        {
          "concept": "bounds",
          "relevance": 0.418
        },
        {
          "concept": "training",
          "relevance": 0.408
        },
        {
          "concept": "Google",
          "relevance": 0.407
        },
        {
          "concept": "data",
          "relevance": 0.404
        },
        {
          "concept": "answers",
          "relevance": 0.385
        },
        {
          "concept": "human variability",
          "relevance": 0.38
        },
        {
          "concept": "system",
          "relevance": 0.375
        },
        {
          "concept": "engineering",
          "relevance": 0.374
        },
        {
          "concept": "results",
          "relevance": 0.365
        },
        {
          "concept": "method",
          "relevance": 0.362
        },
        {
          "concept": "quality",
          "relevance": 0.352
        },
        {
          "concept": "experiments",
          "relevance": 0.343
        },
        {
          "concept": "research",
          "relevance": 0.334
        },
        {
          "concept": "purposes",
          "relevance": 0.327
        },
        {
          "concept": "questions",
          "relevance": 0.303
        },
        {
          "concept": "literature",
          "relevance": 0.286
        },
        {
          "concept": "development",
          "relevance": 0.281
        },
        {
          "concept": "analysis",
          "relevance": 0.259
        },
        {
          "concept": "test",
          "relevance": 0.257
        },
        {
          "concept": "baseline",
          "relevance": 0.254
        },
        {
          "concept": "variables",
          "relevance": 0.232
        },
        {
          "concept": "release",
          "relevance": 0.167
        }
      ]
    },
    {
      "paperId": "pub.1099239594",
      "doi": "10.3115/1073083.1073135",
      "title": "BLEU: a method for automatic evaluation of machine translation",
      "year": 2001,
      "citationCount": 11516,
      "fieldCitationRatio": 3629.45,
      "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.",
      "reference_ids": NaN,
      "concepts_scores": [
        {
          "concept": "evaluation of machine translation",
          "relevance": 0.757
        },
        {
          "concept": "human evaluation",
          "relevance": 0.713
        },
        {
          "concept": "machine translation",
          "relevance": 0.682
        },
        {
          "concept": "automatic evaluation of machine translation",
          "relevance": 0.667
        },
        {
          "concept": "human evaluation of machine translation",
          "relevance": 0.649
        },
        {
          "concept": "machine translation evaluation",
          "relevance": 0.641
        },
        {
          "concept": "language-independent",
          "relevance": 0.582
        },
        {
          "concept": "automatic evaluation",
          "relevance": 0.571
        },
        {
          "concept": "human judges",
          "relevance": 0.568
        },
        {
          "concept": "translation evaluation",
          "relevance": 0.555
        },
        {
          "concept": "human labor",
          "relevance": 0.505
        },
        {
          "concept": "translation",
          "relevance": 0.474
        },
        {
          "concept": "evaluation",
          "relevance": 0.418
        },
        {
          "concept": "method",
          "relevance": 0.41
        },
        {
          "concept": "understudies",
          "relevance": 0.407
        },
        {
          "concept": "cost",
          "relevance": 0.349
        },
        {
          "concept": "judges",
          "relevance": 0.333
        },
        {
          "concept": "frequent evaluation",
          "relevance": 0.307
        },
        {
          "concept": "labor",
          "relevance": 0.302
        },
        {
          "concept": "months",
          "relevance": 0.249
        },
        {
          "concept": "marginal cost",
          "relevance": 0.227
        }
      ]
    },
    {
      "paperId": "pub.1115977877",
      "doi": "10.1162/tacl_a_00266",
      "title": "CoQA: A Conversational Question Answering Challenge",
      "year": 2019,
      "citationCount": 427,
      "fieldCitationRatio": 104.44,
      "abstract": "Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa .",
      "reference_ids": [
        "pub.1104321612",
        "pub.1106023141",
        "pub.1100517021",
        "pub.1096025521",
        "pub.1096025551",
        "pub.1098653802",
        "pub.1117659387",
        "pub.1100516767",
        "pub.1117658780",
        "pub.1099115153",
        "pub.1096025897",
        "pub.1104321149",
        "pub.1096025530",
        "pub.1117659404",
        "pub.1104321113",
        "pub.1117659532",
        "pub.1099150709",
        "pub.1096025097",
        "pub.1096025331",
        "pub.1099113663",
        "pub.1117658887",
        "pub.1113540457",
        "pub.1117658722",
        "pub.1117659708",
        "pub.1110957692",
        "pub.1106022887",
        "pub.1100517171",
        "pub.1099110523",
        "pub.1094434189",
        "pub.1117658879",
        "pub.1098653837",
        "pub.1099127825",
        "pub.1096025508",
        "pub.1117659409"
      ],
      "concepts_scores": [
        {
          "concept": "question answering system",
          "relevance": 0.65
        },
        {
          "concept": "reading comprehension dataset",
          "relevance": 0.65
        },
        {
          "concept": "free-form text",
          "relevance": 0.645
        },
        {
          "concept": "reading comprehension model",
          "relevance": 0.635
        },
        {
          "concept": "conversational questions",
          "relevance": 0.631
        },
        {
          "concept": "answering system",
          "relevance": 0.602
        },
        {
          "concept": "F1 score",
          "relevance": 0.591
        },
        {
          "concept": "CoQA",
          "relevance": 0.581
        },
        {
          "concept": "diverse domains",
          "relevance": 0.576
        },
        {
          "concept": "human performance",
          "relevance": 0.556
        },
        {
          "concept": "dataset",
          "relevance": 0.538
        },
        {
          "concept": "information gathering",
          "relevance": 0.533
        },
        {
          "concept": "text passages",
          "relevance": 0.525
        },
        {
          "concept": "information",
          "relevance": 0.454
        },
        {
          "concept": "text",
          "relevance": 0.452
        },
        {
          "concept": "answers",
          "relevance": 0.443
        },
        {
          "concept": "machine",
          "relevance": 0.427
        },
        {
          "concept": "stronger dialogue",
          "relevance": 0.427
        },
        {
          "concept": "comprehensive dataset",
          "relevance": 0.426
        },
        {
          "concept": "interconnected questions",
          "relevance": 0.411
        },
        {
          "concept": "system",
          "relevance": 0.41
        },
        {
          "concept": "performance",
          "relevance": 0.389
        },
        {
          "concept": "comprehensive model",
          "relevance": 0.387
        },
        {
          "concept": "gathering",
          "relevance": 0.381
        },
        {
          "concept": "questions",
          "relevance": 0.369
        },
        {
          "concept": "domain",
          "relevance": 0.369
        },
        {
          "concept": "passage",
          "relevance": 0.346
        },
        {
          "concept": "reading",
          "relevance": 0.344
        },
        {
          "concept": "dialogue",
          "relevance": 0.339
        },
        {
          "concept": "model",
          "relevance": 0.324
        },
        {
          "concept": "conversion",
          "relevance": 0.312
        },
        {
          "concept": "improvement",
          "relevance": 0.312
        },
        {
          "concept": "community",
          "relevance": 0.269
        },
        {
          "concept": "humans",
          "relevance": 0.264
        },
        {
          "concept": "phenomenon",
          "relevance": 0.264
        },
        {
          "concept": "depth",
          "relevance": 0.238
        },
        {
          "concept": "scores",
          "relevance": 0.232
        },
        {
          "concept": "evidence",
          "relevance": 0.21
        }
      ]
    },
    {
      "paperId": "pub.1129670253",
      "doi": "10.1145/3397271.3401075",
      "title": "ColBERT",
      "year": 2020,
      "citationCount": 662,
      "fieldCitationRatio": 181.83,
      "abstract": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.",
      "reference_ids": [
        "pub.1107892475",
        "pub.1086033413",
        "pub.1116851502",
        "pub.1050758935",
        "pub.1107775063",
        "pub.1052509117",
        "pub.1100885466",
        "pub.1118155230",
        "pub.1118155203",
        "pub.1122290369",
        "pub.1040711396",
        "pub.1091081973"
      ],
      "concepts_scores": [
        {
          "concept": "deep language models",
          "relevance": 0.823
        },
        {
          "concept": "natural language understanding",
          "relevance": 0.796
        },
        {
          "concept": "language model",
          "relevance": 0.787
        },
        {
          "concept": "information retrieval",
          "relevance": 0.738
        },
        {
          "concept": "query-document pairs",
          "relevance": 0.693
        },
        {
          "concept": "BERT-based models",
          "relevance": 0.682
        },
        {
          "concept": "end-to-end",
          "relevance": 0.674
        },
        {
          "concept": "massive neural networks",
          "relevance": 0.668
        },
        {
          "concept": "end-to-end retrievals",
          "relevance": 0.658
        },
        {
          "concept": "document ranking",
          "relevance": 0.641
        },
        {
          "concept": "document representation",
          "relevance": 0.641
        },
        {
          "concept": "query processing",
          "relevance": 0.641
        },
        {
          "concept": "search datasets",
          "relevance": 0.639
        },
        {
          "concept": "efficient retrieval",
          "relevance": 0.631
        },
        {
          "concept": "language understanding",
          "relevance": 0.629
        },
        {
          "concept": "ranking models",
          "relevance": 0.627
        },
        {
          "concept": "neural network",
          "relevance": 0.622
        },
        {
          "concept": "interaction architecture",
          "relevance": 0.61
        },
        {
          "concept": "computational cost",
          "relevance": 0.603
        },
        {
          "concept": "query",
          "relevance": 0.603
        },
        {
          "concept": "interaction steps",
          "relevance": 0.564
        },
        {
          "concept": "retrieval",
          "relevance": 0.562
        },
        {
          "concept": "orders-of-magnitude",
          "relevance": 0.549
        },
        {
          "concept": "documents",
          "relevance": 0.506
        },
        {
          "concept": "BERT",
          "relevance": 0.497
        },
        {
          "concept": "rank",
          "relevance": 0.471
        },
        {
          "concept": "dataset",
          "relevance": 0.469
        },
        {
          "concept": "architecture",
          "relevance": 0.466
        },
        {
          "concept": "network",
          "relevance": 0.463
        },
        {
          "concept": "flop",
          "relevance": 0.463
        },
        {
          "concept": "representation",
          "relevance": 0.439
        },
        {
          "concept": "model",
          "relevance": 0.425
        },
        {
          "concept": "information",
          "relevance": 0.416
        },
        {
          "concept": "similarity",
          "relevance": 0.376
        },
        {
          "concept": "cost",
          "relevance": 0.375
        },
        {
          "concept": "interaction mechanism",
          "relevance": 0.36
        },
        {
          "concept": "steps",
          "relevance": 0.345
        },
        {
          "concept": "process",
          "relevance": 0.325
        },
        {
          "concept": "pairs",
          "relevance": 0.32
        },
        {
          "concept": "results",
          "relevance": 0.316
        },
        {
          "concept": "interaction",
          "relevance": 0.309
        },
        {
          "concept": "understanding",
          "relevance": 0.29
        },
        {
          "concept": "Colbert",
          "relevance": 0.272
        },
        {
          "concept": "nature",
          "relevance": 0.268
        },
        {
          "concept": "mechanism",
          "relevance": 0.241
        },
        {
          "concept": "index",
          "relevance": 0.232
        },
        {
          "concept": "expression",
          "relevance": 0.23
        },
        {
          "concept": "effect",
          "relevance": 0.206
        },
        {
          "concept": "progression",
          "relevance": 0.204
        },
        {
          "concept": "magnitude",
          "relevance": 0.203
        },
        {
          "concept": "passage",
          "relevance": 0.163
        },
        {
          "concept": "approach",
          "relevance": 0.133
        }
      ]
    },
    {
      "paperId": "pub.1052509117",
      "doi": "10.1145/2505515.2505665",
      "title": "Learning deep structured semantic models for web search using clickthrough data",
      "year": 2013,
      "citationCount": 1368,
      "fieldCitationRatio": 363.47,
      "abstract": "Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.",
      "reference_ids": [
        "pub.1093660189",
        "pub.1005107725",
        "pub.1038052886",
        "pub.1061744364",
        "pub.1015128878",
        "pub.1042072205",
        "pub.1034987360",
        "pub.1061423808",
        "pub.1012153938",
        "pub.1068001401",
        "pub.1061516742",
        "pub.1140718785",
        "pub.1094071566",
        "pub.1026287512",
        "pub.1110614954",
        "pub.1023914772",
        "pub.1044685375",
        "pub.1025364122",
        "pub.1093343850",
        "pub.1061423108"
      ],
      "concepts_scores": [
        {
          "concept": "latent semantic models",
          "relevance": 0.837
        },
        {
          "concept": "deep structured semantic model",
          "relevance": 0.814
        },
        {
          "concept": "semantic model",
          "relevance": 0.775
        },
        {
          "concept": "clickthrough data",
          "relevance": 0.741
        },
        {
          "concept": "Web document ranking task",
          "relevance": 0.709
        },
        {
          "concept": "document ranking task",
          "relevance": 0.69
        },
        {
          "concept": "web search applications",
          "relevance": 0.689
        },
        {
          "concept": "low-dimensional space",
          "relevance": 0.673
        },
        {
          "concept": "real-world data",
          "relevance": 0.652
        },
        {
          "concept": "clicked documents",
          "relevance": 0.639
        },
        {
          "concept": "projection queries",
          "relevance": 0.637
        },
        {
          "concept": "search applications",
          "relevance": 0.629
        },
        {
          "concept": "web search",
          "relevance": 0.624
        },
        {
          "concept": "ranking tasks",
          "relevance": 0.615
        },
        {
          "concept": "query",
          "relevance": 0.612
        },
        {
          "concept": "semantic level",
          "relevance": 0.609
        },
        {
          "concept": "Web",
          "relevance": 0.531
        },
        {
          "concept": "task",
          "relevance": 0.519
        },
        {
          "concept": "conditional likelihood",
          "relevance": 0.514
        },
        {
          "concept": "documents",
          "relevance": 0.513
        },
        {
          "concept": "hash",
          "relevance": 0.499
        },
        {
          "concept": "clickthrough",
          "relevance": 0.493
        },
        {
          "concept": "model",
          "relevance": 0.428
        },
        {
          "concept": "LSA",
          "relevance": 0.423
        },
        {
          "concept": "deep structure",
          "relevance": 0.421
        },
        {
          "concept": "performance",
          "relevance": 0.412
        },
        {
          "concept": "search",
          "relevance": 0.403
        },
        {
          "concept": "data",
          "relevance": 0.397
        },
        {
          "concept": "vocabulary",
          "relevance": 0.394
        },
        {
          "concept": "applications",
          "relevance": 0.388
        },
        {
          "concept": "words",
          "relevance": 0.384
        },
        {
          "concept": "technique",
          "relevance": 0.368
        },
        {
          "concept": "space",
          "relevance": 0.364
        },
        {
          "concept": "project",
          "relevance": 0.355
        },
        {
          "concept": "distance",
          "relevance": 0.351
        },
        {
          "concept": "results",
          "relevance": 0.315
        },
        {
          "concept": "relevance",
          "relevance": 0.283
        },
        {
          "concept": "likelihood",
          "relevance": 0.283
        },
        {
          "concept": "structure",
          "relevance": 0.262
        },
        {
          "concept": "levels",
          "relevance": 0.209
        },
        {
          "concept": "study",
          "relevance": 0.187
        }
      ]
    },
    {
      "paperId": "pub.1050758935",
      "doi": "10.1145/2983323.2983769",
      "title": "A Deep Relevance Matching Model for Ad-hoc Retrieval",
      "year": 2016,
      "citationCount": 504,
      "fieldCitationRatio": 112.74,
      "abstract": "In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.",
      "reference_ids": [
        "pub.1025364122",
        "pub.1010393599",
        "pub.1016180918",
        "pub.1051915812",
        "pub.1099110523",
        "pub.1008453260",
        "pub.1052509117",
        "pub.1148917369",
        "pub.1019714596",
        "pub.1011865814",
        "pub.1040868287",
        "pub.1024360504",
        "pub.1061423808",
        "pub.1099115014",
        "pub.1018367015",
        "pub.1099110441",
        "pub.1012309215",
        "pub.1003671352",
        "pub.1017494025"
      ],
      "concepts_scores": [
        {
          "concept": "deep relevance matching model",
          "relevance": 0.87
        },
        {
          "concept": "natural language processing",
          "relevance": 0.832
        },
        {
          "concept": "ad hoc retrieval",
          "relevance": 0.806
        },
        {
          "concept": "relevance matching model",
          "relevance": 0.802
        },
        {
          "concept": "retrieval tasks",
          "relevance": 0.786
        },
        {
          "concept": "deep models",
          "relevance": 0.779
        },
        {
          "concept": "relevant matches",
          "relevance": 0.762
        },
        {
          "concept": "deep matching model",
          "relevance": 0.697
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.69
        },
        {
          "concept": "deep neural networks",
          "relevance": 0.689
        },
        {
          "concept": "matching model",
          "relevance": 0.654
        },
        {
          "concept": "NLP tasks",
          "relevance": 0.642
        },
        {
          "concept": "paraphrase identification",
          "relevance": 0.642
        },
        {
          "concept": "deep architecture",
          "relevance": 0.642
        },
        {
          "concept": "computer vision",
          "relevance": 0.64
        },
        {
          "concept": "benchmark collections",
          "relevance": 0.635
        },
        {
          "concept": "semantic matching",
          "relevance": 0.634
        },
        {
          "concept": "gating network",
          "relevance": 0.632
        },
        {
          "concept": "speech recognition",
          "relevance": 0.631
        },
        {
          "concept": "histogram mapping",
          "relevance": 0.627
        },
        {
          "concept": "retrieval model",
          "relevance": 0.626
        },
        {
          "concept": "neural network",
          "relevance": 0.625
        },
        {
          "concept": "language processing",
          "relevance": 0.622
        },
        {
          "concept": "automatic conversion",
          "relevance": 0.61
        },
        {
          "concept": "term importance",
          "relevance": 0.6
        },
        {
          "concept": "ad hoc",
          "relevance": 0.596
        },
        {
          "concept": "term level",
          "relevance": 0.583
        },
        {
          "concept": "retrieval",
          "relevance": 0.581
        },
        {
          "concept": "query",
          "relevance": 0.578
        },
        {
          "concept": "experimental results",
          "relevance": 0.568
        },
        {
          "concept": "network",
          "relevance": 0.566
        },
        {
          "concept": "task",
          "relevance": 0.565
        },
        {
          "concept": "matching task",
          "relevance": 0.539
        },
        {
          "concept": "matching network",
          "relevance": 0.539
        },
        {
          "concept": "matching",
          "relevance": 0.523
        },
        {
          "concept": "matching requirements",
          "relevance": 0.494
        },
        {
          "concept": "Deep",
          "relevance": 0.48
        },
        {
          "concept": "architecture",
          "relevance": 0.469
        },
        {
          "concept": "histogram",
          "relevance": 0.466
        },
        {
          "concept": "computer",
          "relevance": 0.465
        },
        {
          "concept": "benchmarks",
          "relevance": 0.461
        },
        {
          "concept": "paraphrasing",
          "relevance": 0.442
        },
        {
          "concept": "recognition",
          "relevance": 0.434
        },
        {
          "concept": "model",
          "relevance": 0.431
        },
        {
          "concept": "vision",
          "relevance": 0.427
        },
        {
          "concept": "speech",
          "relevance": 0.413
        },
        {
          "concept": "requirements",
          "relevance": 0.413
        },
        {
          "concept": "maps",
          "relevance": 0.401
        },
        {
          "concept": "collection",
          "relevance": 0.369
        },
        {
          "concept": "signal",
          "relevance": 0.366
        },
        {
          "concept": "matching factors",
          "relevance": 0.358
        },
        {
          "concept": "relevance",
          "relevance": 0.353
        },
        {
          "concept": "handling",
          "relevance": 0.35
        },
        {
          "concept": "process",
          "relevance": 0.327
        },
        {
          "concept": "results",
          "relevance": 0.318
        },
        {
          "concept": "identification",
          "relevance": 0.309
        },
        {
          "concept": "breakthrough",
          "relevance": 0.284
        },
        {
          "concept": "positive results",
          "relevance": 0.283
        },
        {
          "concept": "gate",
          "relevance": 0.279
        },
        {
          "concept": "characteristics",
          "relevance": 0.264
        },
        {
          "concept": "importance",
          "relevance": 0.228
        },
        {
          "concept": "conversion",
          "relevance": 0.213
        },
        {
          "concept": "levels",
          "relevance": 0.21
        },
        {
          "concept": "years",
          "relevance": 0.196
        },
        {
          "concept": "factors",
          "relevance": 0.194
        },
        {
          "concept": "feeding",
          "relevance": 0.146
        },
        {
          "concept": "problem",
          "relevance": 0.138
        }
      ]
    },
    {
      "paperId": "pub.1172706643",
      "doi": "10.1145/3644815.3644945",
      "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
      "year": 2024,
      "citationCount": 51,
      "fieldCitationRatio": 0.0,
      "abstract": "Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.",
      "reference_ids": [
        "pub.1132450500",
        "pub.1160800776",
        "pub.1156525082",
        "pub.1154787683",
        "pub.1132678494",
        "pub.1150325217",
        "pub.1160719363"
      ],
      "concepts_scores": [
        {
          "concept": "software engineering community",
          "relevance": 0.699
        },
        {
          "concept": "semantic search capabilities",
          "relevance": 0.697
        },
        {
          "concept": "information retrieval systems",
          "relevance": 0.696
        },
        {
          "concept": "software engineering",
          "relevance": 0.646
        },
        {
          "concept": "annotated documents",
          "relevance": 0.64
        },
        {
          "concept": "retrieval system",
          "relevance": 0.64
        },
        {
          "concept": "potential research directions",
          "relevance": 0.639
        },
        {
          "concept": "language model",
          "relevance": 0.638
        },
        {
          "concept": "search capability",
          "relevance": 0.632
        },
        {
          "concept": "meta-data",
          "relevance": 0.628
        },
        {
          "concept": "experience report",
          "relevance": 0.575
        },
        {
          "concept": "engineering community",
          "relevance": 0.564
        },
        {
          "concept": "research directions",
          "relevance": 0.561
        },
        {
          "concept": "software",
          "relevance": 0.537
        },
        {
          "concept": "failure point",
          "relevance": 0.508
        },
        {
          "concept": "ChatGPT",
          "relevance": 0.502
        },
        {
          "concept": "query",
          "relevance": 0.501
        },
        {
          "concept": "documents",
          "relevance": 0.486
        },
        {
          "concept": "case study",
          "relevance": 0.482
        },
        {
          "concept": "system",
          "relevance": 0.474
        },
        {
          "concept": "retrieval",
          "relevance": 0.466
        },
        {
          "concept": "LLM",
          "relevance": 0.458
        },
        {
          "concept": "robustness",
          "relevance": 0.45
        },
        {
          "concept": "capability",
          "relevance": 0.42
        },
        {
          "concept": "language",
          "relevance": 0.417
        },
        {
          "concept": "domain",
          "relevance": 0.395
        },
        {
          "concept": "applications",
          "relevance": 0.393
        },
        {
          "concept": "point",
          "relevance": 0.387
        },
        {
          "concept": "operation",
          "relevance": 0.381
        },
        {
          "concept": "engineering",
          "relevance": 0.379
        },
        {
          "concept": "augmented generation",
          "relevance": 0.369
        },
        {
          "concept": "experiments",
          "relevance": 0.348
        },
        {
          "concept": "model",
          "relevance": 0.346
        },
        {
          "concept": "research",
          "relevance": 0.338
        },
        {
          "concept": "validity",
          "relevance": 0.337
        },
        {
          "concept": "generation",
          "relevance": 0.325
        },
        {
          "concept": "lessons",
          "relevance": 0.324
        },
        {
          "concept": "direction",
          "relevance": 0.309
        },
        {
          "concept": "limitations",
          "relevance": 0.298
        },
        {
          "concept": "failure",
          "relevance": 0.277
        },
        {
          "concept": "community",
          "relevance": 0.269
        },
        {
          "concept": "cases",
          "relevance": 0.264
        },
        {
          "concept": "education",
          "relevance": 0.215
        },
        {
          "concept": "response",
          "relevance": 0.197
        },
        {
          "concept": "study",
          "relevance": 0.189
        },
        {
          "concept": "reports",
          "relevance": 0.181
        },
        {
          "concept": "problem",
          "relevance": 0.159
        }
      ]
    },
    {
      "paperId": "pub.1154787683",
      "doi": "10.1162/tacl_a_00530",
      "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
      "year": 2023,
      "citationCount": 150,
      "fieldCitationRatio": 91.16,
      "abstract": "Abstract\n                  Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work’s credibility and technical consistency.",
      "reference_ids": [
        "pub.1007668921",
        "pub.1148390680",
        "pub.1116884455",
        "pub.1143223853",
        "pub.1139947575",
        "pub.1133177199",
        "pub.1133174687",
        "pub.1133174463",
        "pub.1142776650",
        "pub.1141818682",
        "pub.1098653837",
        "pub.1148391060",
        "pub.1096025530",
        "pub.1096025331",
        "pub.1099237519",
        "pub.1144245142",
        "pub.1121024848",
        "pub.1129757334",
        "pub.1120096978",
        "pub.1128296727",
        "pub.1121025204",
        "pub.1133177310",
        "pub.1142776657",
        "pub.1099106215",
        "pub.1121025212",
        "pub.1068001284"
      ],
      "concepts_scores": [
        {
          "concept": "open-domain question answering",
          "relevance": 0.874
        },
        {
          "concept": "external knowledge base",
          "relevance": 0.829
        },
        {
          "concept": "domain adaptation",
          "relevance": 0.775
        },
        {
          "concept": "question answering",
          "relevance": 0.737
        },
        {
          "concept": "knowledge base",
          "relevance": 0.726
        },
        {
          "concept": "task of domain adaptation",
          "relevance": 0.708
        },
        {
          "concept": "domain-specific knowledge base",
          "relevance": 0.7
        },
        {
          "concept": "domain question answering",
          "relevance": 0.684
        },
        {
          "concept": "domain-specific knowledge",
          "relevance": 0.665
        },
        {
          "concept": "QA task",
          "relevance": 0.627
        },
        {
          "concept": "training signals",
          "relevance": 0.625
        },
        {
          "concept": "transformation library",
          "relevance": 0.619
        },
        {
          "concept": "joint training",
          "relevance": 0.617
        },
        {
          "concept": "performance improvement",
          "relevance": 0.593
        },
        {
          "concept": "open domain",
          "relevance": 0.568
        },
        {
          "concept": "specialized domains",
          "relevance": 0.568
        },
        {
          "concept": "generation component",
          "relevance": 0.542
        },
        {
          "concept": "relevant information",
          "relevance": 0.539
        },
        {
          "concept": "retrieval",
          "relevance": 0.533
        },
        {
          "concept": "task",
          "relevance": 0.519
        },
        {
          "concept": "training",
          "relevance": 0.496
        },
        {
          "concept": "HuggingFace",
          "relevance": 0.491
        },
        {
          "concept": "domain",
          "relevance": 0.482
        },
        {
          "concept": "dataset",
          "relevance": 0.467
        },
        {
          "concept": "answers",
          "relevance": 0.445
        },
        {
          "concept": "library",
          "relevance": 0.439
        },
        {
          "concept": "news",
          "relevance": 0.438
        },
        {
          "concept": "base",
          "relevance": 0.428
        },
        {
          "concept": "augmented generation",
          "relevance": 0.422
        },
        {
          "concept": "adaptation",
          "relevance": 0.421
        },
        {
          "concept": "information",
          "relevance": 0.414
        },
        {
          "concept": "sentences",
          "relevance": 0.413
        },
        {
          "concept": "performance",
          "relevance": 0.411
        },
        {
          "concept": "model",
          "relevance": 0.396
        },
        {
          "concept": "generation",
          "relevance": 0.39
        },
        {
          "concept": "extension",
          "relevance": 0.369
        },
        {
          "concept": "signal",
          "relevance": 0.363
        },
        {
          "concept": "consistency",
          "relevance": 0.36
        },
        {
          "concept": "knowledge",
          "relevance": 0.359
        },
        {
          "concept": "credibility",
          "relevance": 0.358
        },
        {
          "concept": "healthcare",
          "relevance": 0.354
        },
        {
          "concept": "improvement",
          "relevance": 0.329
        },
        {
          "concept": "COVID-19",
          "relevance": 0.312
        },
        {
          "concept": "technical consistency",
          "relevance": 0.312
        },
        {
          "concept": "Abstract",
          "relevance": 0.297
        },
        {
          "concept": "contribution",
          "relevance": 0.292
        },
        {
          "concept": "components",
          "relevance": 0.29
        },
        {
          "concept": "impact",
          "relevance": 0.25
        },
        {
          "concept": "conversion",
          "relevance": 0.211
        }
      ]
    },
    {
      "paperId": "pub.1116884455",
      "doi": "10.1109/tbdata.2019.2921572",
      "title": "Billion-Scale Similarity Search with GPUs",
      "year": 2019,
      "citationCount": 2050,
      "fieldCitationRatio": 475.37,
      "abstract": "Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as $k$k-min selection, or make poor use of the memory hierarchy. We propose a novel design for $k$k-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 × faster than prior GPU state of the art. It enables the construction of a high accuracy $k$k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.",
      "reference_ids": [
        "pub.1061408600",
        "pub.1092896100",
        "pub.1061743975",
        "pub.1095726446",
        "pub.1061744426",
        "pub.1048236487",
        "pub.1107502756",
        "pub.1094027886",
        "pub.1094975445",
        "pub.1093401702",
        "pub.1048813203",
        "pub.1093481891",
        "pub.1095318400",
        "pub.1030416589",
        "pub.1095191978",
        "pub.1095453892",
        "pub.1006419213",
        "pub.1061744551",
        "pub.1093645378",
        "pub.1093359587",
        "pub.1040497148",
        "pub.1110957153",
        "pub.1095369335",
        "pub.1085642448",
        "pub.1034291012",
        "pub.1095741777",
        "pub.1094944758",
        "pub.1041862539",
        "pub.1095576275",
        "pub.1112447813",
        "pub.1068092481",
        "pub.1044800349",
        "pub.1096024934",
        "pub.1053152786",
        "pub.1067367418",
        "pub.1067368858",
        "pub.1093345145",
        "pub.1018807394",
        "pub.1094520012",
        "pub.1110720487",
        "pub.1023002428",
        "pub.1030042306",
        "pub.1008504036",
        "pub.1032984348"
      ],
      "concepts_scores": [
        {
          "concept": "similarity search",
          "relevance": 0.699
        },
        {
          "concept": "Titan X GPU",
          "relevance": 0.688
        },
        {
          "concept": "high-dimensional features",
          "relevance": 0.685
        },
        {
          "concept": "YFCC100M dataset",
          "relevance": 0.68
        },
        {
          "concept": "data parallel tasks",
          "relevance": 0.667
        },
        {
          "concept": "nearest neighbor implementation",
          "relevance": 0.665
        },
        {
          "concept": "theoretical peak performance",
          "relevance": 0.646
        },
        {
          "concept": "billion-scale",
          "relevance": 0.642
        },
        {
          "concept": "M dataset",
          "relevance": 0.638
        },
        {
          "concept": "product quantization",
          "relevance": 0.637
        },
        {
          "concept": "search scenarios",
          "relevance": 0.634
        },
        {
          "concept": "index structure",
          "relevance": 0.633
        },
        {
          "concept": "database systems",
          "relevance": 0.631
        },
        {
          "concept": "distance computation",
          "relevance": 0.63
        },
        {
          "concept": "parallel tasks",
          "relevance": 0.619
        },
        {
          "concept": "GPU",
          "relevance": 0.612
        },
        {
          "concept": "brute force",
          "relevance": 0.609
        },
        {
          "concept": "memory hierarchy",
          "relevance": 0.588
        },
        {
          "concept": "peak performance",
          "relevance": 0.557
        },
        {
          "concept": "task",
          "relevance": 0.521
        },
        {
          "concept": "implementation",
          "relevance": 0.493
        },
        {
          "concept": "search",
          "relevance": 0.492
        },
        {
          "concept": "video",
          "relevance": 0.475
        },
        {
          "concept": "images",
          "relevance": 0.474
        },
        {
          "concept": "algorithm",
          "relevance": 0.471
        },
        {
          "concept": "quantization",
          "relevance": 0.471
        },
        {
          "concept": "computer",
          "relevance": 0.462
        },
        {
          "concept": "graph",
          "relevance": 0.452
        },
        {
          "concept": "similarity",
          "relevance": 0.435
        },
        {
          "concept": "scenarios",
          "relevance": 0.429
        },
        {
          "concept": "accuracy",
          "relevance": 0.429
        },
        {
          "concept": "parallel",
          "relevance": 0.419
        },
        {
          "concept": "performance",
          "relevance": 0.413
        },
        {
          "concept": "nearest",
          "relevance": 0.406
        },
        {
          "concept": "hierarchy",
          "relevance": 0.404
        },
        {
          "concept": "database",
          "relevance": 0.402
        },
        {
          "concept": "vector",
          "relevance": 0.401
        },
        {
          "concept": "memory",
          "relevance": 0.398
        },
        {
          "concept": "features",
          "relevance": 0.397
        },
        {
          "concept": "domain",
          "relevance": 0.391
        },
        {
          "concept": "applications",
          "relevance": 0.389
        },
        {
          "concept": "data",
          "relevance": 0.38
        },
        {
          "concept": "system",
          "relevance": 0.376
        },
        {
          "concept": "setup",
          "relevance": 0.371
        },
        {
          "concept": "design",
          "relevance": 0.363
        },
        {
          "concept": "distance",
          "relevance": 0.352
        },
        {
          "concept": "selection",
          "relevance": 0.344
        },
        {
          "concept": "poor use",
          "relevance": 0.341
        },
        {
          "concept": "construction",
          "relevance": 0.317
        },
        {
          "concept": "state",
          "relevance": 0.3
        },
        {
          "concept": "Maxwell",
          "relevance": 0.293
        },
        {
          "concept": "comparison",
          "relevance": 0.283
        },
        {
          "concept": "structure",
          "relevance": 0.263
        },
        {
          "concept": "use",
          "relevance": 0.257
        },
        {
          "concept": "index",
          "relevance": 0.232
        },
        {
          "concept": "margin",
          "relevance": 0.224
        },
        {
          "concept": "reproducibility",
          "relevance": 0.222
        },
        {
          "concept": "approach",
          "relevance": 0.207
        },
        {
          "concept": "production",
          "relevance": 0.202
        },
        {
          "concept": "problem",
          "relevance": 0.201
        },
        {
          "concept": "hours",
          "relevance": 0.168
        },
        {
          "concept": "minutes",
          "relevance": 0.141
        }
      ]
    },
    {
      "paperId": "pub.1099237519",
      "doi": "10.3115/1118108.1118117",
      "title": "NLTK: the Natural Language Toolkit",
      "year": 2002,
      "citationCount": 1796,
      "fieldCitationRatio": 554.49,
      "abstract": "NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.",
      "reference_ids": [
        "pub.1099237518"
      ],
      "concepts_scores": [
        {
          "concept": "statistical natural language processing",
          "relevance": 0.682
        },
        {
          "concept": "Natural Language Toolkit",
          "relevance": 0.669
        },
        {
          "concept": "natural language processing",
          "relevance": 0.667
        },
        {
          "concept": "annotated corpus",
          "relevance": 0.621
        },
        {
          "concept": "Language Toolkit",
          "relevance": 0.615
        },
        {
          "concept": "language processing",
          "relevance": 0.607
        },
        {
          "concept": "NLTK",
          "relevance": 0.567
        },
        {
          "concept": "structured program",
          "relevance": 0.535
        },
        {
          "concept": "corpus",
          "relevance": 0.474
        },
        {
          "concept": "sophisticated models",
          "relevance": 0.453
        },
        {
          "concept": "courseware",
          "relevance": 0.446
        },
        {
          "concept": "toolkit",
          "relevance": 0.437
        },
        {
          "concept": "tutorial",
          "relevance": 0.423
        },
        {
          "concept": "students",
          "relevance": 0.423
        },
        {
          "concept": "modulation",
          "relevance": 0.38
        },
        {
          "concept": "program",
          "relevance": 0.348
        },
        {
          "concept": "nature",
          "relevance": 0.341
        },
        {
          "concept": "model",
          "relevance": 0.337
        },
        {
          "concept": "process",
          "relevance": 0.319
        },
        {
          "concept": "components",
          "relevance": 0.286
        },
        {
          "concept": "problem",
          "relevance": 0.246
        }
      ]
    },
    {
      "paperId": "pub.1160719363",
      "doi": "10.1109/icse48619.2023.00205",
      "title": "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning",
      "year": 2023,
      "citationCount": 121,
      "fieldCitationRatio": 73.58,
      "abstract": "Large language models trained on massive code corpora can generalize to new tasks without the need for task-specific fine-tuning. In few-shot learning, these models take as input a prompt, composed of natural language instructions, a few instances of task demonstration, and a query and generate an output. However, the creation of an effective prompt for code-related tasks in few-shot learning has received little attention. We present a technique for prompt creation that automatically retrieves code demonstrations similar to the developer task, based on embedding or frequency analysis. We apply our approach, Cedar, to two different programming languages, statically and dynamically typed, and two different tasks, namely, test assertion generation and program repair. For each task, we compare Cedar with state-of-the-art task-specific and fine-tuned models. The empirical results show that, with only a few relevant code demonstrations, our prompt creation technique is effective in both tasks with an accuracy of 76% and 52% for exact matches in test assertion generation and program repair tasks, respectively. For assertion generation, Cedar outperforms existing task-specific and fine-tuned models by 333% and 11%, respectively. For program repair, Cedar yields 189% better accuracy than task-specific models and is competitive with recent fine-tuned models. These findings have practical implications for practitioners, as Cedar could potentially be applied to multilingual and multitask settings without task or language-specific training with minimal examples and effort.",
      "reference_ids": [
        "pub.1139225646",
        "pub.1155161025",
        "pub.1140503176",
        "pub.1138840378",
        "pub.1146057267",
        "pub.1149798057",
        "pub.1148904539",
        "pub.1151546099",
        "pub.1120262060",
        "pub.1148626598",
        "pub.1068001284",
        "pub.1035923637",
        "pub.1149257009",
        "pub.1122290427",
        "pub.1061252036",
        "pub.1133175050",
        "pub.1149257018",
        "pub.1144826036",
        "pub.1120980723",
        "pub.1131375846",
        "pub.1148581733",
        "pub.1153629455",
        "pub.1131375762",
        "pub.1149687512",
        "pub.1092571776",
        "pub.1139948543",
        "pub.1147514535",
        "pub.1143949474",
        "pub.1148894321",
        "pub.1148904699",
        "pub.1147478078",
        "pub.1134916555",
        "pub.1107823225",
        "pub.1148390971",
        "pub.1133174829",
        "pub.1137866064",
        "pub.1147478009",
        "pub.1137805304",
        "pub.1154263433",
        "pub.1149606008",
        "pub.1148581735",
        "pub.1120799441",
        "pub.1120986120",
        "pub.1148697564",
        "pub.1148899442",
        "pub.1098672059",
        "pub.1105908371",
        "pub.1104321292",
        "pub.1143949249",
        "pub.1129334333",
        "pub.1137865998",
        "pub.1152622074",
        "pub.1128474472",
        "pub.1149299491",
        "pub.1144123804",
        "pub.1149300959"
      ],
      "concepts_scores": [
        {
          "concept": "few-shot learning",
          "relevance": 0.835
        },
        {
          "concept": "fine-tuned models",
          "relevance": 0.782
        },
        {
          "concept": "assertion generation",
          "relevance": 0.767
        },
        {
          "concept": "program repair",
          "relevance": 0.739
        },
        {
          "concept": "code demonstrations",
          "relevance": 0.714
        },
        {
          "concept": "task-specific fine-tuning",
          "relevance": 0.707
        },
        {
          "concept": "code-related tasks",
          "relevance": 0.689
        },
        {
          "concept": "natural language instructions",
          "relevance": 0.686
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.683
        },
        {
          "concept": "task-specific models",
          "relevance": 0.674
        },
        {
          "concept": "task-specific",
          "relevance": 0.663
        },
        {
          "concept": "code corpus",
          "relevance": 0.636
        },
        {
          "concept": "language model",
          "relevance": 0.628
        },
        {
          "concept": "programming language",
          "relevance": 0.626
        },
        {
          "concept": "task demonstrations",
          "relevance": 0.621
        },
        {
          "concept": "retrieval-based",
          "relevance": 0.62
        },
        {
          "concept": "multitask setting",
          "relevance": 0.612
        },
        {
          "concept": "development tasks",
          "relevance": 0.6
        },
        {
          "concept": "repair tasks",
          "relevance": 0.593
        },
        {
          "concept": "prompt selection",
          "relevance": 0.591
        },
        {
          "concept": "fine-tuning",
          "relevance": 0.585
        },
        {
          "concept": "creation techniques",
          "relevance": 0.581
        },
        {
          "concept": "task",
          "relevance": 0.559
        },
        {
          "concept": "code",
          "relevance": 0.553
        },
        {
          "concept": "learning",
          "relevance": 0.536
        },
        {
          "concept": "empirical results",
          "relevance": 0.497
        },
        {
          "concept": "accuracy",
          "relevance": 0.494
        },
        {
          "concept": "query",
          "relevance": 0.493
        },
        {
          "concept": "effective prompts",
          "relevance": 0.483
        },
        {
          "concept": "language",
          "relevance": 0.475
        },
        {
          "concept": "embedding",
          "relevance": 0.471
        },
        {
          "concept": "language instruction",
          "relevance": 0.437
        },
        {
          "concept": "program",
          "relevance": 0.437
        },
        {
          "concept": "creation",
          "relevance": 0.429
        },
        {
          "concept": "model",
          "relevance": 0.426
        },
        {
          "concept": "minimal example",
          "relevance": 0.426
        },
        {
          "concept": "technique",
          "relevance": 0.425
        },
        {
          "concept": "corpus",
          "relevance": 0.424
        },
        {
          "concept": "training",
          "relevance": 0.407
        },
        {
          "concept": "demonstration",
          "relevance": 0.4
        },
        {
          "concept": "output",
          "relevance": 0.391
        },
        {
          "concept": "generation",
          "relevance": 0.389
        },
        {
          "concept": "examples",
          "relevance": 0.38
        },
        {
          "concept": "sets",
          "relevance": 0.372
        },
        {
          "concept": "prompts",
          "relevance": 0.355
        },
        {
          "concept": "selection",
          "relevance": 0.342
        },
        {
          "concept": "instruction",
          "relevance": 0.341
        },
        {
          "concept": "frequency analysis",
          "relevance": 0.34
        },
        {
          "concept": "efforts",
          "relevance": 0.332
        },
        {
          "concept": "attention",
          "relevance": 0.327
        },
        {
          "concept": "assertion",
          "relevance": 0.322
        },
        {
          "concept": "results",
          "relevance": 0.314
        },
        {
          "concept": "practitioners",
          "relevance": 0.306
        },
        {
          "concept": "test",
          "relevance": 0.297
        },
        {
          "concept": "development",
          "relevance": 0.281
        },
        {
          "concept": "analysis",
          "relevance": 0.259
        },
        {
          "concept": "frequency",
          "relevance": 0.256
        },
        {
          "concept": "cedar",
          "relevance": 0.253
        },
        {
          "concept": "findings",
          "relevance": 0.201
        },
        {
          "concept": "repair",
          "relevance": 0.146
        }
      ]
    },
    {
      "paperId": "pub.1148581733",
      "doi": "10.1145/3520312.3534862",
      "title": "A systematic evaluation of large language models of code",
      "year": 2022,
      "citationCount": 387,
      "fieldCitationRatio": 502.78,
      "abstract": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.",
      "reference_ids": [
        "pub.1130798748",
        "pub.1131375759",
        "pub.1138840287",
        "pub.1091293947",
        "pub.1121664431",
        "pub.1138840378",
        "pub.1012988085",
        "pub.1143949474",
        "pub.1013452927",
        "pub.1099113598",
        "pub.1143980784",
        "pub.1121024947",
        "pub.1041651699",
        "pub.1140657128"
      ],
      "concepts_scores": [
        {
          "concept": "language model",
          "relevance": 0.737
        },
        {
          "concept": "programming language",
          "relevance": 0.717
        },
        {
          "concept": "open-source",
          "relevance": 0.657
        },
        {
          "concept": "multi-lingual corpus",
          "relevance": 0.649
        },
        {
          "concept": "natural language descriptions",
          "relevance": 0.644
        },
        {
          "concept": "corpus of code",
          "relevance": 0.644
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.643
        },
        {
          "concept": "natural language model",
          "relevance": 0.638
        },
        {
          "concept": "C programming language",
          "relevance": 0.634
        },
        {
          "concept": "models of code",
          "relevance": 0.618
        },
        {
          "concept": "synthesized code",
          "relevance": 0.59
        },
        {
          "concept": "GPT-2",
          "relevance": 0.585
        },
        {
          "concept": "language description",
          "relevance": 0.584
        },
        {
          "concept": "training model",
          "relevance": 0.562
        },
        {
          "concept": "design decisions",
          "relevance": 0.549
        },
        {
          "concept": "open-source model",
          "relevance": 0.542
        },
        {
          "concept": "code",
          "relevance": 0.535
        },
        {
          "concept": "language",
          "relevance": 0.483
        },
        {
          "concept": "polycode",
          "relevance": 0.449
        },
        {
          "concept": "architecture",
          "relevance": 0.437
        },
        {
          "concept": "OpenSource",
          "relevance": 0.434
        },
        {
          "concept": "systematic evaluation",
          "relevance": 0.432
        },
        {
          "concept": "data",
          "relevance": 0.421
        },
        {
          "concept": "online appendix",
          "relevance": 0.411
        },
        {
          "concept": "Codex",
          "relevance": 0.404
        },
        {
          "concept": "program",
          "relevance": 0.404
        },
        {
          "concept": "model",
          "relevance": 0.402
        },
        {
          "concept": "evaluation",
          "relevance": 0.395
        },
        {
          "concept": "decision",
          "relevance": 0.371
        },
        {
          "concept": "applications",
          "relevance": 0.365
        },
        {
          "concept": "description",
          "relevance": 0.319
        },
        {
          "concept": "questions",
          "relevance": 0.315
        },
        {
          "concept": "research",
          "relevance": 0.314
        },
        {
          "concept": "results",
          "relevance": 0.296
        },
        {
          "concept": "parameters",
          "relevance": 0.281
        },
        {
          "concept": "appendix",
          "relevance": 0.254
        },
        {
          "concept": "area",
          "relevance": 0.246
        },
        {
          "concept": "blank",
          "relevance": 0.212
        }
      ]
    },
    {
      "paperId": "pub.1149798057",
      "doi": "10.1109/sp46214.2022.9833571",
      "title": "Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions",
      "year": 2022,
      "citationCount": 297,
      "fieldCitationRatio": 128.25,
      "abstract": "There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the first self-described ‘AI pair programmer’, GitHub Copilot, which is a language model trained over open-source GitHub code. However, code often contains bugs—and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot’s code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk cybersecurity weaknesses, e.g. those from MITRE’s “Top 25” Common Weakness Enumeration (CWE) list. We explore Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40% to be vulnerable.",
      "reference_ids": [
        "pub.1132552589",
        "pub.1137866064",
        "pub.1023230615",
        "pub.1099252525",
        "pub.1104274876",
        "pub.1094578301",
        "pub.1121734426",
        "pub.1010282076",
        "pub.1132888962",
        "pub.1140685010",
        "pub.1018714489",
        "pub.1115977877",
        "pub.1129857044",
        "pub.1117477125"
      ],
      "concepts_scores": [
        {
          "concept": "Common Weakness Enumeration",
          "relevance": 0.845
        },
        {
          "concept": "GitHub Copilot",
          "relevance": 0.747
        },
        {
          "concept": "code contributions",
          "relevance": 0.736
        },
        {
          "concept": "language model",
          "relevance": 0.735
        },
        {
          "concept": "designing AI-based systems",
          "relevance": 0.715
        },
        {
          "concept": "design computer systems",
          "relevance": 0.686
        },
        {
          "concept": "AI-based systems",
          "relevance": 0.678
        },
        {
          "concept": "diversity of domains",
          "relevance": 0.659
        },
        {
          "concept": "buggy code",
          "relevance": 0.645
        },
        {
          "concept": "insecure code",
          "relevance": 0.645
        },
        {
          "concept": "Weakness Enumeration",
          "relevance": 0.645
        },
        {
          "concept": "pair programmers",
          "relevance": 0.632
        },
        {
          "concept": "GitHub code",
          "relevance": 0.625
        },
        {
          "concept": "cybersecurity weaknesses",
          "relevance": 0.61
        },
        {
          "concept": "copilot",
          "relevance": 0.581
        },
        {
          "concept": "code",
          "relevance": 0.574
        },
        {
          "concept": "GitHub",
          "relevance": 0.555
        },
        {
          "concept": "security",
          "relevance": 0.549
        },
        {
          "concept": "scenarios",
          "relevance": 0.5
        },
        {
          "concept": "buggy",
          "relevance": 0.494
        },
        {
          "concept": "computer code",
          "relevance": 0.493
        },
        {
          "concept": "keyboard",
          "relevance": 0.486
        },
        {
          "concept": "language",
          "relevance": 0.48
        },
        {
          "concept": "MITRE",
          "relevance": 0.473
        },
        {
          "concept": "computer",
          "relevance": 0.464
        },
        {
          "concept": "system",
          "relevance": 0.437
        },
        {
          "concept": "performance",
          "relevance": 0.415
        },
        {
          "concept": "model",
          "relevance": 0.399
        },
        {
          "concept": "weakness",
          "relevance": 0.395
        },
        {
          "concept": "domain",
          "relevance": 0.393
        },
        {
          "concept": "tools",
          "relevance": 0.378
        },
        {
          "concept": "enumeration",
          "relevance": 0.365
        },
        {
          "concept": "diversity",
          "relevance": 0.362
        },
        {
          "concept": "program",
          "relevance": 0.356
        },
        {
          "concept": "contribution",
          "relevance": 0.341
        },
        {
          "concept": "prompts",
          "relevance": 0.31
        },
        {
          "concept": "concerns",
          "relevance": 0.284
        },
        {
          "concept": "humans",
          "relevance": 0.281
        },
        {
          "concept": "programme",
          "relevance": 0.275
        },
        {
          "concept": "analysis",
          "relevance": 0.262
        },
        {
          "concept": "quantity",
          "relevance": 0.236
        },
        {
          "concept": "conditions",
          "relevance": 0.209
        },
        {
          "concept": "prevalence",
          "relevance": 0.161
        }
      ]
    }
  ],
  "evolution_links": [
    {
      "source": "pub.1182518197",
      "target": "pub.1173700042",
      "source_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
      "target_title": "The Power of Noise: Redefining Retrieval for RAG Systems"
    },
    {
      "source": "pub.1173700042",
      "target": "pub.1120096978",
      "source_title": "The Power of Noise: Redefining Retrieval for RAG Systems",
      "target_title": "Natural Questions: A Benchmark for Question Answering Research"
    },
    {
      "source": "pub.1120096978",
      "target": "pub.1099239594",
      "source_title": "Natural Questions: A Benchmark for Question Answering Research",
      "target_title": "BLEU: a method for automatic evaluation of machine translation"
    },
    {
      "source": "pub.1120096978",
      "target": "pub.1115977877",
      "source_title": "Natural Questions: A Benchmark for Question Answering Research",
      "target_title": "CoQA: A Conversational Question Answering Challenge"
    },
    {
      "source": "pub.1173700042",
      "target": "pub.1129670253",
      "source_title": "The Power of Noise: Redefining Retrieval for RAG Systems",
      "target_title": "ColBERT"
    },
    {
      "source": "pub.1129670253",
      "target": "pub.1052509117",
      "source_title": "ColBERT",
      "target_title": "Learning deep structured semantic models for web search using clickthrough data"
    },
    {
      "source": "pub.1129670253",
      "target": "pub.1050758935",
      "source_title": "ColBERT",
      "target_title": "A Deep Relevance Matching Model for Ad-hoc Retrieval"
    },
    {
      "source": "pub.1182518197",
      "target": "pub.1172706643",
      "source_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
      "target_title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System"
    },
    {
      "source": "pub.1172706643",
      "target": "pub.1154787683",
      "source_title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
      "target_title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering"
    },
    {
      "source": "pub.1154787683",
      "target": "pub.1116884455",
      "source_title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
      "target_title": "Billion-Scale Similarity Search with GPUs"
    },
    {
      "source": "pub.1154787683",
      "target": "pub.1099237519",
      "source_title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
      "target_title": "NLTK: the Natural Language Toolkit"
    },
    {
      "source": "pub.1172706643",
      "target": "pub.1160719363",
      "source_title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
      "target_title": "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning"
    },
    {
      "source": "pub.1160719363",
      "target": "pub.1148581733",
      "source_title": "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning",
      "target_title": "A systematic evaluation of large language models of code"
    },
    {
      "source": "pub.1160719363",
      "target": "pub.1149798057",
      "source_title": "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning",
      "target_title": "Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"
    }
  ]
}