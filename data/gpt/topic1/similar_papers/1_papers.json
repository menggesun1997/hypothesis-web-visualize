[
  {
    "paperId": "pub.1186491988",
    "doi": "10.1109/tcyb.2025.3527632",
    "title": "Industrial Foundation Model",
    "year": 2025,
    "citationCount": 1,
    "fieldCitationRatio": 0.0,
    "abstract": "Recently, foundation models (such as ChatGPT) have emerged with powerful learning, understanding, and generalization abilities, showcasing tremendous potential to revolutionarily promote modern industry. Despite significant advancements in various fields, existing general foundation models face challenges in industry when dealing with the data of specialized modalities, the tasks of varying-scenario with multiple processes, and the requirements of trustworthy output, which makes industrial foundation model (IFM) a necessity. This article proposes a system architecture of termed IFMsys, including model training, model adaptation, and model application. Specifically, in model training, a base model is constructed by pretraining on multimodal industrial data and fine-tuning with fundamental industrial mechanisms. In model adaptation, the base model is developed into a series of task-oriented and domain-specific IFMs through fine-tuning with representative tasks and domain knowledge. In model application, an industrial agent-centric collaboration system and a comprehensive application framework of IFM are proposed to enhance the industrial product lifecycle applications. In addition, a prototype system of the IFM, namely, MetaIndux, is delivered, with application examples presented in typical industrial tasks. Finally, future research directions and open issues of IFM are prospected. We hope this article will inspire the advancements in the theories, technologies, and applications in this emerging research field of IFM.",
    "reference_ids": [
      "pub.1133284858",
      "pub.1157851791",
      "pub.1167961810",
      "pub.1170005241",
      "pub.1134574723",
      "pub.1157286508",
      "pub.1145901979",
      "pub.1127318696",
      "pub.1099653251",
      "pub.1134591315",
      "pub.1123988461",
      "pub.1163041724",
      "pub.1160041465",
      "pub.1134973747",
      "pub.1140668479",
      "pub.1176032971",
      "pub.1157184122",
      "pub.1167639363",
      "pub.1183245772",
      "pub.1163456125",
      "pub.1145901663",
      "pub.1151380999",
      "pub.1128537995",
      "pub.1163453407",
      "pub.1157672384",
      "pub.1123642846",
      "pub.1111313779",
      "pub.1166060882",
      "pub.1172246746",
      "pub.1155094781",
      "pub.1176028899",
      "pub.1151380649"
    ],
    "concepts_scores": [
      {
        "concept": "model training",
        "relevance": 0.608
      },
      {
        "concept": "model adaptation",
        "relevance": 0.593
      },
      {
        "concept": "base model",
        "relevance": 0.543
      },
      {
        "concept": "domain knowledge",
        "relevance": 0.535
      },
      {
        "concept": "generalization ability",
        "relevance": 0.531
      },
      {
        "concept": "application framework",
        "relevance": 0.531
      },
      {
        "concept": "collaborative systems",
        "relevance": 0.529
      },
      {
        "concept": "trustworthy outputs",
        "relevance": 0.527
      },
      {
        "concept": "system architecture",
        "relevance": 0.521
      },
      {
        "concept": "representative tasks",
        "relevance": 0.521
      },
      {
        "concept": "prototype system",
        "relevance": 0.514
      },
      {
        "concept": "industrial tasks",
        "relevance": 0.503
      },
      {
        "concept": "research directions",
        "relevance": 0.471
      },
      {
        "concept": "task",
        "relevance": 0.463
      },
      {
        "concept": "research field",
        "relevance": 0.462
      },
      {
        "concept": "model application",
        "relevance": 0.457
      },
      {
        "concept": "task-oriented",
        "relevance": 0.445
      },
      {
        "concept": "foundation model",
        "relevance": 0.44
      },
      {
        "concept": "industrial data",
        "relevance": 0.429
      },
      {
        "concept": "modern industry",
        "relevance": 0.423
      },
      {
        "concept": "applications",
        "relevance": 0.408
      },
      {
        "concept": "industrial mechanisms",
        "relevance": 0.408
      },
      {
        "concept": "training",
        "relevance": 0.402
      },
      {
        "concept": "architecture",
        "relevance": 0.395
      },
      {
        "concept": "system",
        "relevance": 0.387
      },
      {
        "concept": "learning",
        "relevance": 0.375
      },
      {
        "concept": "prototype",
        "relevance": 0.366
      },
      {
        "concept": "model",
        "relevance": 0.363
      },
      {
        "concept": "multiple processes",
        "relevance": 0.363
      },
      {
        "concept": "adaptation",
        "relevance": 0.358
      },
      {
        "concept": "framework",
        "relevance": 0.351
      },
      {
        "concept": "requirements",
        "relevance": 0.348
      },
      {
        "concept": "technology",
        "relevance": 0.348
      },
      {
        "concept": "generalization",
        "relevance": 0.341
      },
      {
        "concept": "base",
        "relevance": 0.34
      },
      {
        "concept": "output",
        "relevance": 0.333
      },
      {
        "concept": "domain",
        "relevance": 0.331
      },
      {
        "concept": "research",
        "relevance": 0.328
      },
      {
        "concept": "data",
        "relevance": 0.322
      },
      {
        "concept": "industry",
        "relevance": 0.319
      },
      {
        "concept": "specialized modalities",
        "relevance": 0.315
      },
      {
        "concept": "knowledge",
        "relevance": 0.305
      },
      {
        "concept": "advances",
        "relevance": 0.286
      },
      {
        "concept": "direction",
        "relevance": 0.279
      },
      {
        "concept": "process",
        "relevance": 0.275
      },
      {
        "concept": "necessity",
        "relevance": 0.273
      },
      {
        "concept": "field",
        "relevance": 0.267
      },
      {
        "concept": "ability",
        "relevance": 0.253
      },
      {
        "concept": "understanding",
        "relevance": 0.245
      },
      {
        "concept": "modalities",
        "relevance": 0.24
      },
      {
        "concept": "theory",
        "relevance": 0.233
      },
      {
        "concept": "mechanism",
        "relevance": 0.23
      },
      {
        "concept": "article",
        "relevance": 0.227
      }
    ]
  },
  {
    "paperId": "pub.1183720527",
    "doi": "10.1177/20552076241308987",
    "title": "Clinical concept annotation with contextual word embedding in active transfer learning environment",
    "year": 2024,
    "citationCount": 1,
    "fieldCitationRatio": 0.0,
    "abstract": "Objective: The study aims to present an active learning approach that automatically extracts clinical concepts from unstructured data and classifies them into explicit categories such as Problem, Treatment, and Test while preserving high precision and recall and demonstrating the approach through experiments using i2b2 public datasets.\nMethods: Initially labeled data are acquired from a lexical-based approach in sufficient amounts to perform an active learning process. A contextual word embedding similarity approach is adopted using BERT base variant models such as ClinicalBERT, DistilBERT, and SCIBERT to automatically classify the unlabeled clinical concept into explicit categories. Additionally, deep learning and large language model (LLM) are trained on acquiring label data through active learning.\nResults: Using i2b2 datasets (426 clinical notes), the lexical-based method achieved precision, recall, and F1-scores of 76%, 70%, and 73%. SCIBERT excelled in active transfer learning, yielding precision of 70.84%, recall of 77.40%, F1-score of 73.97%, and accuracy of 69.30%, surpassing counterpart models. Among deep learning models, convolutional neural networks (CNNs) trained with embeddings (BERTBase, DistilBERT, SCIBERT, ClinicalBERT) achieved training accuracies of 92-95% and testing accuracies of 89-93%. These results were higher compared to other deep learning models. Additionally, we individually evaluated these LLMs; among them, ClinicalBERT achieved the highest performance, with a training accuracy of 98.4% and a testing accuracy of 96%, outperforming the others.\nConclusions: The proposed methodology enhances clinical concept extraction by integrating active learning and models like SCIBERT and CNN. It improves annotation efficiency while maintaining high accuracy, showcasing potential for clinical applications.",
    "reference_ids": [
      "pub.1123589470",
      "pub.1031977255",
      "pub.1158632885",
      "pub.1122290724",
      "pub.1085915560",
      "pub.1010898373",
      "pub.1037476787",
      "pub.1126024985",
      "pub.1085376800",
      "pub.1141744131",
      "pub.1115678138",
      "pub.1059744116",
      "pub.1117657814",
      "pub.1141775194",
      "pub.1168196409",
      "pub.1122469906",
      "pub.1156937810",
      "pub.1032191662",
      "pub.1059743923",
      "pub.1079139320",
      "pub.1005757061",
      "pub.1172263760",
      "pub.1037494609",
      "pub.1075199214",
      "pub.1168467862",
      "pub.1096025544",
      "pub.1129906535",
      "pub.1003012510",
      "pub.1031861138",
      "pub.1091050481",
      "pub.1095298342",
      "pub.1040096592",
      "pub.1118170219",
      "pub.1059744077",
      "pub.1000251285",
      "pub.1074670458",
      "pub.1092866584",
      "pub.1104336087",
      "pub.1029203376",
      "pub.1111225157",
      "pub.1164233474",
      "pub.1062977837"
    ],
    "concepts_scores": [
      {
        "concept": "convolutional neural network",
        "relevance": 0.784
      },
      {
        "concept": "deep learning models",
        "relevance": 0.736
      },
      {
        "concept": "training accuracy",
        "relevance": 0.684
      },
      {
        "concept": "learning models",
        "relevance": 0.663
      },
      {
        "concept": "transfer learning environment",
        "relevance": 0.655
      },
      {
        "concept": "initial labeled data",
        "relevance": 0.655
      },
      {
        "concept": "lexical-based methods",
        "relevance": 0.655
      },
      {
        "concept": "clinical concept extraction",
        "relevance": 0.647
      },
      {
        "concept": "active transfer learning",
        "relevance": 0.643
      },
      {
        "concept": "test accuracy",
        "relevance": 0.641
      },
      {
        "concept": "lexical-based approach",
        "relevance": 0.638
      },
      {
        "concept": "active learning",
        "relevance": 0.638
      },
      {
        "concept": "active learning approach",
        "relevance": 0.616
      },
      {
        "concept": "concept annotation",
        "relevance": 0.601
      },
      {
        "concept": "labeled data",
        "relevance": 0.601
      },
      {
        "concept": "concept extraction",
        "relevance": 0.601
      },
      {
        "concept": "language model",
        "relevance": 0.597
      },
      {
        "concept": "transfer learning",
        "relevance": 0.597
      },
      {
        "concept": "public datasets",
        "relevance": 0.595
      },
      {
        "concept": "contextual words",
        "relevance": 0.594
      },
      {
        "concept": "i2b2 dataset",
        "relevance": 0.594
      },
      {
        "concept": "deep learning",
        "relevance": 0.592
      },
      {
        "concept": "F1 score",
        "relevance": 0.592
      },
      {
        "concept": "annotation efficiency",
        "relevance": 0.592
      },
      {
        "concept": "neural network",
        "relevance": 0.588
      },
      {
        "concept": "active learning process",
        "relevance": 0.587
      },
      {
        "concept": "explicit categories",
        "relevance": 0.574
      },
      {
        "concept": "learning approach",
        "relevance": 0.573
      },
      {
        "concept": "SciBERT",
        "relevance": 0.549
      },
      {
        "concept": "similarity approach",
        "relevance": 0.548
      },
      {
        "concept": "i2b2",
        "relevance": 0.542
      },
      {
        "concept": "variant models",
        "relevance": 0.53
      },
      {
        "concept": "learning",
        "relevance": 0.518
      },
      {
        "concept": "dataset",
        "relevance": 0.514
      },
      {
        "concept": "learning process",
        "relevance": 0.508
      },
      {
        "concept": "clinical concepts",
        "relevance": 0.506
      },
      {
        "concept": "accuracy",
        "relevance": 0.506
      },
      {
        "concept": "annotation",
        "relevance": 0.502
      },
      {
        "concept": "high performance",
        "relevance": 0.492
      },
      {
        "concept": "counterpart models",
        "relevance": 0.475
      },
      {
        "concept": "ClinicalBERT",
        "relevance": 0.473
      },
      {
        "concept": "learning environment",
        "relevance": 0.472
      },
      {
        "concept": "DistilBERT",
        "relevance": 0.47
      },
      {
        "concept": "BERT",
        "relevance": 0.469
      },
      {
        "concept": "recall",
        "relevance": 0.464
      },
      {
        "concept": "precision",
        "relevance": 0.45
      },
      {
        "concept": "training",
        "relevance": 0.448
      },
      {
        "concept": "embedding",
        "relevance": 0.447
      },
      {
        "concept": "automatically",
        "relevance": 0.445
      },
      {
        "concept": "network",
        "relevance": 0.437
      },
      {
        "concept": "model",
        "relevance": 0.405
      },
      {
        "concept": "LLM",
        "relevance": 0.401
      },
      {
        "concept": "concept",
        "relevance": 0.395
      },
      {
        "concept": "language",
        "relevance": 0.39
      },
      {
        "concept": "performance",
        "relevance": 0.39
      },
      {
        "concept": "data",
        "relevance": 0.377
      },
      {
        "concept": "labeling",
        "relevance": 0.376
      },
      {
        "concept": "applications",
        "relevance": 0.368
      },
      {
        "concept": "environment",
        "relevance": 0.365
      },
      {
        "concept": "words",
        "relevance": 0.364
      },
      {
        "concept": "explicit",
        "relevance": 0.354
      },
      {
        "concept": "categories",
        "relevance": 0.347
      },
      {
        "concept": "method",
        "relevance": 0.343
      },
      {
        "concept": "efficiency",
        "relevance": 0.34
      },
      {
        "concept": "methodology",
        "relevance": 0.339
      },
      {
        "concept": "experiments",
        "relevance": 0.326
      },
      {
        "concept": "process",
        "relevance": 0.308
      },
      {
        "concept": "extraction",
        "relevance": 0.305
      },
      {
        "concept": "results",
        "relevance": 0.299
      },
      {
        "concept": "test",
        "relevance": 0.296
      },
      {
        "concept": "counterparts",
        "relevance": 0.25
      },
      {
        "concept": "treatment",
        "relevance": 0.233
      },
      {
        "concept": "clinical application",
        "relevance": 0.227
      },
      {
        "concept": "study",
        "relevance": 0.219
      },
      {
        "concept": "initiation",
        "relevance": 0.219
      },
      {
        "concept": "approach",
        "relevance": 0.201
      },
      {
        "concept": "problem",
        "relevance": 0.177
      }
    ]
  }
]