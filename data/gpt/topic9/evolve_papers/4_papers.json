{
  "papers": [
    {
      "paperId": "pub.1162852356",
      "doi": "10.3390/computers12080151",
      "title": "Convolutional Neural Networks: A Survey",
      "year": 2023,
      "citationCount": 340,
      "fieldCitationRatio": 206.76,
      "abstract": "Artificial intelligence (AI) has become a cornerstone of modern technology, revolutionizing industries from healthcare to finance. Convolutional neural networks (CNNs) are a subset of AI that have emerged as a powerful tool for various tasks including image recognition, speech recognition, natural language processing (NLP), and even in the field of genomics, where they have been utilized to classify DNA sequences. This paper provides a comprehensive overview of CNNs and their applications in image recognition tasks. It first introduces the fundamentals of CNNs, including the layers of CNNs, convolution operation (Conv_Op), Feat_Maps, activation functions (Activ_Func), and training methods. It then discusses several popular CNN architectures such as LeNet, AlexNet, VGG, ResNet, and InceptionNet, and compares their performance. It also examines when to use CNNs, their advantages and limitations, and provides recommendations for developers and data scientists, including preprocessing the data, choosing appropriate hyperparameters (Hyper_Param), and evaluating model performance. It further explores the existing platforms and libraries for CNNs such as TensorFlow, Keras, PyTorch, Caffe, and MXNet, and compares their features and functionalities. Moreover, it estimates the cost of using CNNs and discusses potential cost-saving strategies. Finally, it reviews recent developments in CNNs, including attention mechanisms, capsule networks, transfer learning, adversarial training, quantization and compression, and enhancing the reliability and efficiency of CNNs through formal methods. The paper is concluded by summarizing the key takeaways and discussing the future directions of CNN research and development.",
      "reference_ids": [
        "pub.1125786875",
        "pub.1117375291",
        "pub.1101393448",
        "pub.1160781055",
        "pub.1123731001",
        "pub.1123202829",
        "pub.1144701632",
        "pub.1132869521",
        "pub.1124672124",
        "pub.1132783862",
        "pub.1139915316",
        "pub.1144983156",
        "pub.1150076584",
        "pub.1142659874",
        "pub.1149902301",
        "pub.1156575308",
        "pub.1125041187",
        "pub.1094544787",
        "pub.1009321967",
        "pub.1114228425",
        "pub.1126150875",
        "pub.1130481913",
        "pub.1137039895",
        "pub.1122513648",
        "pub.1094087390",
        "pub.1110420054",
        "pub.1110535344",
        "pub.1150967742",
        "pub.1156308909",
        "pub.1146286863",
        "pub.1119901626",
        "pub.1085962510",
        "pub.1103164678",
        "pub.1153850995",
        "pub.1121268462",
        "pub.1131886054",
        "pub.1137557648",
        "pub.1147199550",
        "pub.1147263903",
        "pub.1156685749",
        "pub.1147189385",
        "pub.1146139004",
        "pub.1125952040",
        "pub.1125684310",
        "pub.1156169238",
        "pub.1048213032",
        "pub.1122397015",
        "pub.1137854516",
        "pub.1134757643",
        "pub.1159411225",
        "pub.1154718868",
        "pub.1145510853",
        "pub.1048906442",
        "pub.1152934824",
        "pub.1122629162",
        "pub.1136616116",
        "pub.1144557672",
        "pub.1150000242",
        "pub.1157592603",
        "pub.1149937534",
        "pub.1121119215",
        "pub.1092954770",
        "pub.1095711658",
        "pub.1125165962",
        "pub.1152143204",
        "pub.1132352369",
        "pub.1156349557",
        "pub.1133590988",
        "pub.1084526515",
        "pub.1061658622",
        "pub.1120198175",
        "pub.1134878093",
        "pub.1138403244",
        "pub.1121257770",
        "pub.1121996834",
        "pub.1158226052",
        "pub.1111909356",
        "pub.1105945287",
        "pub.1125827700",
        "pub.1153623809",
        "pub.1135795334",
        "pub.1042245350",
        "pub.1107019611",
        "pub.1153511326",
        "pub.1067441956",
        "pub.1134333553",
        "pub.1126900364",
        "pub.1155929399",
        "pub.1111309854",
        "pub.1153188905",
        "pub.1134895728",
        "pub.1129424823",
        "pub.1134799751",
        "pub.1155035673",
        "pub.1120774142",
        "pub.1123538436",
        "pub.1141879018",
        "pub.1157160654",
        "pub.1141969549",
        "pub.1146438918",
        "pub.1154995834",
        "pub.1127526642",
        "pub.1139876555",
        "pub.1147386534",
        "pub.1143750593",
        "pub.1134779881",
        "pub.1107906150",
        "pub.1127724340",
        "pub.1134804002",
        "pub.1105258199",
        "pub.1154119523",
        "pub.1022557095",
        "pub.1105752195",
        "pub.1134334540",
        "pub.1086005516",
        "pub.1123469235",
        "pub.1127720559",
        "pub.1142513833",
        "pub.1150422034",
        "pub.1152739325",
        "pub.1133592177",
        "pub.1158056553",
        "pub.1113899513",
        "pub.1155683509",
        "pub.1110481787",
        "pub.1103242128",
        "pub.1104478124",
        "pub.1010020120",
        "pub.1002124872",
        "pub.1138415240",
        "pub.1157915744",
        "pub.1113891096",
        "pub.1148799087",
        "pub.1132654618",
        "pub.1136633587",
        "pub.1121400268",
        "pub.1158522388",
        "pub.1107644811",
        "pub.1105760271",
        "pub.1146524559",
        "pub.1100478653",
        "pub.1101401728",
        "pub.1112616834",
        "pub.1130926336",
        "pub.1137548656",
        "pub.1144476590",
        "pub.1112778382",
        "pub.1151893937",
        "pub.1103542813",
        "pub.1092184633",
        "pub.1155321567",
        "pub.1112017416",
        "pub.1138444702",
        "pub.1095686333",
        "pub.1124550561",
        "pub.1141774658",
        "pub.1127594370",
        "pub.1139552558",
        "pub.1034902741",
        "pub.1112393993",
        "pub.1111012183",
        "pub.1084824986",
        "pub.1137808468",
        "pub.1122340054",
        "pub.1131574022",
        "pub.1150905020",
        "pub.1124419254",
        "pub.1092592561",
        "pub.1155172791",
        "pub.1156416060",
        "pub.1116851118",
        "pub.1155645697",
        "pub.1139044513",
        "pub.1061614442",
        "pub.1150683763",
        "pub.1138787573",
        "pub.1107669895",
        "pub.1110334319",
        "pub.1130420380",
        "pub.1100792561",
        "pub.1125296175",
        "pub.1149384111",
        "pub.1134387586",
        "pub.1123251033",
        "pub.1145056101",
        "pub.1134306461",
        "pub.1144759430",
        "pub.1135330462",
        "pub.1112286303",
        "pub.1125648096",
        "pub.1140815901",
        "pub.1127778678",
        "pub.1110102034",
        "pub.1158511309",
        "pub.1138755051",
        "pub.1110091969",
        "pub.1101392464"
      ],
      "concepts_scores": [
        {
          "concept": "convolutional neural network",
          "relevance": 0.861
        },
        {
          "concept": "natural language processing",
          "relevance": 0.795
        },
        {
          "concept": "artificial intelligence",
          "relevance": 0.753
        },
        {
          "concept": "layers of convolutional neural networks",
          "relevance": 0.721
        },
        {
          "concept": "efficiency of convolutional neural networks",
          "relevance": 0.717
        },
        {
          "concept": "convolutional neural network architecture",
          "relevance": 0.71
        },
        {
          "concept": "image recognition tasks",
          "relevance": 0.695
        },
        {
          "concept": "adversarial training",
          "relevance": 0.646
        },
        {
          "concept": "capsule network",
          "relevance": 0.642
        },
        {
          "concept": "image recognition",
          "relevance": 0.638
        },
        {
          "concept": "attention mechanism",
          "relevance": 0.638
        },
        {
          "concept": "transfer learning",
          "relevance": 0.637
        },
        {
          "concept": "speech recognition",
          "relevance": 0.633
        },
        {
          "concept": "CNN research",
          "relevance": 0.632
        },
        {
          "concept": "neural network",
          "relevance": 0.628
        },
        {
          "concept": "activation function",
          "relevance": 0.626
        },
        {
          "concept": "convolution operation",
          "relevance": 0.625
        },
        {
          "concept": "data scientists",
          "relevance": 0.625
        },
        {
          "concept": "language processing",
          "relevance": 0.624
        },
        {
          "concept": "recognition task",
          "relevance": 0.615
        },
        {
          "concept": "training methods",
          "relevance": 0.58
        },
        {
          "concept": "network",
          "relevance": 0.541
        },
        {
          "concept": "revolutionizing industries",
          "relevance": 0.534
        },
        {
          "concept": "task",
          "relevance": 0.526
        },
        {
          "concept": "model performance",
          "relevance": 0.525
        },
        {
          "concept": "MXNet",
          "relevance": 0.509
        },
        {
          "concept": "recognition",
          "relevance": 0.505
        },
        {
          "concept": "PyTorch",
          "relevance": 0.505
        },
        {
          "concept": "LeNet",
          "relevance": 0.504
        },
        {
          "concept": "VGG",
          "relevance": 0.503
        },
        {
          "concept": "TensorFlow",
          "relevance": 0.503
        },
        {
          "concept": "ResNet",
          "relevance": 0.502
        },
        {
          "concept": "AlexNet",
          "relevance": 0.501
        },
        {
          "concept": "InceptionNet",
          "relevance": 0.501
        },
        {
          "concept": "Keras",
          "relevance": 0.499
        },
        {
          "concept": "Caffe",
          "relevance": 0.499
        },
        {
          "concept": "hyperparameters",
          "relevance": 0.494
        },
        {
          "concept": "convolution",
          "relevance": 0.487
        },
        {
          "concept": "performance",
          "relevance": 0.483
        },
        {
          "concept": "modern technology",
          "relevance": 0.481
        },
        {
          "concept": "images",
          "relevance": 0.479
        },
        {
          "concept": "training",
          "relevance": 0.479
        },
        {
          "concept": "quantization",
          "relevance": 0.476
        },
        {
          "concept": "architecture",
          "relevance": 0.471
        },
        {
          "concept": "intelligence",
          "relevance": 0.467
        },
        {
          "concept": "evaluate model performance",
          "relevance": 0.455
        },
        {
          "concept": "learning",
          "relevance": 0.447
        },
        {
          "concept": "library",
          "relevance": 0.445
        },
        {
          "concept": "platform",
          "relevance": 0.438
        },
        {
          "concept": "comprehensive overview",
          "relevance": 0.436
        },
        {
          "concept": "technology",
          "relevance": 0.415
        },
        {
          "concept": "speech",
          "relevance": 0.415
        },
        {
          "concept": "features",
          "relevance": 0.401
        },
        {
          "concept": "applications",
          "relevance": 0.393
        },
        {
          "concept": "data",
          "relevance": 0.383
        },
        {
          "concept": "operation",
          "relevance": 0.381
        },
        {
          "concept": "cost",
          "relevance": 0.379
        },
        {
          "concept": "reliability",
          "relevance": 0.376
        },
        {
          "concept": "method",
          "relevance": 0.367
        },
        {
          "concept": "compression",
          "relevance": 0.363
        },
        {
          "concept": "efficiency",
          "relevance": 0.363
        },
        {
          "concept": "healthcare",
          "relevance": 0.359
        },
        {
          "concept": "function",
          "relevance": 0.352
        },
        {
          "concept": "recent developments",
          "relevance": 0.348
        },
        {
          "concept": "development",
          "relevance": 0.347
        },
        {
          "concept": "research",
          "relevance": 0.338
        },
        {
          "concept": "attention",
          "relevance": 0.332
        },
        {
          "concept": "process",
          "relevance": 0.329
        },
        {
          "concept": "industry",
          "relevance": 0.328
        },
        {
          "concept": "takeaway",
          "relevance": 0.328
        },
        {
          "concept": "field of genomics",
          "relevance": 0.326
        },
        {
          "concept": "recommendations",
          "relevance": 0.323
        },
        {
          "concept": "scientists",
          "relevance": 0.318
        },
        {
          "concept": "strategies",
          "relevance": 0.318
        },
        {
          "concept": "review recent developments",
          "relevance": 0.312
        },
        {
          "concept": "direction",
          "relevance": 0.309
        },
        {
          "concept": "field",
          "relevance": 0.3
        },
        {
          "concept": "limitations",
          "relevance": 0.299
        },
        {
          "concept": "layer",
          "relevance": 0.286
        },
        {
          "concept": "sequence",
          "relevance": 0.28
        },
        {
          "concept": "fundamentals",
          "relevance": 0.271
        },
        {
          "concept": "transfer",
          "relevance": 0.254
        },
        {
          "concept": "mechanism",
          "relevance": 0.243
        },
        {
          "concept": "finance",
          "relevance": 0.241
        },
        {
          "concept": "cost-saving strategy",
          "relevance": 0.222
        },
        {
          "concept": "potential cost-saving strategy",
          "relevance": 0.206
        },
        {
          "concept": "activity",
          "relevance": 0.171
        },
        {
          "concept": "DNA sequences",
          "relevance": 0.167
        },
        {
          "concept": "capsule",
          "relevance": 0.162
        },
        {
          "concept": "DNA",
          "relevance": 0.146
        },
        {
          "concept": "genome",
          "relevance": 0.116
        }
      ]
    },
    {
      "paperId": "pub.1156685749",
      "doi": "10.3390/su15075930",
      "title": "A Study of CNN and Transfer Learning in Medical Imaging: Advantages, Challenges, Future Scope",
      "year": 2023,
      "citationCount": 304,
      "fieldCitationRatio": 184.86,
      "abstract": "This paper presents a comprehensive study of Convolutional Neural Networks (CNN) and transfer learning in the context of medical imaging. Medical imaging plays a critical role in the diagnosis and treatment of diseases, and CNN-based models have demonstrated significant improvements in image analysis and classification tasks. Transfer learning, which involves reusing pre-trained CNN models, has also shown promise in addressing challenges related to small datasets and limited computational resources. This paper reviews the advantages of CNN and transfer learning in medical imaging, including improved accuracy, reduced time and resource requirements, and the ability to address class imbalances. It also discusses challenges, such as the need for large and diverse datasets, and the limited interpretability of deep learning models. What factors contribute to the success of these networks? How are they fashioned, exactly? What motivated them to build the structures that they did? Finally, the paper presents current and future research directions and opportunities, including the development of specialized architectures and the exploration of new modalities and applications for medical imaging using CNN and transfer learning techniques. Overall, the paper highlights the significant potential of CNN and transfer learning in the field of medical imaging, while also acknowledging the need for continued research and development to overcome existing challenges and limitations.",
      "reference_ids": [
        "pub.1020788182",
        "pub.1149929714",
        "pub.1122596670",
        "pub.1125166823",
        "pub.1128368920",
        "pub.1129782323",
        "pub.1131701531",
        "pub.1136806687",
        "pub.1141808767",
        "pub.1099760331",
        "pub.1140576677",
        "pub.1147578027",
        "pub.1151327611",
        "pub.1148843027",
        "pub.1152055499",
        "pub.1144681770",
        "pub.1095843442",
        "pub.1127608445",
        "pub.1136381437",
        "pub.1043583090",
        "pub.1138091392",
        "pub.1146128987",
        "pub.1093715844",
        "pub.1139414550",
        "pub.1117942452",
        "pub.1128498683",
        "pub.1084896429",
        "pub.1020852238",
        "pub.1131890874",
        "pub.1131073592",
        "pub.1041504602",
        "pub.1135786328",
        "pub.1143936494",
        "pub.1094981103",
        "pub.1139494734",
        "pub.1117799804",
        "pub.1139455256",
        "pub.1085642448",
        "pub.1061179979",
        "pub.1132976410",
        "pub.1142179066",
        "pub.1095689025",
        "pub.1024899264",
        "pub.1093497718",
        "pub.1123676397",
        "pub.1130791234",
        "pub.1136829719",
        "pub.1148061673",
        "pub.1138090108",
        "pub.1140324097",
        "pub.1147325349",
        "pub.1085598131",
        "pub.1153580719",
        "pub.1150260070",
        "pub.1130129740",
        "pub.1116677770",
        "pub.1147783017",
        "pub.1094612410",
        "pub.1150927390",
        "pub.1009767488",
        "pub.1149659230",
        "pub.1127720559",
        "pub.1122747050",
        "pub.1141409373",
        "pub.1121129556",
        "pub.1140890875",
        "pub.1093359587",
        "pub.1115983167",
        "pub.1046426641",
        "pub.1099654127",
        "pub.1140667610",
        "pub.1111101170",
        "pub.1151218784",
        "pub.1130682342",
        "pub.1112063952"
      ],
      "concepts_scores": [
        {
          "concept": "transfer learning",
          "relevance": 0.733
        },
        {
          "concept": "medical images",
          "relevance": 0.713
        },
        {
          "concept": "study of convolutional neural networks",
          "relevance": 0.669
        },
        {
          "concept": "pre-trained CNN models",
          "relevance": 0.651
        },
        {
          "concept": "interpretability of deep learning models",
          "relevance": 0.642
        },
        {
          "concept": "context of medical imaging",
          "relevance": 0.64
        },
        {
          "concept": "convolutional neural network",
          "relevance": 0.635
        },
        {
          "concept": "transfer learning technique",
          "relevance": 0.634
        },
        {
          "concept": "CNN-based models",
          "relevance": 0.633
        },
        {
          "concept": "field of medical imaging",
          "relevance": 0.628
        },
        {
          "concept": "deep learning models",
          "relevance": 0.626
        },
        {
          "concept": "classification task",
          "relevance": 0.592
        },
        {
          "concept": "CNN model",
          "relevance": 0.586
        },
        {
          "concept": "neural network",
          "relevance": 0.579
        },
        {
          "concept": "computational resources",
          "relevance": 0.579
        },
        {
          "concept": "learning techniques",
          "relevance": 0.578
        },
        {
          "concept": "specialized architectures",
          "relevance": 0.567
        },
        {
          "concept": "diverse datasets",
          "relevance": 0.564
        },
        {
          "concept": "learning models",
          "relevance": 0.564
        },
        {
          "concept": "resource requirements",
          "relevance": 0.552
        },
        {
          "concept": "improved accuracy",
          "relevance": 0.518
        },
        {
          "concept": "research directions",
          "relevance": 0.518
        },
        {
          "concept": "learning",
          "relevance": 0.514
        },
        {
          "concept": "future scopes",
          "relevance": 0.507
        },
        {
          "concept": "dataset",
          "relevance": 0.506
        },
        {
          "concept": "network",
          "relevance": 0.499
        },
        {
          "concept": "images",
          "relevance": 0.477
        },
        {
          "concept": "image analysis",
          "relevance": 0.471
        },
        {
          "concept": "reduce time",
          "relevance": 0.465
        },
        {
          "concept": "architecture",
          "relevance": 0.434
        },
        {
          "concept": "task",
          "relevance": 0.419
        },
        {
          "concept": "resources",
          "relevance": 0.418
        },
        {
          "concept": "classification",
          "relevance": 0.416
        },
        {
          "concept": "accuracy",
          "relevance": 0.4
        },
        {
          "concept": "model",
          "relevance": 0.389
        },
        {
          "concept": "challenges",
          "relevance": 0.382
        },
        {
          "concept": "requirements",
          "relevance": 0.382
        },
        {
          "concept": "applications",
          "relevance": 0.363
        },
        {
          "concept": "research",
          "relevance": 0.361
        },
        {
          "concept": "technique",
          "relevance": 0.344
        },
        {
          "concept": "comprehensive study",
          "relevance": 0.341
        },
        {
          "concept": "exploration",
          "relevance": 0.337
        },
        {
          "concept": "context",
          "relevance": 0.318
        },
        {
          "concept": "advantage",
          "relevance": 0.317
        },
        {
          "concept": "improvement",
          "relevance": 0.308
        },
        {
          "concept": "opportunities",
          "relevance": 0.308
        },
        {
          "concept": "development",
          "relevance": 0.305
        },
        {
          "concept": "success",
          "relevance": 0.295
        },
        {
          "concept": "scope",
          "relevance": 0.294
        },
        {
          "concept": "transfer",
          "relevance": 0.293
        },
        {
          "concept": "time",
          "relevance": 0.292
        },
        {
          "concept": "direction",
          "relevance": 0.286
        },
        {
          "concept": "field",
          "relevance": 0.277
        },
        {
          "concept": "limitations",
          "relevance": 0.275
        },
        {
          "concept": "imbalance",
          "relevance": 0.274
        },
        {
          "concept": "future",
          "relevance": 0.266
        },
        {
          "concept": "modalities",
          "relevance": 0.264
        },
        {
          "concept": "interpretation",
          "relevance": 0.262
        },
        {
          "concept": "structure",
          "relevance": 0.245
        },
        {
          "concept": "analysis",
          "relevance": 0.243
        },
        {
          "concept": "treatment of diseases",
          "relevance": 0.231
        },
        {
          "concept": "study",
          "relevance": 0.22
        },
        {
          "concept": "potential",
          "relevance": 0.205
        },
        {
          "concept": "factors",
          "relevance": 0.18
        },
        {
          "concept": "diagnosis",
          "relevance": 0.176
        },
        {
          "concept": "medication",
          "relevance": 0.13
        },
        {
          "concept": "disease",
          "relevance": 0.128
        },
        {
          "concept": "treatment",
          "relevance": 0.126
        }
      ]
    },
    {
      "paperId": "pub.1141808767",
      "doi": "10.3390/electronics10202470",
      "title": "CNN Variants for Computer Vision: History, Architecture, Application, Challenges and Future Scope",
      "year": 2021,
      "citationCount": 635,
      "fieldCitationRatio": 161.47,
      "abstract": "Computer vision is becoming an increasingly trendy word in the area of image processing. With the emergence of computer vision applications, there is a significant demand to recognize objects automatically. Deep CNN (convolution neural network) has benefited the computer vision community by producing excellent results in video processing, object recognition, picture classification and segmentation, natural language processing, speech recognition, and many other fields. Furthermore, the introduction of large amounts of data and readily available hardware has opened new avenues for CNN study. Several inspirational concepts for the progress of CNN have been investigated, including alternative activation functions, regularization, parameter optimization, and architectural advances. Furthermore, achieving innovations in architecture results in a tremendous enhancement in the capacity of the deep CNN. Significant emphasis has been given to leveraging channel and spatial information, with a depth of architecture and information processing via multi-path. This survey paper focuses mainly on the primary taxonomy and newly released deep CNN architectures, and it divides numerous recent developments in CNN architectures into eight groups. Spatial exploitation, multi-path, depth, breadth, dimension, channel boosting, feature-map exploitation, and attention-based CNN are the eight categories. The main contribution of this manuscript is in comparing various architectural evolutions in CNN by its architectural change, strengths, and weaknesses. Besides, it also includes an explanation of the CNN’s components, the strengths and weaknesses of various CNN variants, research gap or open challenges, CNN applications, and the future research direction.",
      "reference_ids": [
        "pub.1113540189",
        "pub.1100060520",
        "pub.1061379809",
        "pub.1110720879",
        "pub.1003948507",
        "pub.1061745216",
        "pub.1068001387",
        "pub.1100060307",
        "pub.1105148233",
        "pub.1095650815",
        "pub.1121060437",
        "pub.1099110648",
        "pub.1094291017",
        "pub.1113830167",
        "pub.1035884169",
        "pub.1095072289",
        "pub.1092833667",
        "pub.1027821665",
        "pub.1095536743",
        "pub.1106997065",
        "pub.1093254042",
        "pub.1115171653",
        "pub.1094910418",
        "pub.1000622690",
        "pub.1093068190",
        "pub.1094709720",
        "pub.1083781379",
        "pub.1148955875",
        "pub.1061379783",
        "pub.1093475332",
        "pub.1108912650",
        "pub.1061530052",
        "pub.1107569381",
        "pub.1099595076",
        "pub.1093497718",
        "pub.1110720850",
        "pub.1133759749",
        "pub.1000266427",
        "pub.1007568599",
        "pub.1095847715",
        "pub.1059171386",
        "pub.1117786490",
        "pub.1061744884",
        "pub.1110720231",
        "pub.1137507049",
        "pub.1111641418",
        "pub.1126862968",
        "pub.1095379009",
        "pub.1095213418",
        "pub.1061744395",
        "pub.1133368819",
        "pub.1116643643",
        "pub.1012313573"
      ],
      "concepts_scores": [
        {
          "concept": "CNN variants",
          "relevance": 0.742
        },
        {
          "concept": "computer vision",
          "relevance": 0.735
        },
        {
          "concept": "deep CNN",
          "relevance": 0.733
        },
        {
          "concept": "CNN architecture",
          "relevance": 0.733
        },
        {
          "concept": "multi-path",
          "relevance": 0.723
        },
        {
          "concept": "area of image processing",
          "relevance": 0.698
        },
        {
          "concept": "alternative activation functions",
          "relevance": 0.691
        },
        {
          "concept": "computer vision applications",
          "relevance": 0.689
        },
        {
          "concept": "computer vision community",
          "relevance": 0.688
        },
        {
          "concept": "deep CNN architecture",
          "relevance": 0.681
        },
        {
          "concept": "depth of architecture",
          "relevance": 0.68
        },
        {
          "concept": "natural language processing",
          "relevance": 0.679
        },
        {
          "concept": "vision applications",
          "relevance": 0.637
        },
        {
          "concept": "vision community",
          "relevance": 0.637
        },
        {
          "concept": "video processing",
          "relevance": 0.634
        },
        {
          "concept": "channel boosting",
          "relevance": 0.634
        },
        {
          "concept": "CNN applications",
          "relevance": 0.634
        },
        {
          "concept": "CNN component",
          "relevance": 0.631
        },
        {
          "concept": "speech recognition",
          "relevance": 0.626
        },
        {
          "concept": "activation function",
          "relevance": 0.619
        },
        {
          "concept": "CNN",
          "relevance": 0.617
        },
        {
          "concept": "language processing",
          "relevance": 0.617
        },
        {
          "concept": "object recognition",
          "relevance": 0.612
        },
        {
          "concept": "survey paper",
          "relevance": 0.611
        },
        {
          "concept": "architecture evolution",
          "relevance": 0.607
        },
        {
          "concept": "image processing",
          "relevance": 0.604
        },
        {
          "concept": "architectural advances",
          "relevance": 0.597
        },
        {
          "concept": "spatial exploitation",
          "relevance": 0.593
        },
        {
          "concept": "spatial information",
          "relevance": 0.58
        },
        {
          "concept": "architecture",
          "relevance": 0.579
        },
        {
          "concept": "leverage channel",
          "relevance": 0.579
        },
        {
          "concept": "computer",
          "relevance": 0.561
        },
        {
          "concept": "research directions",
          "relevance": 0.555
        },
        {
          "concept": "future scopes",
          "relevance": 0.543
        },
        {
          "concept": "parameter optimization",
          "relevance": 0.542
        },
        {
          "concept": "inspired concepts",
          "relevance": 0.528
        },
        {
          "concept": "information processing",
          "relevance": 0.51
        },
        {
          "concept": "architectural changes",
          "relevance": 0.51
        },
        {
          "concept": "recognition",
          "relevance": 0.499
        },
        {
          "concept": "research gap",
          "relevance": 0.491
        },
        {
          "concept": "vision",
          "relevance": 0.49
        },
        {
          "concept": "information",
          "relevance": 0.48
        },
        {
          "concept": "Deep",
          "relevance": 0.477
        },
        {
          "concept": "hardware",
          "relevance": 0.476
        },
        {
          "concept": "video",
          "relevance": 0.475
        },
        {
          "concept": "channel",
          "relevance": 0.474
        },
        {
          "concept": "applications",
          "relevance": 0.472
        },
        {
          "concept": "significant demand",
          "relevance": 0.462
        },
        {
          "concept": "classification",
          "relevance": 0.445
        },
        {
          "concept": "objective",
          "relevance": 0.438
        },
        {
          "concept": "exploitation",
          "relevance": 0.432
        },
        {
          "concept": "optimization",
          "relevance": 0.427
        },
        {
          "concept": "regularization",
          "relevance": 0.421
        },
        {
          "concept": "speech",
          "relevance": 0.41
        },
        {
          "concept": "process",
          "relevance": 0.402
        },
        {
          "concept": "taxonomy",
          "relevance": 0.402
        },
        {
          "concept": "weakness",
          "relevance": 0.392
        },
        {
          "concept": "segments",
          "relevance": 0.389
        },
        {
          "concept": "research",
          "relevance": 0.387
        },
        {
          "concept": "words",
          "relevance": 0.385
        },
        {
          "concept": "significant emphasis",
          "relevance": 0.382
        },
        {
          "concept": "concept",
          "relevance": 0.36
        },
        {
          "concept": "boost",
          "relevance": 0.356
        },
        {
          "concept": "challenges",
          "relevance": 0.353
        },
        {
          "concept": "variants",
          "relevance": 0.352
        },
        {
          "concept": "recent developments",
          "relevance": 0.344
        },
        {
          "concept": "advances",
          "relevance": 0.337
        },
        {
          "concept": "components",
          "relevance": 0.336
        },
        {
          "concept": "data",
          "relevance": 0.327
        },
        {
          "concept": "demand",
          "relevance": 0.321
        },
        {
          "concept": "dimensions",
          "relevance": 0.32
        },
        {
          "concept": "categories",
          "relevance": 0.317
        },
        {
          "concept": "scope",
          "relevance": 0.315
        },
        {
          "concept": "results",
          "relevance": 0.315
        },
        {
          "concept": "direction",
          "relevance": 0.306
        },
        {
          "concept": "manuscript",
          "relevance": 0.303
        },
        {
          "concept": "function",
          "relevance": 0.3
        },
        {
          "concept": "parameters",
          "relevance": 0.299
        },
        {
          "concept": "field",
          "relevance": 0.296
        },
        {
          "concept": "contribution",
          "relevance": 0.293
        },
        {
          "concept": "depth",
          "relevance": 0.292
        },
        {
          "concept": "gap",
          "relevance": 0.286
        },
        {
          "concept": "future",
          "relevance": 0.285
        },
        {
          "concept": "strength",
          "relevance": 0.283
        },
        {
          "concept": "development",
          "relevance": 0.282
        },
        {
          "concept": "introduction",
          "relevance": 0.268
        },
        {
          "concept": "evolution",
          "relevance": 0.268
        },
        {
          "concept": "community",
          "relevance": 0.265
        },
        {
          "concept": "area",
          "relevance": 0.262
        },
        {
          "concept": "capacity",
          "relevance": 0.262
        },
        {
          "concept": "emergency",
          "relevance": 0.252
        },
        {
          "concept": "breadth",
          "relevance": 0.248
        },
        {
          "concept": "survey",
          "relevance": 0.246
        },
        {
          "concept": "emphasis",
          "relevance": 0.241
        },
        {
          "concept": "paper",
          "relevance": 0.216
        },
        {
          "concept": "explanation",
          "relevance": 0.212
        },
        {
          "concept": "progression",
          "relevance": 0.204
        },
        {
          "concept": "study",
          "relevance": 0.187
        },
        {
          "concept": "changes",
          "relevance": 0.181
        },
        {
          "concept": "history",
          "relevance": 0.167
        },
        {
          "concept": "group",
          "relevance": 0.16
        },
        {
          "concept": "increase",
          "relevance": 0.147
        }
      ]
    },
    {
      "paperId": "pub.1110720879",
      "doi": "10.1109/cvpr.2018.00745",
      "title": "Squeeze-and-Excitation Networks",
      "year": 2018,
      "citationCount": 26747,
      "fieldCitationRatio": 5951.8,
      "abstract": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ∼25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.",
      "reference_ids": [
        "pub.1038140272",
        "pub.1045321436",
        "pub.1099426768",
        "pub.1093828312",
        "pub.1003844693",
        "pub.1095843442",
        "pub.1095850430",
        "pub.1009767488",
        "pub.1022557095",
        "pub.1090555548",
        "pub.1093626237",
        "pub.1061745117",
        "pub.1049647714",
        "pub.1095851020",
        "pub.1094165002",
        "pub.1036345336",
        "pub.1094869203",
        "pub.1095646840",
        "pub.1082711856",
        "pub.1095837716",
        "pub.1093359587",
        "pub.1093672793",
        "pub.1061644414",
        "pub.1095839391",
        "pub.1004476131",
        "pub.1085642448",
        "pub.1093497718",
        "pub.1095180230",
        "pub.1095850372",
        "pub.1061156881",
        "pub.1094291017",
        "pub.1095838414",
        "pub.1095837190"
      ],
      "concepts_scores": [
        {
          "concept": "adaptively recalibrates channel-wise feature responses",
          "relevance": 0.661
        },
        {
          "concept": "state-of-the-art deep architectures",
          "relevance": 0.659
        },
        {
          "concept": "channel-wise feature responses",
          "relevance": 0.652
        },
        {
          "concept": "channel-wise information",
          "relevance": 0.631
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.629
        },
        {
          "concept": "local receptive fields",
          "relevance": 0.626
        },
        {
          "concept": "convolutional neural network",
          "relevance": 0.625
        },
        {
          "concept": "top-5 error",
          "relevance": 0.621
        },
        {
          "concept": "extract informative features",
          "relevance": 0.62
        },
        {
          "concept": "significant performance improvement",
          "relevance": 0.616
        },
        {
          "concept": "deep architecture",
          "relevance": 0.585
        },
        {
          "concept": "representational power",
          "relevance": 0.578
        },
        {
          "concept": "feature responses",
          "relevance": 0.577
        },
        {
          "concept": "top-5",
          "relevance": 0.571
        },
        {
          "concept": "neural network",
          "relevance": 0.57
        },
        {
          "concept": "informative features",
          "relevance": 0.569
        },
        {
          "concept": "convolution operation",
          "relevance": 0.568
        },
        {
          "concept": "computational cost",
          "relevance": 0.553
        },
        {
          "concept": "performance improvement",
          "relevance": 0.547
        },
        {
          "concept": "model interdependencies",
          "relevance": 0.535
        },
        {
          "concept": "receptive fields",
          "relevance": 0.527
        },
        {
          "concept": "SENet",
          "relevance": 0.52
        },
        {
          "concept": "architectural units",
          "relevance": 0.496
        },
        {
          "concept": "architecture",
          "relevance": 0.495
        },
        {
          "concept": "network",
          "relevance": 0.492
        },
        {
          "concept": "spatial encoding",
          "relevance": 0.484
        },
        {
          "concept": "ILSVRC",
          "relevance": 0.462
        },
        {
          "concept": "channel relationships",
          "relevance": 0.444
        },
        {
          "concept": "convolution",
          "relevance": 0.442
        },
        {
          "concept": "encoding",
          "relevance": 0.439
        },
        {
          "concept": "channel",
          "relevance": 0.436
        },
        {
          "concept": "dataset",
          "relevance": 0.43
        },
        {
          "concept": "code",
          "relevance": 0.419
        },
        {
          "concept": "classification",
          "relevance": 0.409
        },
        {
          "concept": "block",
          "relevance": 0.401
        },
        {
          "concept": "information",
          "relevance": 0.381
        },
        {
          "concept": "error",
          "relevance": 0.377
        },
        {
          "concept": "features",
          "relevance": 0.364
        },
        {
          "concept": "submission",
          "relevance": 0.355
        },
        {
          "concept": "improvement",
          "relevance": 0.351
        },
        {
          "concept": "operation",
          "relevance": 0.346
        },
        {
          "concept": "cost",
          "relevance": 0.344
        },
        {
          "concept": "adaptation",
          "relevance": 0.335
        },
        {
          "concept": "power",
          "relevance": 0.318
        },
        {
          "concept": "model",
          "relevance": 0.315
        },
        {
          "concept": "interdependence",
          "relevance": 0.305
        },
        {
          "concept": "field",
          "relevance": 0.301
        },
        {
          "concept": "benefits",
          "relevance": 0.285
        },
        {
          "concept": "entry",
          "relevance": 0.264
        },
        {
          "concept": "Se",
          "relevance": 0.252
        },
        {
          "concept": "units",
          "relevance": 0.223
        },
        {
          "concept": "relationship",
          "relevance": 0.207
        },
        {
          "concept": "response",
          "relevance": 0.155
        }
      ]
    },
    {
      "paperId": "pub.1148955875",
      "doi": "10.1609/aaai.v31i1.11231",
      "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
      "year": 2017,
      "citationCount": 10711,
      "fieldCitationRatio": 2159.21,
      "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.",
      "reference_ids": [
        "pub.1094869203",
        "pub.1016635886",
        "pub.1094727707"
      ],
      "concepts_scores": [
        {
          "concept": "residual connections",
          "relevance": 0.731
        },
        {
          "concept": "Inception network",
          "relevance": 0.727
        },
        {
          "concept": "Inception architecture",
          "relevance": 0.679
        },
        {
          "concept": "recognition performance",
          "relevance": 0.669
        },
        {
          "concept": "state-of-the-art performance",
          "relevance": 0.661
        },
        {
          "concept": "introduction of residual connections",
          "relevance": 0.66
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.637
        },
        {
          "concept": "deep convolutional networks",
          "relevance": 0.636
        },
        {
          "concept": "Inception-v4 network",
          "relevance": 0.636
        },
        {
          "concept": "image recognition performance",
          "relevance": 0.633
        },
        {
          "concept": "Inception-v3 network",
          "relevance": 0.617
        },
        {
          "concept": "ImageNet classification",
          "relevance": 0.596
        },
        {
          "concept": "classification task",
          "relevance": 0.59
        },
        {
          "concept": "convolutional network",
          "relevance": 0.589
        },
        {
          "concept": "Inception-ResNet",
          "relevance": 0.587
        },
        {
          "concept": "Inception-v4",
          "relevance": 0.578
        },
        {
          "concept": "Inception-v3",
          "relevance": 0.578
        },
        {
          "concept": "computational cost",
          "relevance": 0.56
        },
        {
          "concept": "test set",
          "relevance": 0.551
        },
        {
          "concept": "ILSVRC",
          "relevance": 0.541
        },
        {
          "concept": "network",
          "relevance": 0.537
        },
        {
          "concept": "architecture",
          "relevance": 0.535
        },
        {
          "concept": "traditional architecture",
          "relevance": 0.516
        },
        {
          "concept": "classification",
          "relevance": 0.504
        },
        {
          "concept": "performance",
          "relevance": 0.477
        },
        {
          "concept": "ImageNet",
          "relevance": 0.466
        },
        {
          "concept": "training",
          "relevance": 0.462
        },
        {
          "concept": "connection",
          "relevance": 0.431
        },
        {
          "concept": "task",
          "relevance": 0.418
        },
        {
          "concept": "learning",
          "relevance": 0.411
        },
        {
          "concept": "error",
          "relevance": 0.382
        },
        {
          "concept": "images",
          "relevance": 0.38
        },
        {
          "concept": "ensemble",
          "relevance": 0.373
        },
        {
          "concept": "cost",
          "relevance": 0.348
        },
        {
          "concept": "sets",
          "relevance": 0.347
        },
        {
          "concept": "challenges",
          "relevance": 0.328
        },
        {
          "concept": "thin margins",
          "relevance": 0.316
        },
        {
          "concept": "advances",
          "relevance": 0.313
        },
        {
          "concept": "Activity Scale",
          "relevance": 0.312
        },
        {
          "concept": "inception",
          "relevance": 0.307
        },
        {
          "concept": "empirical evidence",
          "relevance": 0.306
        },
        {
          "concept": "benefits",
          "relevance": 0.288
        },
        {
          "concept": "conjunction",
          "relevance": 0.263
        },
        {
          "concept": "introduction",
          "relevance": 0.25
        },
        {
          "concept": "evidence",
          "relevance": 0.245
        },
        {
          "concept": "questions",
          "relevance": 0.244
        },
        {
          "concept": "test",
          "relevance": 0.239
        },
        {
          "concept": "impact",
          "relevance": 0.233
        },
        {
          "concept": "years",
          "relevance": 0.225
        },
        {
          "concept": "scale",
          "relevance": 0.222
        },
        {
          "concept": "margin",
          "relevance": 0.208
        },
        {
          "concept": "activity",
          "relevance": 0.208
        },
        {
          "concept": "variation",
          "relevance": 0.193
        }
      ]
    },
    {
      "paperId": "pub.1146128987",
      "doi": "10.1155/2022/3264367",
      "title": "Deep Transfer Learning Approaches in Performance Analysis of Brain Tumor Classification Using MRI Images",
      "year": 2022,
      "citationCount": 237,
      "fieldCitationRatio": 53.75,
      "abstract": "Brain tumor classification is a very important and the most prominent step for assessing life-threatening abnormal tissues and providing an efficient treatment in patient recovery. To identify pathological conditions in the brain, there exist various medical imaging technologies. Magnetic Resonance Imaging (MRI) is extensively used in medical imaging due to its excellent image quality and independence from ionizing radiations. The significance of deep learning, a subset of artificial intelligence in the area of medical diagnosis applications, has macadamized the path in rapid developments for brain tumor detection from MRI to higher prediction rate. For brain tumor analysis and classification, the convolution neural network (CNN) is the most extensive and widely used deep learning algorithm. In this work, we present a comparative performance analysis of transfer learning-based CNN-pretrained VGG-16, ResNet-50, and Inception-v3 models for automatic prediction of tumor cells in the brain. Pretrained models are demonstrated on the MRI brain tumor images dataset consisting of 233 images. Our paper aims to locate brain tumors with the utilization of the VGG-16 pretrained CNN model. The performance of our model will be evaluated on accuracy. As an outcome, we can estimate that the pretrained model VGG-16 determines highly adequate results with an increase in the accuracy rate of training and validation.",
      "reference_ids": [
        "pub.1090904008",
        "pub.1134655724",
        "pub.1134397080",
        "pub.1093334105",
        "pub.1112352463",
        "pub.1107685694",
        "pub.1110816555",
        "pub.1100347056",
        "pub.1127720551",
        "pub.1120963973",
        "pub.1112574273",
        "pub.1132195501",
        "pub.1127234112",
        "pub.1128032267",
        "pub.1111721809",
        "pub.1013793089",
        "pub.1090252395",
        "pub.1120678821",
        "pub.1100084301",
        "pub.1117137498",
        "pub.1121663361",
        "pub.1128463314",
        "pub.1122597590",
        "pub.1106082735",
        "pub.1133747177",
        "pub.1130891682",
        "pub.1104265935",
        "pub.1132511084",
        "pub.1117650760",
        "pub.1093584168",
        "pub.1093211855"
      ],
      "concepts_scores": [
        {
          "concept": "convolutional neural network",
          "relevance": 0.74
        },
        {
          "concept": "VGG-16",
          "relevance": 0.687
        },
        {
          "concept": "brain tumor classification",
          "relevance": 0.68
        },
        {
          "concept": "convolutional neural network model",
          "relevance": 0.621
        },
        {
          "concept": "deep transfer learning approach",
          "relevance": 0.619
        },
        {
          "concept": "transfer learning approach",
          "relevance": 0.607
        },
        {
          "concept": "model VGG-16",
          "relevance": 0.603
        },
        {
          "concept": "deep learning algorithms",
          "relevance": 0.601
        },
        {
          "concept": "medical diagnosis applications",
          "relevance": 0.599
        },
        {
          "concept": "Inception-v3 model",
          "relevance": 0.599
        },
        {
          "concept": "brain tumor detection",
          "relevance": 0.587
        },
        {
          "concept": "brain tumor analysis",
          "relevance": 0.587
        },
        {
          "concept": "significance of deep learning",
          "relevance": 0.581
        },
        {
          "concept": "comparative performance analysis",
          "relevance": 0.579
        },
        {
          "concept": "accuracy rate of training",
          "relevance": 0.577
        },
        {
          "concept": "pretrained models",
          "relevance": 0.566
        },
        {
          "concept": "ResNet-50",
          "relevance": 0.562
        },
        {
          "concept": "image datasets",
          "relevance": 0.562
        },
        {
          "concept": "deep learning",
          "relevance": 0.559
        },
        {
          "concept": "learning algorithms",
          "relevance": 0.559
        },
        {
          "concept": "highest prediction rate",
          "relevance": 0.558
        },
        {
          "concept": "Inception-v3",
          "relevance": 0.556
        },
        {
          "concept": "neural network",
          "relevance": 0.555
        },
        {
          "concept": "medical imaging technology",
          "relevance": 0.554
        },
        {
          "concept": "medical images",
          "relevance": 0.547
        },
        {
          "concept": "artificial intelligence",
          "relevance": 0.547
        },
        {
          "concept": "automatic prediction",
          "relevance": 0.546
        },
        {
          "concept": "learning approach",
          "relevance": 0.542
        },
        {
          "concept": "performance analysis",
          "relevance": 0.536
        },
        {
          "concept": "accuracy rate",
          "relevance": 0.535
        },
        {
          "concept": "diagnosis applications",
          "relevance": 0.532
        },
        {
          "concept": "rate of training",
          "relevance": 0.52
        },
        {
          "concept": "image quality",
          "relevance": 0.509
        },
        {
          "concept": "tumor classification",
          "relevance": 0.504
        },
        {
          "concept": "magnetic resonance imaging images",
          "relevance": 0.494
        },
        {
          "concept": "classification",
          "relevance": 0.484
        },
        {
          "concept": "prediction rate",
          "relevance": 0.484
        },
        {
          "concept": "images",
          "relevance": 0.452
        },
        {
          "concept": "accuracy",
          "relevance": 0.443
        },
        {
          "concept": "convolution",
          "relevance": 0.43
        },
        {
          "concept": "performance",
          "relevance": 0.427
        },
        {
          "concept": "Deep",
          "relevance": 0.426
        },
        {
          "concept": "algorithm",
          "relevance": 0.421
        },
        {
          "concept": "magnetic resonance imaging",
          "relevance": 0.42
        },
        {
          "concept": "dataset",
          "relevance": 0.419
        },
        {
          "concept": "tumor detection",
          "relevance": 0.416
        },
        {
          "concept": "imaging technology",
          "relevance": 0.413
        },
        {
          "concept": "intelligence",
          "relevance": 0.413
        },
        {
          "concept": "network",
          "relevance": 0.413
        },
        {
          "concept": "learning",
          "relevance": 0.395
        },
        {
          "concept": "adequate results",
          "relevance": 0.392
        },
        {
          "concept": "abnormal tissue",
          "relevance": 0.383
        },
        {
          "concept": "excellent image quality",
          "relevance": 0.38
        },
        {
          "concept": "model",
          "relevance": 0.379
        },
        {
          "concept": "technology",
          "relevance": 0.366
        },
        {
          "concept": "training",
          "relevance": 0.366
        },
        {
          "concept": "path",
          "relevance": 0.356
        },
        {
          "concept": "detection",
          "relevance": 0.355
        },
        {
          "concept": "applications",
          "relevance": 0.347
        },
        {
          "concept": "tumor cells",
          "relevance": 0.327
        },
        {
          "concept": "tumor analysis",
          "relevance": 0.319
        },
        {
          "concept": "brain tumors",
          "relevance": 0.317
        },
        {
          "concept": "quality",
          "relevance": 0.315
        },
        {
          "concept": "resonance imaging",
          "relevance": 0.312
        },
        {
          "concept": "patient recovery",
          "relevance": 0.31
        },
        {
          "concept": "utilization",
          "relevance": 0.303
        },
        {
          "concept": "pathological conditions",
          "relevance": 0.298
        },
        {
          "concept": "validity",
          "relevance": 0.298
        },
        {
          "concept": "ionizing radiation",
          "relevance": 0.295
        },
        {
          "concept": "brain",
          "relevance": 0.291
        },
        {
          "concept": "magnetization",
          "relevance": 0.291
        },
        {
          "concept": "efficient treatment",
          "relevance": 0.288
        },
        {
          "concept": "results",
          "relevance": 0.282
        },
        {
          "concept": "conditions",
          "relevance": 0.271
        },
        {
          "concept": "radiation",
          "relevance": 0.257
        },
        {
          "concept": "tumor",
          "relevance": 0.257
        },
        {
          "concept": "independence",
          "relevance": 0.255
        },
        {
          "concept": "patients",
          "relevance": 0.254
        },
        {
          "concept": "development",
          "relevance": 0.252
        },
        {
          "concept": "rate",
          "relevance": 0.241
        },
        {
          "concept": "approach",
          "relevance": 0.237
        },
        {
          "concept": "increase",
          "relevance": 0.237
        },
        {
          "concept": "treatment",
          "relevance": 0.236
        },
        {
          "concept": "area",
          "relevance": 0.234
        },
        {
          "concept": "outcomes",
          "relevance": 0.234
        },
        {
          "concept": "tissue",
          "relevance": 0.232
        },
        {
          "concept": "analysis",
          "relevance": 0.232
        },
        {
          "concept": "recovery",
          "relevance": 0.231
        },
        {
          "concept": "cells",
          "relevance": 0.226
        },
        {
          "concept": "significance",
          "relevance": 0.198
        }
      ]
    },
    {
      "paperId": "pub.1117650760",
      "doi": "10.1016/j.compbiomed.2019.103345",
      "title": "Brain tumor classification using deep CNN features via transfer learning",
      "year": 2019,
      "citationCount": 1037,
      "fieldCitationRatio": 254.01,
      "abstract": "Brain tumor classification is an important problem in computer-aided diagnosis (CAD) for medical applications. This paper focuses on a 3-class classification problem to differentiate among glioma, meningioma and pituitary tumors, which form three prominent types of brain tumor. The proposed classification system adopts the concept of deep transfer learning and uses a pre-trained GoogLeNet to extract features from brain MRI images. Proven classifier models are integrated to classify the extracted features. The experiment follows a patient-level five-fold cross-validation process, on MRI dataset from figshare. The proposed system records a mean classification accuracy of 98%, outperforming all state-of-the-art methods. Other performance measures used in the study are the area under the curve (AUC), precision, recall, F-score and specificity. In addition, the paper addresses a practical aspect by evaluating the system with fewer training samples. The observations of the study imply that transfer learning is a useful technique when the availability of medical images is limited. The paper provides an analytical discussion on misclassifications also.",
      "reference_ids": [
        "pub.1094291017",
        "pub.1052051489",
        "pub.1084678648",
        "pub.1110014317",
        "pub.1103211931",
        "pub.1107273573",
        "pub.1111407017",
        "pub.1099646994",
        "pub.1095081256",
        "pub.1111101170",
        "pub.1083781158",
        "pub.1107352239",
        "pub.1036925836",
        "pub.1110514923",
        "pub.1111638359",
        "pub.1109905908",
        "pub.1100950449",
        "pub.1104265935",
        "pub.1107573740",
        "pub.1107791594",
        "pub.1013793089",
        "pub.1061718612",
        "pub.1041355599",
        "pub.1110816555",
        "pub.1113541490",
        "pub.1091100724"
      ],
      "concepts_scores": [
        {
          "concept": "transfer learning",
          "relevance": 0.64
        },
        {
          "concept": "brain tumor classification",
          "relevance": 0.636
        },
        {
          "concept": "computer-aided diagnosis",
          "relevance": 0.632
        },
        {
          "concept": "state-of-the-art methods",
          "relevance": 0.591
        },
        {
          "concept": "deep CNN features",
          "relevance": 0.575
        },
        {
          "concept": "availability of medical images",
          "relevance": 0.573
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.572
        },
        {
          "concept": "deep transfer learning",
          "relevance": 0.566
        },
        {
          "concept": "brain MRI images",
          "relevance": 0.544
        },
        {
          "concept": "CNN features",
          "relevance": 0.533
        },
        {
          "concept": "classification problem",
          "relevance": 0.529
        },
        {
          "concept": "extract features",
          "relevance": 0.525
        },
        {
          "concept": "training samples",
          "relevance": 0.523
        },
        {
          "concept": "cross-validation process",
          "relevance": 0.52
        },
        {
          "concept": "F-score",
          "relevance": 0.52
        },
        {
          "concept": "classification accuracy",
          "relevance": 0.519
        },
        {
          "concept": "classifier model",
          "relevance": 0.518
        },
        {
          "concept": "medical images",
          "relevance": 0.511
        },
        {
          "concept": "MRI datasets",
          "relevance": 0.483
        },
        {
          "concept": "tumor classification",
          "relevance": 0.471
        },
        {
          "concept": "classification",
          "relevance": 0.463
        },
        {
          "concept": "performance measures",
          "relevance": 0.45
        },
        {
          "concept": "learning",
          "relevance": 0.449
        },
        {
          "concept": "medical applications",
          "relevance": 0.426
        },
        {
          "concept": "MRI images",
          "relevance": 0.426
        },
        {
          "concept": "features",
          "relevance": 0.403
        },
        {
          "concept": "classification system",
          "relevance": 0.398
        },
        {
          "concept": "images",
          "relevance": 0.396
        },
        {
          "concept": "dataset",
          "relevance": 0.391
        },
        {
          "concept": "system",
          "relevance": 0.381
        },
        {
          "concept": "analytical discussion",
          "relevance": 0.37
        },
        {
          "concept": "Figshare",
          "relevance": 0.36
        },
        {
          "concept": "accuracy",
          "relevance": 0.358
        },
        {
          "concept": "practical aspects",
          "relevance": 0.356
        },
        {
          "concept": "recall",
          "relevance": 0.354
        },
        {
          "concept": "misclassification",
          "relevance": 0.353
        },
        {
          "concept": "performance",
          "relevance": 0.344
        },
        {
          "concept": "training",
          "relevance": 0.342
        },
        {
          "concept": "precision",
          "relevance": 0.327
        },
        {
          "concept": "applications",
          "relevance": 0.325
        },
        {
          "concept": "technique",
          "relevance": 0.308
        },
        {
          "concept": "method",
          "relevance": 0.303
        },
        {
          "concept": "concept",
          "relevance": 0.301
        },
        {
          "concept": "experiments",
          "relevance": 0.287
        },
        {
          "concept": "model",
          "relevance": 0.286
        },
        {
          "concept": "area under the curve",
          "relevance": 0.279
        },
        {
          "concept": "availability",
          "relevance": 0.273
        },
        {
          "concept": "process",
          "relevance": 0.271
        },
        {
          "concept": "specificity",
          "relevance": 0.266
        },
        {
          "concept": "aspects",
          "relevance": 0.265
        },
        {
          "concept": "transfer",
          "relevance": 0.243
        },
        {
          "concept": "pituitary tumors",
          "relevance": 0.226
        },
        {
          "concept": "brain tumors",
          "relevance": 0.225
        },
        {
          "concept": "discussion",
          "relevance": 0.223
        },
        {
          "concept": "area",
          "relevance": 0.219
        },
        {
          "concept": "problem",
          "relevance": 0.218
        },
        {
          "concept": "brain",
          "relevance": 0.211
        },
        {
          "concept": "measurements",
          "relevance": 0.21
        },
        {
          "concept": "tumor",
          "relevance": 0.202
        },
        {
          "concept": "study",
          "relevance": 0.186
        },
        {
          "concept": "curves",
          "relevance": 0.186
        },
        {
          "concept": "observations",
          "relevance": 0.178
        },
        {
          "concept": "meningiomas",
          "relevance": 0.176
        },
        {
          "concept": "glioma",
          "relevance": 0.172
        },
        {
          "concept": "MRI",
          "relevance": 0.169
        },
        {
          "concept": "diagnosis",
          "relevance": 0.168
        },
        {
          "concept": "samples",
          "relevance": 0.164
        }
      ]
    },
    {
      "paperId": "pub.1120963973",
      "doi": "10.1007/s00034-019-01246-3",
      "title": "A Deep Learning-Based Framework for Automatic Brain Tumors Classification Using Transfer Learning",
      "year": 2019,
      "citationCount": 540,
      "fieldCitationRatio": 114.83,
      "abstract": "Brain tumors are the most destructive disease, leading to a very short life expectancy in their highest grade. The misdiagnosis of brain tumors will result in wrong medical intercession and reduce chance of survival of patients. The accurate diagnosis of brain tumor is a key point to make a proper treatment planning to cure and improve the existence of patients with brain tumors disease. The computer-aided tumor detection systems and convolutional neural networks provided success stories and have made important strides in the field of machine learning. The deep convolutional layers extract important and robust features automatically from the input space as compared to traditional predecessor neural network layers. In the proposed framework, we conduct three studies using three architectures of convolutional neural networks (AlexNet, GoogLeNet, and VGGNet) to classify brain tumors such as meningioma, glioma, and pituitary. Each study then explores the transfer learning techniques, i.e., fine-tune and freeze using MRI slices of brain tumor dataset—Figshare. The data augmentation techniques are applied to the MRI slices for generalization of results, increasing the dataset samples and reducing the chance of over-fitting. In the proposed studies, the fine-tune VGG16 architecture attained highest accuracy up to 98.69 in terms of classification and detection.",
      "reference_ids": [
        "pub.1009767488",
        "pub.1094218945",
        "pub.1107791594",
        "pub.1111934823",
        "pub.1107999419",
        "pub.1094291017",
        "pub.1025911090",
        "pub.1014894112",
        "pub.1111582992",
        "pub.1084098375",
        "pub.1107055260",
        "pub.1092665654",
        "pub.1112829603",
        "pub.1036925836",
        "pub.1013793089",
        "pub.1106861476",
        "pub.1107416730",
        "pub.1104265935",
        "pub.1011682275",
        "pub.1028539157",
        "pub.1095689025"
      ],
      "concepts_scores": [
        {
          "concept": "convolutional neural network",
          "relevance": 0.711
        },
        {
          "concept": "neural network",
          "relevance": 0.648
        },
        {
          "concept": "architectures of convolutional neural networks",
          "relevance": 0.641
        },
        {
          "concept": "chance of over-fitting",
          "relevance": 0.636
        },
        {
          "concept": "automatic brain tumor classification",
          "relevance": 0.63
        },
        {
          "concept": "field of machine learning",
          "relevance": 0.63
        },
        {
          "concept": "deep learning-based framework",
          "relevance": 0.621
        },
        {
          "concept": "neural network layers",
          "relevance": 0.619
        },
        {
          "concept": "deep convolutional layers",
          "relevance": 0.618
        },
        {
          "concept": "transfer learning technique",
          "relevance": 0.613
        },
        {
          "concept": "data augmentation techniques",
          "relevance": 0.613
        },
        {
          "concept": "brain tumor disease",
          "relevance": 0.597
        },
        {
          "concept": "brain tumor classification",
          "relevance": 0.593
        },
        {
          "concept": "accurate diagnosis of brain tumors",
          "relevance": 0.593
        },
        {
          "concept": "tumor detection system",
          "relevance": 0.593
        },
        {
          "concept": "convolutional layers",
          "relevance": 0.574
        },
        {
          "concept": "network layer",
          "relevance": 0.572
        },
        {
          "concept": "VGG16 architecture",
          "relevance": 0.569
        },
        {
          "concept": "transfer learning",
          "relevance": 0.568
        },
        {
          "concept": "input space",
          "relevance": 0.568
        },
        {
          "concept": "dataset samples",
          "relevance": 0.561
        },
        {
          "concept": "over-fitting",
          "relevance": 0.561
        },
        {
          "concept": "robust features",
          "relevance": 0.559
        },
        {
          "concept": "learning techniques",
          "relevance": 0.558
        },
        {
          "concept": "machine learning",
          "relevance": 0.554
        },
        {
          "concept": "MRI slices",
          "relevance": 0.551
        },
        {
          "concept": "augmentation techniques",
          "relevance": 0.544
        },
        {
          "concept": "detection system",
          "relevance": 0.541
        },
        {
          "concept": "chances of survival of patients",
          "relevance": 0.527
        },
        {
          "concept": "diagnosis of brain tumors",
          "relevance": 0.501
        },
        {
          "concept": "architecture",
          "relevance": 0.486
        },
        {
          "concept": "network",
          "relevance": 0.483
        },
        {
          "concept": "classification",
          "relevance": 0.465
        },
        {
          "concept": "learning",
          "relevance": 0.462
        },
        {
          "concept": "VGG16",
          "relevance": 0.445
        },
        {
          "concept": "tumor classification",
          "relevance": 0.439
        },
        {
          "concept": "framework",
          "relevance": 0.432
        },
        {
          "concept": "Deep",
          "relevance": 0.43
        },
        {
          "concept": "automatically",
          "relevance": 0.424
        },
        {
          "concept": "dataset",
          "relevance": 0.423
        },
        {
          "concept": "brain tumors",
          "relevance": 0.408
        },
        {
          "concept": "accuracy",
          "relevance": 0.386
        },
        {
          "concept": "technique",
          "relevance": 0.385
        },
        {
          "concept": "input",
          "relevance": 0.377
        },
        {
          "concept": "survival of patients",
          "relevance": 0.372
        },
        {
          "concept": "success stories",
          "relevance": 0.37
        },
        {
          "concept": "generalization of results",
          "relevance": 0.368
        },
        {
          "concept": "generalization",
          "relevance": 0.363
        },
        {
          "concept": "features",
          "relevance": 0.358
        },
        {
          "concept": "detection",
          "relevance": 0.358
        },
        {
          "concept": "short life expectancy",
          "relevance": 0.338
        },
        {
          "concept": "system",
          "relevance": 0.338
        },
        {
          "concept": "tumor",
          "relevance": 0.33
        },
        {
          "concept": "accurate diagnosis",
          "relevance": 0.33
        },
        {
          "concept": "space",
          "relevance": 0.328
        },
        {
          "concept": "tumor disease",
          "relevance": 0.324
        },
        {
          "concept": "patients",
          "relevance": 0.305
        },
        {
          "concept": "i.",
          "relevance": 0.305
        },
        {
          "concept": "MRI",
          "relevance": 0.299
        },
        {
          "concept": "stride",
          "relevance": 0.297
        },
        {
          "concept": "slices",
          "relevance": 0.297
        },
        {
          "concept": "reduced chance",
          "relevance": 0.296
        },
        {
          "concept": "layer",
          "relevance": 0.296
        },
        {
          "concept": "data",
          "relevance": 0.295
        },
        {
          "concept": "brain",
          "relevance": 0.294
        },
        {
          "concept": "disease",
          "relevance": 0.289
        },
        {
          "concept": "life expectancy",
          "relevance": 0.289
        },
        {
          "concept": "success",
          "relevance": 0.285
        },
        {
          "concept": "results",
          "relevance": 0.285
        },
        {
          "concept": "field",
          "relevance": 0.269
        },
        {
          "concept": "meningiomas",
          "relevance": 0.268
        },
        {
          "concept": "intercession",
          "relevance": 0.265
        },
        {
          "concept": "transfer",
          "relevance": 0.262
        },
        {
          "concept": "misdiagnosis",
          "relevance": 0.262
        },
        {
          "concept": "glioma",
          "relevance": 0.262
        },
        {
          "concept": "pituitary",
          "relevance": 0.261
        },
        {
          "concept": "study",
          "relevance": 0.246
        },
        {
          "concept": "treatment",
          "relevance": 0.245
        },
        {
          "concept": "chance",
          "relevance": 0.244
        },
        {
          "concept": "expectations",
          "relevance": 0.24
        },
        {
          "concept": "grade",
          "relevance": 0.239
        },
        {
          "concept": "destructive disease",
          "relevance": 0.214
        },
        {
          "concept": "story",
          "relevance": 0.186
        },
        {
          "concept": "samples",
          "relevance": 0.178
        }
      ]
    },
    {
      "paperId": "pub.1157160654",
      "doi": "10.1109/tii.2023.3266366",
      "title": "LDCNet: Limb Direction Cues-Aware Network for Flexible HPE in Industrial Behavioral Biometrics Systems",
      "year": 2023,
      "citationCount": 95,
      "fieldCitationRatio": NaN,
      "abstract": "Two-dimensional human pose estimation (HPE) has been widely used in the many fields, such as behavioral understanding, identity authentication, and industrial automatic manufacturing. Most of the previous studies have encountered many constraints, such as restricted scenarios and strict inputs. To solve this problem, we present a simple yet effective HPE network called limb direction cues (LDCs) aware network (LDCNet) with LDCs and differentiated Cauchy labels, which can efficiently suppress uncertainties and prevent deep networks from over-fitting uncertain keypoint positions. In particular, LDCNet suppresses the uncertainties from two aspects. First, a differentiated Cauchy coordinate encoding method is designed to reveal the limb direction information among adjacent keypoints. Second, Jeffreys divergence is introduced as loss function to measure the prediction heatmap and ground-truth one. Positions of keypoints are perceived at the limb direction based deep network in an end-to-end manner. An extensive study on two benchmark datasets (i.e., MS COCO and MPII) illustrates the superiority of the proposed LDCNet model over state-of-the-art approaches.",
      "reference_ids": [
        "pub.1058328114",
        "pub.1061794133",
        "pub.1061718811",
        "pub.1095852425",
        "pub.1092348681",
        "pub.1124013538",
        "pub.1061693628",
        "pub.1045321436",
        "pub.1093518416",
        "pub.1112615919",
        "pub.1106102677",
        "pub.1061632733",
        "pub.1129913039",
        "pub.1092172538",
        "pub.1061644754",
        "pub.1093572203",
        "pub.1093359587",
        "pub.1129913069",
        "pub.1049647714",
        "pub.1061744403",
        "pub.1123988590",
        "pub.1123988062",
        "pub.1107454549",
        "pub.1134086137",
        "pub.1129913235",
        "pub.1118069516",
        "pub.1061744416",
        "pub.1123987841",
        "pub.1101697837",
        "pub.1129913207",
        "pub.1025034598",
        "pub.1094869203",
        "pub.1094368585",
        "pub.1061744217"
      ],
      "concepts_scores": [
        {
          "concept": "human pose estimation",
          "relevance": 0.794
        },
        {
          "concept": "deep networks",
          "relevance": 0.697
        },
        {
          "concept": "end-to-end manner",
          "relevance": 0.668
        },
        {
          "concept": "state-of-the-art approaches",
          "relevance": 0.668
        },
        {
          "concept": "prevents deep networks",
          "relevance": 0.654
        },
        {
          "concept": "behavioral biometric systems",
          "relevance": 0.649
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.648
        },
        {
          "concept": "end-to-end",
          "relevance": 0.636
        },
        {
          "concept": "positions of keypoints",
          "relevance": 0.624
        },
        {
          "concept": "identity authentication",
          "relevance": 0.603
        },
        {
          "concept": "biometric systems",
          "relevance": 0.602
        },
        {
          "concept": "pose estimation",
          "relevance": 0.602
        },
        {
          "concept": "benchmark datasets",
          "relevance": 0.601
        },
        {
          "concept": "keypoint positions",
          "relevance": 0.599
        },
        {
          "concept": "predicted heatmaps",
          "relevance": 0.599
        },
        {
          "concept": "adjacent keypoints",
          "relevance": 0.594
        },
        {
          "concept": "encoding method",
          "relevance": 0.592
        },
        {
          "concept": "loss function",
          "relevance": 0.582
        },
        {
          "concept": "behavior understanding",
          "relevance": 0.579
        },
        {
          "concept": "ground truth",
          "relevance": 0.571
        },
        {
          "concept": "Jeffreys divergence",
          "relevance": 0.568
        },
        {
          "concept": "directional information",
          "relevance": 0.559
        },
        {
          "concept": "keypoints",
          "relevance": 0.545
        },
        {
          "concept": "network",
          "relevance": 0.544
        },
        {
          "concept": "suppress uncertainty",
          "relevance": 0.515
        },
        {
          "concept": "automatic manufacturing",
          "relevance": 0.512
        },
        {
          "concept": "restricted scenarios",
          "relevance": 0.511
        },
        {
          "concept": "authentication",
          "relevance": 0.456
        },
        {
          "concept": "dataset",
          "relevance": 0.443
        },
        {
          "concept": "heatmap",
          "relevance": 0.431
        },
        {
          "concept": "superiority",
          "relevance": 0.409
        },
        {
          "concept": "scenarios",
          "relevance": 0.406
        },
        {
          "concept": "directional cues",
          "relevance": 0.402
        },
        {
          "concept": "input",
          "relevance": 0.395
        },
        {
          "concept": "information",
          "relevance": 0.392
        },
        {
          "concept": "constraints",
          "relevance": 0.389
        },
        {
          "concept": "labeling",
          "relevance": 0.375
        },
        {
          "concept": "uncertainty",
          "relevance": 0.369
        },
        {
          "concept": "system",
          "relevance": 0.355
        },
        {
          "concept": "method",
          "relevance": 0.343
        },
        {
          "concept": "prediction",
          "relevance": 0.334
        },
        {
          "concept": "estimation",
          "relevance": 0.333
        },
        {
          "concept": "manufacturing",
          "relevance": 0.325
        },
        {
          "concept": "model",
          "relevance": 0.324
        },
        {
          "concept": "position",
          "relevance": 0.322
        },
        {
          "concept": "cues",
          "relevance": 0.32
        },
        {
          "concept": "industry",
          "relevance": 0.307
        },
        {
          "concept": "manner",
          "relevance": 0.301
        },
        {
          "concept": "direction",
          "relevance": 0.289
        },
        {
          "concept": "function",
          "relevance": 0.284
        },
        {
          "concept": "field",
          "relevance": 0.28
        },
        {
          "concept": "limb direction",
          "relevance": 0.28
        },
        {
          "concept": "understanding",
          "relevance": 0.274
        },
        {
          "concept": "problem",
          "relevance": 0.26
        },
        {
          "concept": "loss",
          "relevance": 0.239
        },
        {
          "concept": "identity",
          "relevance": 0.235
        },
        {
          "concept": "divergence",
          "relevance": 0.233
        },
        {
          "concept": "Jeffrey",
          "relevance": 0.232
        },
        {
          "concept": "approach",
          "relevance": 0.231
        },
        {
          "concept": "study",
          "relevance": 0.205
        },
        {
          "concept": "limb",
          "relevance": 0.179
        }
      ]
    },
    {
      "paperId": "pub.1093359587",
      "doi": "10.1109/cvpr.2016.90",
      "title": "Deep Residual Learning for Image Recognition",
      "year": 2016,
      "citationCount": 186455,
      "fieldCitationRatio": 36441.39,
      "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions11http://image-net.org/challenges/LSVRC/2015/ and http://mscoco.org/dataset/#detections-challenge2015., where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. http://image-net.org/challenges/LSVRC/2015/ and http://mscoco.org/dataset/#detections-challenge2015.",
      "reference_ids": [
        "pub.1061744812",
        "pub.1008345178",
        "pub.1033229571",
        "pub.1037602011",
        "pub.1085642448",
        "pub.1061743975",
        "pub.1032233097",
        "pub.1093456265",
        "pub.1095559903",
        "pub.1014796149",
        "pub.1033986161",
        "pub.1061218416",
        "pub.1099341617",
        "pub.1098556598",
        "pub.1061745117",
        "pub.1061744117",
        "pub.1038140272",
        "pub.1098665985",
        "pub.1061156500",
        "pub.1045321436",
        "pub.1094727707",
        "pub.1030406568",
        "pub.1165598000",
        "pub.1095573598",
        "pub.1050111762",
        "pub.1093626237",
        "pub.1036869950",
        "pub.1094291017",
        "pub.1093828312"
      ],
      "concepts_scores": [
        {
          "concept": "residual nets",
          "relevance": 0.756
        },
        {
          "concept": "COCO object detection dataset",
          "relevance": 0.693
        },
        {
          "concept": "deep residual net",
          "relevance": 0.675
        },
        {
          "concept": "object detection datasets",
          "relevance": 0.673
        },
        {
          "concept": "residual learning framework",
          "relevance": 0.671
        },
        {
          "concept": "training of network",
          "relevance": 0.66
        },
        {
          "concept": "visual recognition tasks",
          "relevance": 0.659
        },
        {
          "concept": "learning residual functions",
          "relevance": 0.657
        },
        {
          "concept": "CIFAR-10",
          "relevance": 0.624
        },
        {
          "concept": "depth of representations",
          "relevance": 0.624
        },
        {
          "concept": "detection dataset",
          "relevance": 0.623
        },
        {
          "concept": "COCO detection",
          "relevance": 0.623
        },
        {
          "concept": "ImageNet dataset",
          "relevance": 0.622
        },
        {
          "concept": "deep representations",
          "relevance": 0.621
        },
        {
          "concept": "VGG-Net",
          "relevance": 0.619
        },
        {
          "concept": "classification task",
          "relevance": 0.619
        },
        {
          "concept": "residual network",
          "relevance": 0.614
        },
        {
          "concept": "learning framework",
          "relevance": 0.613
        },
        {
          "concept": "neural network",
          "relevance": 0.606
        },
        {
          "concept": "layer input",
          "relevance": 0.605
        },
        {
          "concept": "ImageNet",
          "relevance": 0.595
        },
        {
          "concept": "recognition task",
          "relevance": 0.593
        },
        {
          "concept": "ILSVRC",
          "relevance": 0.568
        },
        {
          "concept": "COCO",
          "relevance": 0.557
        },
        {
          "concept": "network",
          "relevance": 0.548
        },
        {
          "concept": "task",
          "relevance": 0.533
        },
        {
          "concept": "dataset",
          "relevance": 0.529
        },
        {
          "concept": "comprehensive empirical evidence",
          "relevance": 0.526
        },
        {
          "concept": "nets",
          "relevance": 0.506
        },
        {
          "concept": "representation",
          "relevance": 0.495
        },
        {
          "concept": "VGG",
          "relevance": 0.486
        },
        {
          "concept": "detection",
          "relevance": 0.448
        },
        {
          "concept": "classification",
          "relevance": 0.435
        },
        {
          "concept": "accuracy",
          "relevance": 0.418
        },
        {
          "concept": "input",
          "relevance": 0.408
        },
        {
          "concept": "framework",
          "relevance": 0.404
        },
        {
          "concept": "error",
          "relevance": 0.401
        },
        {
          "concept": "training",
          "relevance": 0.399
        },
        {
          "concept": "ensemble",
          "relevance": 0.392
        },
        {
          "concept": "segments",
          "relevance": 0.379
        },
        {
          "concept": "submission",
          "relevance": 0.378
        },
        {
          "concept": "residual function",
          "relevance": 0.365
        },
        {
          "concept": "function",
          "relevance": 0.339
        },
        {
          "concept": "layer",
          "relevance": 0.336
        },
        {
          "concept": "localization",
          "relevance": 0.336
        },
        {
          "concept": "complex",
          "relevance": 0.335
        },
        {
          "concept": "improvement",
          "relevance": 0.322
        },
        {
          "concept": "empirical evidence",
          "relevance": 0.321
        },
        {
          "concept": "depth",
          "relevance": 0.299
        },
        {
          "concept": "foundations",
          "relevance": 0.292
        },
        {
          "concept": "increasing depth",
          "relevance": 0.282
        },
        {
          "concept": "analysis",
          "relevance": 0.253
        },
        {
          "concept": "test",
          "relevance": 0.251
        },
        {
          "concept": "evidence",
          "relevance": 0.177
        }
      ]
    },
    {
      "paperId": "pub.1085642448",
      "doi": "10.1145/3065386",
      "title": "ImageNet classification with deep convolutional neural networks",
      "year": 2017,
      "citationCount": 49626,
      "fieldCitationRatio": 10004.01,
      "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
      "reference_ids": [
        "pub.1094646180",
        "pub.1024739340",
        "pub.1020853978",
        "pub.1094291017",
        "pub.1027534025",
        "pub.1010421612",
        "pub.1094709720",
        "pub.1034831185",
        "pub.1094246303",
        "pub.1033296596",
        "pub.1093359587",
        "pub.1093416695",
        "pub.1045861574",
        "pub.1025918724",
        "pub.1016635886",
        "pub.1051709453",
        "pub.1004476131",
        "pub.1004784969",
        "pub.1094714779"
      ],
      "concepts_scores": [
        {
          "concept": "deep convolutional neural network",
          "relevance": 0.788
        },
        {
          "concept": "convolutional neural network",
          "relevance": 0.768
        },
        {
          "concept": "neural network",
          "relevance": 0.735
        },
        {
          "concept": "error rate",
          "relevance": 0.691
        },
        {
          "concept": "efficient GPU implementation",
          "relevance": 0.669
        },
        {
          "concept": "max-pooling layers",
          "relevance": 0.668
        },
        {
          "concept": "test error rate",
          "relevance": 0.65
        },
        {
          "concept": "ImageNet classification",
          "relevance": 0.624
        },
        {
          "concept": "ILSVRC-2012",
          "relevance": 0.622
        },
        {
          "concept": "convolutional layers",
          "relevance": 0.619
        },
        {
          "concept": "GPU implementation",
          "relevance": 0.617
        },
        {
          "concept": "top-5",
          "relevance": 0.606
        },
        {
          "concept": "convolution operation",
          "relevance": 0.602
        },
        {
          "concept": "regularization method",
          "relevance": 0.569
        },
        {
          "concept": "high-resolution images",
          "relevance": 0.568
        },
        {
          "concept": "ImageNet",
          "relevance": 0.565
        },
        {
          "concept": "network",
          "relevance": 0.547
        },
        {
          "concept": "test data",
          "relevance": 0.507
        },
        {
          "concept": "softmax",
          "relevance": 0.487
        },
        {
          "concept": "overfitting",
          "relevance": 0.477
        },
        {
          "concept": "convolution",
          "relevance": 0.468
        },
        {
          "concept": "classification",
          "relevance": 0.434
        },
        {
          "concept": "implementation",
          "relevance": 0.414
        },
        {
          "concept": "regularization",
          "relevance": 0.41
        },
        {
          "concept": "error",
          "relevance": 0.4
        },
        {
          "concept": "images",
          "relevance": 0.398
        },
        {
          "concept": "training",
          "relevance": 0.398
        },
        {
          "concept": "operation",
          "relevance": 0.367
        },
        {
          "concept": "method",
          "relevance": 0.353
        },
        {
          "concept": "class",
          "relevance": 0.341
        },
        {
          "concept": "layer",
          "relevance": 0.341
        },
        {
          "concept": "neurons",
          "relevance": 0.336
        },
        {
          "concept": "model",
          "relevance": 0.334
        },
        {
          "concept": "dropout",
          "relevance": 0.328
        },
        {
          "concept": "data",
          "relevance": 0.319
        },
        {
          "concept": "variants",
          "relevance": 0.296
        },
        {
          "concept": "parameters",
          "relevance": 0.291
        },
        {
          "concept": "entry",
          "relevance": 0.279
        },
        {
          "concept": "rate",
          "relevance": 0.27
        },
        {
          "concept": "competition",
          "relevance": 0.27
        },
        {
          "concept": "test",
          "relevance": 0.25
        },
        {
          "concept": "contest",
          "relevance": 0.248
        }
      ]
    },
    {
      "paperId": "pub.1061745117",
      "doi": "10.1109/tpami.2016.2577031",
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "year": 2016,
      "citationCount": 49518,
      "fieldCitationRatio": 11101.39,
      "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
      "reference_ids": [
        "pub.1061744374",
        "pub.1093359587",
        "pub.1061744921",
        "pub.1099427274",
        "pub.1095738839",
        "pub.1027572846",
        "pub.1011079920",
        "pub.1014796149",
        "pub.1030406568",
        "pub.1099426696",
        "pub.1061743745",
        "pub.1033900312",
        "pub.1093626237",
        "pub.1094390291",
        "pub.1094665428",
        "pub.1094727707",
        "pub.1032233097",
        "pub.1008345178",
        "pub.1095573598",
        "pub.1085642448",
        "pub.1061745147",
        "pub.1094291017",
        "pub.1095604624",
        "pub.1093292479",
        "pub.1009767488",
        "pub.1052031051",
        "pub.1094492451",
        "pub.1061744113",
        "pub.1045321436"
      ],
      "concepts_scores": [
        {
          "concept": "region proposal network",
          "relevance": 0.854
        },
        {
          "concept": "Faster R-CNN",
          "relevance": 0.825
        },
        {
          "concept": "R-CNN",
          "relevance": 0.775
        },
        {
          "concept": "detection network",
          "relevance": 0.753
        },
        {
          "concept": "region proposals",
          "relevance": 0.732
        },
        {
          "concept": "proposed network",
          "relevance": 0.731
        },
        {
          "concept": "state-of-the-art object detection networks",
          "relevance": 0.716
        },
        {
          "concept": "state-of-the-art object detection accuracy",
          "relevance": 0.706
        },
        {
          "concept": "full-image convolutional features",
          "relevance": 0.705
        },
        {
          "concept": "high-quality region proposals",
          "relevance": 0.705
        },
        {
          "concept": "deep VGG-16 model",
          "relevance": 0.705
        },
        {
          "concept": "trained end-to-end",
          "relevance": 0.703
        },
        {
          "concept": "real-time object detection",
          "relevance": 0.701
        },
        {
          "concept": "MS COCO dataset",
          "relevance": 0.686
        },
        {
          "concept": "object detection accuracy",
          "relevance": 0.684
        },
        {
          "concept": "object detection network",
          "relevance": 0.683
        },
        {
          "concept": "Faster R-CNN",
          "relevance": 0.678
        },
        {
          "concept": "VGG-16 model",
          "relevance": 0.676
        },
        {
          "concept": "end-to-end",
          "relevance": 0.669
        },
        {
          "concept": "PASCAL VOC",
          "relevance": 0.636
        },
        {
          "concept": "COCO dataset",
          "relevance": 0.635
        },
        {
          "concept": "convolutional features",
          "relevance": 0.634
        },
        {
          "concept": "object detection",
          "relevance": 0.631
        },
        {
          "concept": "convolutional network",
          "relevance": 0.629
        },
        {
          "concept": "detection accuracy",
          "relevance": 0.62
        },
        {
          "concept": "proposal computation",
          "relevance": 0.618
        },
        {
          "concept": "neural network",
          "relevance": 0.617
        },
        {
          "concept": "object bounds",
          "relevance": 0.616
        },
        {
          "concept": "unified network",
          "relevance": 0.612
        },
        {
          "concept": "object location",
          "relevance": 0.602
        },
        {
          "concept": "detection system",
          "relevance": 0.596
        },
        {
          "concept": "running time",
          "relevance": 0.586
        },
        {
          "concept": "network",
          "relevance": 0.574
        },
        {
          "concept": "objective scores",
          "relevance": 0.538
        },
        {
          "concept": "ILSVRC",
          "relevance": 0.499
        },
        {
          "concept": "proposal",
          "relevance": 0.497
        },
        {
          "concept": "SPPnet",
          "relevance": 0.494
        },
        {
          "concept": "detection",
          "relevance": 0.491
        },
        {
          "concept": "GPU",
          "relevance": 0.491
        },
        {
          "concept": "FPS",
          "relevance": 0.484
        },
        {
          "concept": "Pascal",
          "relevance": 0.477
        },
        {
          "concept": "dataset",
          "relevance": 0.465
        },
        {
          "concept": "computer",
          "relevance": 0.458
        },
        {
          "concept": "COCO",
          "relevance": 0.458
        },
        {
          "concept": "objective",
          "relevance": 0.457
        },
        {
          "concept": "code",
          "relevance": 0.453
        },
        {
          "concept": "bottleneck",
          "relevance": 0.432
        },
        {
          "concept": "accuracy",
          "relevance": 0.426
        },
        {
          "concept": "tracking",
          "relevance": 0.417
        },
        {
          "concept": "bounds",
          "relevance": 0.416
        },
        {
          "concept": "images",
          "relevance": 0.406
        },
        {
          "concept": "features",
          "relevance": 0.394
        },
        {
          "concept": "system",
          "relevance": 0.373
        },
        {
          "concept": "terminology",
          "relevance": 0.343
        },
        {
          "concept": "model",
          "relevance": 0.34
        },
        {
          "concept": "advances",
          "relevance": 0.335
        },
        {
          "concept": "location",
          "relevance": 0.33
        },
        {
          "concept": "VOC",
          "relevance": 0.321
        },
        {
          "concept": "time",
          "relevance": 0.311
        },
        {
          "concept": "region",
          "relevance": 0.301
        },
        {
          "concept": "foundations",
          "relevance": 0.297
        },
        {
          "concept": "position",
          "relevance": 0.292
        },
        {
          "concept": "components",
          "relevance": 0.289
        },
        {
          "concept": "entry",
          "relevance": 0.285
        },
        {
          "concept": "competition",
          "relevance": 0.276
        },
        {
          "concept": "mechanism",
          "relevance": 0.239
        },
        {
          "concept": "rate",
          "relevance": 0.238
        },
        {
          "concept": "scores",
          "relevance": 0.22
        }
      ]
    },
    {
      "paperId": "pub.1123988062",
      "doi": "10.1109/cvpr.2019.00584",
      "title": "Deep High-Resolution Representation Learning for Human Pose Estimation",
      "year": 2019,
      "citationCount": 4753,
      "fieldCitationRatio": 1164.21,
      "abstract": "In this paper, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted key-point heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. In addition, we show the superiority of our network in pose tracking on the PoseTrack dataset. The code and models have been publicly available at https://github.com/leoxiaobin/deep-high-resolution-net.pytorch.",
      "reference_ids": [
        "pub.1095840375",
        "pub.1107463206",
        "pub.1110720676",
        "pub.1105386562",
        "pub.1095843462",
        "pub.1095852425",
        "pub.1095836287",
        "pub.1107463196",
        "pub.1107502672",
        "pub.1093518416",
        "pub.1107502716",
        "pub.1095071982",
        "pub.1030577527",
        "pub.1095837104",
        "pub.1112655550",
        "pub.1092172538",
        "pub.1100060109",
        "pub.1095838770",
        "pub.1027821665",
        "pub.1093625430",
        "pub.1100060314",
        "pub.1095839207",
        "pub.1113871703",
        "pub.1094213207",
        "pub.1094869203",
        "pub.1011746576",
        "pub.1100060307",
        "pub.1107454554",
        "pub.1107463267",
        "pub.1095123369",
        "pub.1095851287",
        "pub.1049647714",
        "pub.1107454549",
        "pub.1093572203",
        "pub.1093276903",
        "pub.1100060469",
        "pub.1094291017",
        "pub.1107454737",
        "pub.1110721171",
        "pub.1094045097",
        "pub.1095271839",
        "pub.1107502595",
        "pub.1045321436",
        "pub.1094296513",
        "pub.1110720358",
        "pub.1100060101",
        "pub.1100060611",
        "pub.1095245634",
        "pub.1100060233",
        "pub.1107463279",
        "pub.1094368585",
        "pub.1110720178",
        "pub.1110721056",
        "pub.1044219864",
        "pub.1093715844",
        "pub.1110720371",
        "pub.1049772801",
        "pub.1085304410",
        "pub.1047850584",
        "pub.1093314175",
        "pub.1095601240",
        "pub.1110720876"
      ],
      "concepts_scores": [
        {
          "concept": "high-resolution representation",
          "relevance": 0.727
        },
        {
          "concept": "pose estimation",
          "relevance": 0.663
        },
        {
          "concept": "high-to-low resolution networks",
          "relevance": 0.644
        },
        {
          "concept": "high-resolution representation learning",
          "relevance": 0.638
        },
        {
          "concept": "COCO keypoint detection dataset",
          "relevance": 0.638
        },
        {
          "concept": "MPII Human Pose dataset",
          "relevance": 0.638
        },
        {
          "concept": "human pose estimation problem",
          "relevance": 0.63
        },
        {
          "concept": "human pose datasets",
          "relevance": 0.621
        },
        {
          "concept": "human pose estimation",
          "relevance": 0.62
        },
        {
          "concept": "multi-scale fusion",
          "relevance": 0.617
        },
        {
          "concept": "pose estimation problem",
          "relevance": 0.614
        },
        {
          "concept": "low-resolution representations",
          "relevance": 0.599
        },
        {
          "concept": "PoseTrack dataset",
          "relevance": 0.576
        },
        {
          "concept": "representation learning",
          "relevance": 0.574
        },
        {
          "concept": "detection dataset",
          "relevance": 0.573
        },
        {
          "concept": "resolution network",
          "relevance": 0.561
        },
        {
          "concept": "resolution representation",
          "relevance": 0.552
        },
        {
          "concept": "dataset",
          "relevance": 0.521
        },
        {
          "concept": "estimation problem",
          "relevance": 0.518
        },
        {
          "concept": "network",
          "relevance": 0.514
        },
        {
          "concept": "representation",
          "relevance": 0.492
        },
        {
          "concept": "subnetworks",
          "relevance": 0.485
        },
        {
          "concept": "PoseTrack",
          "relevance": 0.453
        },
        {
          "concept": "MPII",
          "relevance": 0.443
        },
        {
          "concept": "Deep",
          "relevance": 0.428
        },
        {
          "concept": "whole process",
          "relevance": 0.425
        },
        {
          "concept": "COCO",
          "relevance": 0.414
        },
        {
          "concept": "code",
          "relevance": 0.41
        },
        {
          "concept": "heatmap",
          "relevance": 0.409
        },
        {
          "concept": "learning",
          "relevance": 0.397
        },
        {
          "concept": "superiority",
          "relevance": 0.389
        },
        {
          "concept": "information",
          "relevance": 0.373
        },
        {
          "concept": "estimation",
          "relevance": 0.366
        },
        {
          "concept": "fusion",
          "relevance": 0.345
        },
        {
          "concept": "method",
          "relevance": 0.326
        },
        {
          "concept": "model",
          "relevance": 0.308
        },
        {
          "concept": "process",
          "relevance": 0.292
        },
        {
          "concept": "results",
          "relevance": 0.283
        },
        {
          "concept": "humans",
          "relevance": 0.251
        },
        {
          "concept": "stage",
          "relevance": 0.218
        },
        {
          "concept": "effect",
          "relevance": 0.185
        },
        {
          "concept": "problem",
          "relevance": 0.18
        }
      ]
    },
    {
      "paperId": "pub.1107454549",
      "doi": "10.1007/978-3-030-01231-1_29",
      "title": "Simple Baselines for Human Pose Estimation and Tracking",
      "year": 2018,
      "citationCount": 1761,
      "fieldCitationRatio": 420.77,
      "abstract": "There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github.com/leoxiaobin/pose.pytorch.",
      "reference_ids": [
        "pub.1095843462",
        "pub.1100060307",
        "pub.1031475974",
        "pub.1110720676",
        "pub.1061745117",
        "pub.1100060526",
        "pub.1049647714",
        "pub.1100060101",
        "pub.1094296513",
        "pub.1093518416",
        "pub.1095840375",
        "pub.1095839208",
        "pub.1094869203",
        "pub.1095852425",
        "pub.1095837104",
        "pub.1110720876",
        "pub.1100060109",
        "pub.1045321436",
        "pub.1110720178",
        "pub.1093359587",
        "pub.1148955875",
        "pub.1100060679",
        "pub.1009767488"
      ],
      "concepts_scores": [
        {
          "concept": "state-of-the-art results",
          "relevance": 0.614
        },
        {
          "concept": "effective baseline method",
          "relevance": 0.597
        },
        {
          "concept": "human pose estimation",
          "relevance": 0.596
        },
        {
          "concept": "evaluating new ideas",
          "relevance": 0.559
        },
        {
          "concept": "pose estimation",
          "relevance": 0.551
        },
        {
          "concept": "baseline methods",
          "relevance": 0.547
        },
        {
          "concept": "overall algorithm",
          "relevance": 0.522
        },
        {
          "concept": "algorithm analysis",
          "relevance": 0.511
        },
        {
          "concept": "system complexity",
          "relevance": 0.507
        },
        {
          "concept": "algorithm",
          "relevance": 0.471
        },
        {
          "concept": "benchmarks",
          "relevance": 0.395
        },
        {
          "concept": "code",
          "relevance": 0.394
        },
        {
          "concept": "new ideas",
          "relevance": 0.393
        },
        {
          "concept": "increasing interest",
          "relevance": 0.368
        },
        {
          "concept": "tracking",
          "relevance": 0.363
        },
        {
          "concept": "estimation",
          "relevance": 0.352
        },
        {
          "concept": "system",
          "relevance": 0.324
        },
        {
          "concept": "method",
          "relevance": 0.313
        },
        {
          "concept": "complex",
          "relevance": 0.297
        },
        {
          "concept": "ideas",
          "relevance": 0.284
        },
        {
          "concept": "results",
          "relevance": 0.273
        },
        {
          "concept": "interest",
          "relevance": 0.26
        },
        {
          "concept": "field",
          "relevance": 0.256
        },
        {
          "concept": "comparison",
          "relevance": 0.244
        },
        {
          "concept": "humans",
          "relevance": 0.241
        },
        {
          "concept": "baseline",
          "relevance": 0.231
        },
        {
          "concept": "analysis",
          "relevance": 0.224
        },
        {
          "concept": "progression",
          "relevance": 0.213
        },
        {
          "concept": "years",
          "relevance": 0.205
        }
      ]
    },
    {
      "paperId": "pub.1110720876",
      "doi": "10.1109/cvpr.2018.00742",
      "title": "Cascaded Pyramid Network for Multi-Person Pose Estimation",
      "year": 2018,
      "citationCount": 1607,
      "fieldCitationRatio": 339.41,
      "abstract": "The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these “hard” keypoints. More specifically, our algorithm includes two stages: GlobalNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the “simple” key-points like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the “hard” keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19% relative improvement compared with 60.5 from the COCO 2016 key-point challenge. Code11https://github.com/allenchen9512/tf-CPN and the detection results for person used will be publicly available for further research. https://github.com/allenchen9512/tf-CPN",
      "reference_ids": [
        "pub.1049772801",
        "pub.1094001054",
        "pub.1100060233",
        "pub.1061179979",
        "pub.1011746576",
        "pub.1095852454",
        "pub.1093700510",
        "pub.1093359587",
        "pub.1049647714",
        "pub.1093572203",
        "pub.1100060109",
        "pub.1095573598",
        "pub.1061455715",
        "pub.1094106129",
        "pub.1095837104",
        "pub.1095198769",
        "pub.1095245634",
        "pub.1095549934",
        "pub.1094296513",
        "pub.1009767488",
        "pub.1095093918",
        "pub.1061745117",
        "pub.1095601240",
        "pub.1094727707",
        "pub.1044219864",
        "pub.1093713290",
        "pub.1095850372",
        "pub.1094224567",
        "pub.1100060314"
      ],
      "concepts_scores": [
        {
          "concept": "Cascaded Pyramid Network",
          "relevance": 0.832
        },
        {
          "concept": "multi-person pose estimation",
          "relevance": 0.808
        },
        {
          "concept": "human bounding boxes",
          "relevance": 0.784
        },
        {
          "concept": "invisible keypoints",
          "relevance": 0.73
        },
        {
          "concept": "pyramid network",
          "relevance": 0.725
        },
        {
          "concept": "pose estimation",
          "relevance": 0.725
        },
        {
          "concept": "bounding boxes",
          "relevance": 0.723
        },
        {
          "concept": "multi-person pose estimation problem",
          "relevance": 0.704
        },
        {
          "concept": "development of convolutional neural networks",
          "relevance": 0.701
        },
        {
          "concept": "COCO test-dev dataset",
          "relevance": 0.687
        },
        {
          "concept": "COCO keypoint benchmark",
          "relevance": 0.68
        },
        {
          "concept": "pose estimation problem",
          "relevance": 0.672
        },
        {
          "concept": "convolutional neural network",
          "relevance": 0.669
        },
        {
          "concept": "top-down pipeline",
          "relevance": 0.668
        },
        {
          "concept": "occluded keypoints",
          "relevance": 0.63
        },
        {
          "concept": "complex background",
          "relevance": 0.618
        },
        {
          "concept": "average precision",
          "relevance": 0.616
        },
        {
          "concept": "multi-person",
          "relevance": 0.614
        },
        {
          "concept": "neural network",
          "relevance": 0.61
        },
        {
          "concept": "keypoints",
          "relevance": 0.609
        },
        {
          "concept": "proposed algorithm",
          "relevance": 0.609
        },
        {
          "concept": "mining losses",
          "relevance": 0.579
        },
        {
          "concept": "estimation problem",
          "relevance": 0.567
        },
        {
          "concept": "network",
          "relevance": 0.566
        },
        {
          "concept": "RefineNet",
          "relevance": 0.566
        },
        {
          "concept": "COCO",
          "relevance": 0.561
        },
        {
          "concept": "key points",
          "relevance": 0.558
        },
        {
          "concept": "network structure",
          "relevance": 0.54
        },
        {
          "concept": "GlobalNet",
          "relevance": 0.538
        },
        {
          "concept": "algorithm",
          "relevance": 0.536
        },
        {
          "concept": "dataset",
          "relevance": 0.533
        },
        {
          "concept": "benchmarks",
          "relevance": 0.45
        },
        {
          "concept": "representation",
          "relevance": 0.431
        },
        {
          "concept": "box",
          "relevance": 0.427
        },
        {
          "concept": "pipeline",
          "relevance": 0.409
        },
        {
          "concept": "estimation",
          "relevance": 0.401
        },
        {
          "concept": "detection",
          "relevance": 0.39
        },
        {
          "concept": "precision",
          "relevance": 0.384
        },
        {
          "concept": "detector",
          "relevance": 0.358
        },
        {
          "concept": "localization",
          "relevance": 0.339
        },
        {
          "concept": "hand",
          "relevance": 0.333
        },
        {
          "concept": "research",
          "relevance": 0.329
        },
        {
          "concept": "improvement",
          "relevance": 0.324
        },
        {
          "concept": "results",
          "relevance": 0.31
        },
        {
          "concept": "background",
          "relevance": 0.298
        },
        {
          "concept": "persons",
          "relevance": 0.293
        },
        {
          "concept": "development",
          "relevance": 0.277
        },
        {
          "concept": "structure",
          "relevance": 0.258
        },
        {
          "concept": "cases",
          "relevance": 0.256
        },
        {
          "concept": "eyes",
          "relevance": 0.25
        },
        {
          "concept": "stage",
          "relevance": 0.238
        },
        {
          "concept": "loss",
          "relevance": 0.233
        },
        {
          "concept": "cascade",
          "relevance": 0.226
        },
        {
          "concept": "problem",
          "relevance": 0.219
        },
        {
          "concept": "levels",
          "relevance": 0.205
        }
      ]
    }
  ],
  "evolution_links": [
    {
      "source": "pub.1162852356",
      "target": "pub.1156685749",
      "source_title": "Convolutional Neural Networks: A Survey",
      "target_title": "A Study of CNN and Transfer Learning in Medical Imaging: Advantages, Challenges, Future Scope"
    },
    {
      "source": "pub.1156685749",
      "target": "pub.1141808767",
      "source_title": "A Study of CNN and Transfer Learning in Medical Imaging: Advantages, Challenges, Future Scope",
      "target_title": "CNN Variants for Computer Vision: History, Architecture, Application, Challenges and Future Scope"
    },
    {
      "source": "pub.1141808767",
      "target": "pub.1110720879",
      "source_title": "CNN Variants for Computer Vision: History, Architecture, Application, Challenges and Future Scope",
      "target_title": "Squeeze-and-Excitation Networks"
    },
    {
      "source": "pub.1141808767",
      "target": "pub.1148955875",
      "source_title": "CNN Variants for Computer Vision: History, Architecture, Application, Challenges and Future Scope",
      "target_title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"
    },
    {
      "source": "pub.1156685749",
      "target": "pub.1146128987",
      "source_title": "A Study of CNN and Transfer Learning in Medical Imaging: Advantages, Challenges, Future Scope",
      "target_title": "Deep Transfer Learning Approaches in Performance Analysis of Brain Tumor Classification Using MRI Images"
    },
    {
      "source": "pub.1146128987",
      "target": "pub.1117650760",
      "source_title": "Deep Transfer Learning Approaches in Performance Analysis of Brain Tumor Classification Using MRI Images",
      "target_title": "Brain tumor classification using deep CNN features via transfer learning"
    },
    {
      "source": "pub.1146128987",
      "target": "pub.1120963973",
      "source_title": "Deep Transfer Learning Approaches in Performance Analysis of Brain Tumor Classification Using MRI Images",
      "target_title": "A Deep Learning-Based Framework for Automatic Brain Tumors Classification Using Transfer Learning"
    },
    {
      "source": "pub.1162852356",
      "target": "pub.1157160654",
      "source_title": "Convolutional Neural Networks: A Survey",
      "target_title": "LDCNet: Limb Direction Cues-Aware Network for Flexible HPE in Industrial Behavioral Biometrics Systems"
    },
    {
      "source": "pub.1157160654",
      "target": "pub.1093359587",
      "source_title": "LDCNet: Limb Direction Cues-Aware Network for Flexible HPE in Industrial Behavioral Biometrics Systems",
      "target_title": "Deep Residual Learning for Image Recognition"
    },
    {
      "source": "pub.1093359587",
      "target": "pub.1085642448",
      "source_title": "Deep Residual Learning for Image Recognition",
      "target_title": "ImageNet classification with deep convolutional neural networks"
    },
    {
      "source": "pub.1093359587",
      "target": "pub.1061745117",
      "source_title": "Deep Residual Learning for Image Recognition",
      "target_title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
    },
    {
      "source": "pub.1157160654",
      "target": "pub.1123988062",
      "source_title": "LDCNet: Limb Direction Cues-Aware Network for Flexible HPE in Industrial Behavioral Biometrics Systems",
      "target_title": "Deep High-Resolution Representation Learning for Human Pose Estimation"
    },
    {
      "source": "pub.1123988062",
      "target": "pub.1107454549",
      "source_title": "Deep High-Resolution Representation Learning for Human Pose Estimation",
      "target_title": "Simple Baselines for Human Pose Estimation and Tracking"
    },
    {
      "source": "pub.1123988062",
      "target": "pub.1110720876",
      "source_title": "Deep High-Resolution Representation Learning for Human Pose Estimation",
      "target_title": "Cascaded Pyramid Network for Multi-Person Pose Estimation"
    }
  ]
}