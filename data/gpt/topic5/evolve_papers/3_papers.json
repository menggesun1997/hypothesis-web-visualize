{
  "papers": [
    {
      "paperId": "pub.1182683020",
      "doi": "10.1007/s13042-024-02443-6",
      "title": "A comprehensive survey on pretrained foundation models: a history from BERT to ChatGPT",
      "year": 2024,
      "citationCount": 243,
      "fieldCitationRatio": NaN,
      "abstract": "Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks across different data modalities. A PFM (e.g., BERT, ChatGPT, GPT-4) is trained on large-scale data, providing a solid parameter initialization for a wide range of downstream applications. In contrast to earlier methods that use convolution and recurrent modules for feature extraction, BERT learns bidirectional encoder representations from Transformers, trained on large datasets as contextual language models. Similarly, the Generative Pretrained Transformer (GPT) method employs Transformers as feature extractors and is trained on large datasets using an autoregressive paradigm. Recently, ChatGPT has demonstrated significant success in large language models, utilizing autoregressive language models with zero-shot or few-shot prompting. The remarkable success of PFMs has driven significant breakthroughs in AI, leading to numerous studies proposing various methods, datasets, and evaluation metrics, which increases the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, and other data modalities. It covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning, while also exploring advanced PFMs for different data modalities and unified PFMs that address data quality and quantity. Additionally, the review discusses key aspects such as model efficiency, security, and privacy, and provides insights into future research directions and challenges in PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and user-friendly interactive ability for artificial general intelligence.",
      "reference_ids": [
        "pub.1129756930",
        "pub.1032314453",
        "pub.1000850552",
        "pub.1139947224",
        "pub.1122290362",
        "pub.1141301975",
        "pub.1137862936",
        "pub.1133176971",
        "pub.1104466107",
        "pub.1088800664",
        "pub.1151381159",
        "pub.1093544489",
        "pub.1159717282",
        "pub.1167960714",
        "pub.1035157989",
        "pub.1061745216",
        "pub.1129756716",
        "pub.1145902197",
        "pub.1112616064",
        "pub.1162764019",
        "pub.1129913149",
        "pub.1133176794",
        "pub.1138840312",
        "pub.1132119538",
        "pub.1148390689",
        "pub.1150866374",
        "pub.1158589518",
        "pub.1128856734",
        "pub.1122290269",
        "pub.1146286863",
        "pub.1121024877",
        "pub.1109352302",
        "pub.1095839569",
        "pub.1025845834",
        "pub.1095849919",
        "pub.1129757771",
        "pub.1140364091",
        "pub.1150865601",
        "pub.1099115173",
        "pub.1100516767",
        "pub.1122290458",
        "pub.1093626237",
        "pub.1017177111",
        "pub.1139255870",
        "pub.1095573598",
        "pub.1136042376",
        "pub.1118169677",
        "pub.1129757043",
        "pub.1128856617",
        "pub.1123987931",
        "pub.1131679925",
        "pub.1140387427",
        "pub.1150208983",
        "pub.1133177249",
        "pub.1093208464",
        "pub.1096024064",
        "pub.1129798362",
        "pub.1145639457",
        "pub.1100060307",
        "pub.1123988721",
        "pub.1117659708",
        "pub.1133177089",
        "pub.1045321436",
        "pub.1018367015",
        "pub.1005797937",
        "pub.1061744975",
        "pub.1095836824",
        "pub.1129913169",
        "pub.1133174462",
        "pub.1139948362",
        "pub.1150865686",
        "pub.1163044255",
        "pub.1118640676",
        "pub.1129757316",
        "pub.1123987679",
        "pub.1118827815",
        "pub.1145901979",
        "pub.1121025093",
        "pub.1006904420",
        "pub.1099106215",
        "pub.1110720220",
        "pub.1121025103",
        "pub.1099115161",
        "pub.1110720879",
        "pub.1095706293",
        "pub.1127636484",
        "pub.1119904902",
        "pub.1095836438",
        "pub.1061787122",
        "pub.1024198410",
        "pub.1095839207",
        "pub.1099151165",
        "pub.1014796149",
        "pub.1094706336",
        "pub.1129078103",
        "pub.1100060688",
        "pub.1107454614",
        "pub.1145901944",
        "pub.1163453788",
        "pub.1095823695",
        "pub.1050223513",
        "pub.1095689025",
        "pub.1110720527",
        "pub.1121790962",
        "pub.1128856964",
        "pub.1136702674",
        "pub.1132270339",
        "pub.1139948145",
        "pub.1044800349",
        "pub.1151381236",
        "pub.1094457070",
        "pub.1133175188",
        "pub.1129724168",
        "pub.1051365551",
        "pub.1129757129",
        "pub.1110448859",
        "pub.1143948984",
        "pub.1131074864",
        "pub.1133175104",
        "pub.1093359587",
        "pub.1099110523",
        "pub.1129757153",
        "pub.1130215742",
        "pub.1128842212",
        "pub.1132119971",
        "pub.1133175498",
        "pub.1139948035",
        "pub.1100060220",
        "pub.1163044197",
        "pub.1028386005",
        "pub.1095811486",
        "pub.1100060465",
        "pub.1099151287",
        "pub.1129757382",
        "pub.1117659471",
        "pub.1163041872",
        "pub.1127172528",
        "pub.1100731697",
        "pub.1061743499",
        "pub.1142390723",
        "pub.1143949327",
        "pub.1098653837",
        "pub.1138571275",
        "pub.1138840291",
        "pub.1099117500",
        "pub.1142776451",
        "pub.1121025051",
        "pub.1052051081",
        "pub.1129913539",
        "pub.1133174427",
        "pub.1150865667",
        "pub.1122290391",
        "pub.1126962100",
        "pub.1125458068",
        "pub.1095181185",
        "pub.1018992209",
        "pub.1151380550",
        "pub.1122290388",
        "pub.1107502756",
        "pub.1037432371",
        "pub.1120590150",
        "pub.1096025493",
        "pub.1125558196",
        "pub.1129756686",
        "pub.1128856715",
        "pub.1148917063",
        "pub.1109703686",
        "pub.1095851797",
        "pub.1099106053",
        "pub.1050587930",
        "pub.1051806676",
        "pub.1095839635",
        "pub.1098653272",
        "pub.1128537995",
        "pub.1128856293",
        "pub.1142386389",
        "pub.1104321191",
        "pub.1107091365",
        "pub.1117658853",
        "pub.1036071780",
        "pub.1149257013",
        "pub.1160174259",
        "pub.1030410723",
        "pub.1152622207",
        "pub.1131136428",
        "pub.1131679904",
        "pub.1151381125",
        "pub.1133175426",
        "pub.1015658513",
        "pub.1104321292",
        "pub.1110957692",
        "pub.1128856356",
        "pub.1129757054",
        "pub.1111638733",
        "pub.1131323312",
        "pub.1069993189",
        "pub.1128856562",
        "pub.1138569911",
        "pub.1148410624",
        "pub.1099110787",
        "pub.1099138490",
        "pub.1126569160",
        "pub.1140685010",
        "pub.1096024344",
        "pub.1122290022",
        "pub.1123112941",
        "pub.1129757149",
        "pub.1094291017",
        "pub.1099106258",
        "pub.1133174463",
        "pub.1133177376",
        "pub.1145901943",
        "pub.1148390892",
        "pub.1129757269",
        "pub.1133174464",
        "pub.1140363638",
        "pub.1001665315",
        "pub.1140693596",
        "pub.1132921735",
        "pub.1127360246",
        "pub.1032677678",
        "pub.1129328797",
        "pub.1128856603",
        "pub.1142372405",
        "pub.1163614899",
        "pub.1095849904",
        "pub.1112140821",
        "pub.1133177059",
        "pub.1039161303",
        "pub.1148881487",
        "pub.1093192734",
        "pub.1122290238",
        "pub.1133290332",
        "pub.1098672059",
        "pub.1135360329",
        "pub.1140657128",
        "pub.1103847832",
        "pub.1129757334",
        "pub.1182813830",
        "pub.1122290589",
        "pub.1146996956",
        "pub.1113541580",
        "pub.1061745117",
        "pub.1125154578",
        "pub.1099110546",
        "pub.1001288203",
        "pub.1024827031",
        "pub.1117658996",
        "pub.1120590151",
        "pub.1003622703",
        "pub.1099110648",
        "pub.1093603006",
        "pub.1107454571",
        "pub.1133174428",
        "pub.1095774569",
        "pub.1149793829",
        "pub.1095850445",
        "pub.1017073734",
        "pub.1121025033",
        "pub.1133175303",
        "pub.1123987743",
        "pub.1129913470",
        "pub.1095390207",
        "pub.1132944314",
        "pub.1100060646",
        "pub.1095843442",
        "pub.1016085920",
        "pub.1134455524",
        "pub.1145901054",
        "pub.1117658665",
        "pub.1099113598",
        "pub.1099221859",
        "pub.1110720299",
        "pub.1085304410",
        "pub.1095180184",
        "pub.1125162766",
        "pub.1133176827",
        "pub.1114229236",
        "pub.1038140272",
        "pub.1099204232",
        "pub.1120536086",
        "pub.1025267224",
        "pub.1002248607",
        "pub.1138571416",
        "pub.1150865897",
        "pub.1130209625",
        "pub.1104321169",
        "pub.1094727707",
        "pub.1123988028",
        "pub.1129756829"
      ],
      "concepts_scores": [
        {
          "concept": "language model",
          "relevance": 0.726
        },
        {
          "concept": "data modalities",
          "relevance": 0.7
        },
        {
          "concept": "contextual language models",
          "relevance": 0.651
        },
        {
          "concept": "Bidirectional Encoder Representations",
          "relevance": 0.648
        },
        {
          "concept": "natural language processing",
          "relevance": 0.642
        },
        {
          "concept": "autoregressive language model",
          "relevance": 0.637
        },
        {
          "concept": "large-scale data",
          "relevance": 0.622
        },
        {
          "concept": "artificial general intelligence",
          "relevance": 0.619
        },
        {
          "concept": "Zero-Shot",
          "relevance": 0.604
        },
        {
          "concept": "downstream tasks",
          "relevance": 0.603
        },
        {
          "concept": "graph learning",
          "relevance": 0.603
        },
        {
          "concept": "pretraining method",
          "relevance": 0.601
        },
        {
          "concept": "computer vision",
          "relevance": 0.601
        },
        {
          "concept": "Encoder Representations",
          "relevance": 0.599
        },
        {
          "concept": "feature extraction",
          "relevance": 0.596
        },
        {
          "concept": "evaluation metrics",
          "relevance": 0.593
        },
        {
          "concept": "recurrent module",
          "relevance": 0.589
        },
        {
          "concept": "parameter initialization",
          "relevance": 0.588
        },
        {
          "concept": "language processing",
          "relevance": 0.584
        },
        {
          "concept": "logical reasoning ability",
          "relevance": 0.573
        },
        {
          "concept": "ChatGPT",
          "relevance": 0.544
        },
        {
          "concept": "BERT",
          "relevance": 0.543
        },
        {
          "concept": "dataset",
          "relevance": 0.539
        },
        {
          "concept": "learning ability",
          "relevance": 0.53
        },
        {
          "concept": "comprehensive survey",
          "relevance": 0.529
        },
        {
          "concept": "research directions",
          "relevance": 0.525
        },
        {
          "concept": "pretraining",
          "relevance": 0.516
        },
        {
          "concept": "data",
          "relevance": 0.516
        },
        {
          "concept": "security",
          "relevance": 0.516
        },
        {
          "concept": "data quality",
          "relevance": 0.513
        },
        {
          "concept": "interaction ability",
          "relevance": 0.513
        },
        {
          "concept": "downstream applications",
          "relevance": 0.495
        },
        {
          "concept": "graph",
          "relevance": 0.494
        },
        {
          "concept": "foundation model",
          "relevance": 0.477
        },
        {
          "concept": "model efficiency",
          "relevance": 0.476
        },
        {
          "concept": "reasoning ability",
          "relevance": 0.474
        },
        {
          "concept": "privacy",
          "relevance": 0.464
        },
        {
          "concept": "scalability",
          "relevance": 0.462
        },
        {
          "concept": "convolution",
          "relevance": 0.455
        },
        {
          "concept": "extractor",
          "relevance": 0.451
        },
        {
          "concept": "intelligence",
          "relevance": 0.437
        },
        {
          "concept": "computer",
          "relevance": 0.436
        },
        {
          "concept": "metrics",
          "relevance": 0.43
        },
        {
          "concept": "task",
          "relevance": 0.425
        },
        {
          "concept": "method",
          "relevance": 0.424
        },
        {
          "concept": "AI",
          "relevance": 0.421
        },
        {
          "concept": "learning",
          "relevance": 0.418
        },
        {
          "concept": "representation",
          "relevance": 0.415
        },
        {
          "concept": "transformation",
          "relevance": 0.411
        },
        {
          "concept": "model",
          "relevance": 0.405
        },
        {
          "concept": "general intelligence",
          "relevance": 0.404
        },
        {
          "concept": "vision",
          "relevance": 0.401
        },
        {
          "concept": "text",
          "relevance": 0.391
        },
        {
          "concept": "language",
          "relevance": 0.39
        },
        {
          "concept": "images",
          "relevance": 0.387
        },
        {
          "concept": "paradigm",
          "relevance": 0.386
        },
        {
          "concept": "research",
          "relevance": 0.385
        },
        {
          "concept": "research advances",
          "relevance": 0.377
        },
        {
          "concept": "features",
          "relevance": 0.375
        },
        {
          "concept": "applications",
          "relevance": 0.368
        },
        {
          "concept": "modulation",
          "relevance": 0.366
        },
        {
          "concept": "data",
          "relevance": 0.359
        },
        {
          "concept": "comprehensive review",
          "relevance": 0.355
        },
        {
          "concept": "success",
          "relevance": 0.346
        },
        {
          "concept": "evaluation",
          "relevance": 0.344
        },
        {
          "concept": "ability",
          "relevance": 0.344
        },
        {
          "concept": "efficiency",
          "relevance": 0.34
        },
        {
          "concept": "challenges",
          "relevance": 0.334
        },
        {
          "concept": "quality",
          "relevance": 0.334
        },
        {
          "concept": "modalities",
          "relevance": 0.326
        },
        {
          "concept": "advances",
          "relevance": 0.319
        },
        {
          "concept": "process",
          "relevance": 0.307
        },
        {
          "concept": "extraction",
          "relevance": 0.305
        },
        {
          "concept": "generation",
          "relevance": 0.304
        },
        {
          "concept": "prompts",
          "relevance": 0.292
        },
        {
          "concept": "direction",
          "relevance": 0.289
        },
        {
          "concept": "opportunities",
          "relevance": 0.286
        },
        {
          "concept": "survey",
          "relevance": 0.284
        },
        {
          "concept": "foundations",
          "relevance": 0.283
        },
        {
          "concept": "components",
          "relevance": 0.275
        },
        {
          "concept": "breakthrough",
          "relevance": 0.267
        },
        {
          "concept": "history",
          "relevance": 0.223
        },
        {
          "concept": "quantity",
          "relevance": 0.221
        },
        {
          "concept": "initiation",
          "relevance": 0.219
        },
        {
          "concept": "review",
          "relevance": 0.214
        },
        {
          "concept": "study",
          "relevance": 0.205
        }
      ]
    },
    {
      "paperId": "pub.1167960714",
      "doi": "10.1109/iccv51070.2023.00371",
      "title": "Segment Anything",
      "year": 2023,
      "citationCount": 5512,
      "fieldCitationRatio": 4894.05,
      "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643.",
      "reference_ids": [
        "pub.1100060535",
        "pub.1152094973",
        "pub.1054097506",
        "pub.1110720201",
        "pub.1094706336",
        "pub.1132870967",
        "pub.1094776080",
        "pub.1100060309",
        "pub.1061744793",
        "pub.1151380046",
        "pub.1121388327",
        "pub.1017544873",
        "pub.1135599058",
        "pub.1096025544",
        "pub.1148786000",
        "pub.1142371099",
        "pub.1093519792",
        "pub.1094727707",
        "pub.1094045097",
        "pub.1142026146",
        "pub.1152463747",
        "pub.1009092998",
        "pub.1133070135",
        "pub.1167962380",
        "pub.1153002462",
        "pub.1110720205",
        "pub.1151379744",
        "pub.1107559592",
        "pub.1093276061",
        "pub.1121964315",
        "pub.1151381448",
        "pub.1094818963",
        "pub.1148738128",
        "pub.1093838265",
        "pub.1148815244",
        "pub.1145901529",
        "pub.1061745117",
        "pub.1061743879",
        "pub.1019522212",
        "pub.1038262463",
        "pub.1145130913",
        "pub.1094752767",
        "pub.1090555548",
        "pub.1016330466",
        "pub.1093603006",
        "pub.1144444869",
        "pub.1163452825",
        "pub.1095756716",
        "pub.1125155425",
        "pub.1093359587",
        "pub.1140080886",
        "pub.1163452243",
        "pub.1079241288",
        "pub.1151381159",
        "pub.1101186140",
        "pub.1107037508",
        "pub.1148146933",
        "pub.1133361893",
        "pub.1100517007",
        "pub.1111334730",
        "pub.1151379755",
        "pub.1123988028",
        "pub.1110448859",
        "pub.1153989295",
        "pub.1143336252",
        "pub.1152509756",
        "pub.1151381653",
        "pub.1061742261",
        "pub.1040861461",
        "pub.1123988441",
        "pub.1151379745",
        "pub.1132270339",
        "pub.1025617980",
        "pub.1095471552",
        "pub.1045321436",
        "pub.1125621483",
        "pub.1039042955",
        "pub.1094653828",
        "pub.1145902066"
      ],
      "concepts_scores": [
        {
          "concept": "zero-shot performance",
          "relevance": 0.566
        },
        {
          "concept": "Zero-Shot",
          "relevance": 0.523
        },
        {
          "concept": "segmentation dataset",
          "relevance": 0.52
        },
        {
          "concept": "computer vision",
          "relevance": 0.52
        },
        {
          "concept": "image segmentation",
          "relevance": 0.511
        },
        {
          "concept": "image distribution",
          "relevance": 0.495
        },
        {
          "concept": "supervision results",
          "relevance": 0.482
        },
        {
          "concept": "dataset",
          "relevance": 0.466
        },
        {
          "concept": "numerous tasks",
          "relevance": 0.454
        },
        {
          "concept": "task",
          "relevance": 0.447
        },
        {
          "concept": "efficient model",
          "relevance": 0.441
        },
        {
          "concept": "images",
          "relevance": 0.414
        },
        {
          "concept": "collection loop",
          "relevance": 0.41
        },
        {
          "concept": "privacy",
          "relevance": 0.402
        },
        {
          "concept": "segments",
          "relevance": 0.394
        },
        {
          "concept": "computer",
          "relevance": 0.378
        },
        {
          "concept": "mask",
          "relevance": 0.377
        },
        {
          "concept": "data",
          "relevance": 0.367
        },
        {
          "concept": "model",
          "relevance": 0.349
        },
        {
          "concept": "vision",
          "relevance": 0.347
        },
        {
          "concept": "capability",
          "relevance": 0.34
        },
        {
          "concept": "performance",
          "relevance": 0.337
        },
        {
          "concept": "research",
          "relevance": 0.274
        },
        {
          "concept": "Anything",
          "relevance": 0.263
        },
        {
          "concept": "results",
          "relevance": 0.258
        },
        {
          "concept": "loop",
          "relevance": 0.248
        },
        {
          "concept": "distribution",
          "relevance": 0.227
        },
        {
          "concept": "impression",
          "relevance": 0.223
        }
      ]
    },
    {
      "paperId": "pub.1151381159",
      "doi": "10.1109/cvpr52688.2022.01553",
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "year": 2022,
      "citationCount": 5360,
      "fieldCitationRatio": 2104.78,
      "abstract": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.",
      "reference_ids": [
        "pub.1145901816",
        "pub.1095823695",
        "pub.1095849919",
        "pub.1145901944",
        "pub.1093359587",
        "pub.1107502560",
        "pub.1051732355",
        "pub.1039042955",
        "pub.1145901084",
        "pub.1110448859",
        "pub.1093497718",
        "pub.1095852454",
        "pub.1024827031",
        "pub.1142370059",
        "pub.1107463261",
        "pub.1092541367",
        "pub.1025150743",
        "pub.1125161350",
        "pub.1095706293",
        "pub.1100060307",
        "pub.1142372405",
        "pub.1145901943",
        "pub.1110721048",
        "pub.1129913470",
        "pub.1129724168",
        "pub.1095181185",
        "pub.1110720527",
        "pub.1093767478",
        "pub.1045321436",
        "pub.1034603392",
        "pub.1142380802",
        "pub.1016085920",
        "pub.1095689025"
      ],
      "concepts_scores": [
        {
          "concept": "input image",
          "relevance": 0.736
        },
        {
          "concept": "asymmetric encoder-decoder architecture",
          "relevance": 0.707
        },
        {
          "concept": "ImageNet-1K data",
          "relevance": 0.707
        },
        {
          "concept": "encoder-decoder architecture",
          "relevance": 0.684
        },
        {
          "concept": "high-capacity models",
          "relevance": 0.683
        },
        {
          "concept": "lightweight decoder",
          "relevance": 0.639
        },
        {
          "concept": "supervised pretraining",
          "relevance": 0.637
        },
        {
          "concept": "downstream tasks",
          "relevance": 0.635
        },
        {
          "concept": "latent representation",
          "relevance": 0.634
        },
        {
          "concept": "computer vision",
          "relevance": 0.633
        },
        {
          "concept": "missing pixels",
          "relevance": 0.631
        },
        {
          "concept": "original image",
          "relevance": 0.63
        },
        {
          "concept": "mask tokens",
          "relevance": 0.629
        },
        {
          "concept": "Masked Autoencoders",
          "relevance": 0.628
        },
        {
          "concept": "visible subset",
          "relevance": 0.611
        },
        {
          "concept": "random patches",
          "relevance": 0.603
        },
        {
          "concept": "accelerated training",
          "relevance": 0.595
        },
        {
          "concept": "autoencoder",
          "relevance": 0.574
        },
        {
          "concept": "improved accuracy",
          "relevance": 0.553
        },
        {
          "concept": "MAE approach",
          "relevance": 0.55
        },
        {
          "concept": "task",
          "relevance": 0.518
        },
        {
          "concept": "images",
          "relevance": 0.496
        },
        {
          "concept": "accuracy",
          "relevance": 0.494
        },
        {
          "concept": "decoding",
          "relevance": 0.492
        },
        {
          "concept": "scalability",
          "relevance": 0.486
        },
        {
          "concept": "input",
          "relevance": 0.483
        },
        {
          "concept": "encoding",
          "relevance": 0.476
        },
        {
          "concept": "pixel",
          "relevance": 0.474
        },
        {
          "concept": "MAE",
          "relevance": 0.472
        },
        {
          "concept": "pretraining",
          "relevance": 0.469
        },
        {
          "concept": "tokens",
          "relevance": 0.465
        },
        {
          "concept": "architecture",
          "relevance": 0.464
        },
        {
          "concept": "computer",
          "relevance": 0.46
        },
        {
          "concept": "vanilla",
          "relevance": 0.456
        },
        {
          "concept": "lightweight",
          "relevance": 0.439
        },
        {
          "concept": "representation",
          "relevance": 0.437
        },
        {
          "concept": "K data",
          "relevance": 0.429
        },
        {
          "concept": "core design",
          "relevance": 0.426
        },
        {
          "concept": "learners",
          "relevance": 0.422
        },
        {
          "concept": "vision",
          "relevance": 0.422
        },
        {
          "concept": "design",
          "relevance": 0.418
        },
        {
          "concept": "model",
          "relevance": 0.415
        },
        {
          "concept": "performance",
          "relevance": 0.411
        },
        {
          "concept": "training",
          "relevance": 0.407
        },
        {
          "concept": "patches",
          "relevance": 0.393
        },
        {
          "concept": "method",
          "relevance": 0.361
        },
        {
          "concept": "scaling behavior",
          "relevance": 0.306
        },
        {
          "concept": "core",
          "relevance": 0.301
        },
        {
          "concept": "behavior",
          "relevance": 0.267
        },
        {
          "concept": "transfer performance",
          "relevance": 0.258
        },
        {
          "concept": "transfer",
          "relevance": 0.25
        },
        {
          "concept": "scale",
          "relevance": 0.238
        },
        {
          "concept": "approach",
          "relevance": 0.151
        },
        {
          "concept": "proportion",
          "relevance": 0.113
        }
      ]
    },
    {
      "paperId": "pub.1100060307",
      "doi": "10.1109/iccv.2017.322",
      "title": "Mask R-CNN",
      "year": 2017,
      "citationCount": 23345,
      "fieldCitationRatio": 4896.65,
      "abstract": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.",
      "reference_ids": [
        "pub.1095604624",
        "pub.1045321436",
        "pub.1093518416",
        "pub.1095840375",
        "pub.1095686079",
        "pub.1094727707",
        "pub.1093292479",
        "pub.1061745117",
        "pub.1093359587",
        "pub.1085642448",
        "pub.1094164376",
        "pub.1095850372",
        "pub.1093572203",
        "pub.1008345178",
        "pub.1095573598",
        "pub.1095646840",
        "pub.1095837104",
        "pub.1095839166",
        "pub.1094492451",
        "pub.1003201959",
        "pub.1030874879",
        "pub.1061744921",
        "pub.1061744812",
        "pub.1093700510",
        "pub.1001688760",
        "pub.1095852454",
        "pub.1093921969",
        "pub.1093626237",
        "pub.1033900312"
      ],
      "concepts_scores": [
        {
          "concept": "Mask R-CNN",
          "relevance": 0.818
        },
        {
          "concept": "R-CNN",
          "relevance": 0.771
        },
        {
          "concept": "high-quality segmentation masks",
          "relevance": 0.695
        },
        {
          "concept": "COCO suite of challenges",
          "relevance": 0.695
        },
        {
          "concept": "person keypoint detection",
          "relevance": 0.677
        },
        {
          "concept": "single-model entries",
          "relevance": 0.677
        },
        {
          "concept": "instance-level recognition",
          "relevance": 0.677
        },
        {
          "concept": "estimate human pose",
          "relevance": 0.675
        },
        {
          "concept": "object instance segmentation",
          "relevance": 0.672
        },
        {
          "concept": "Faster R-CNN",
          "relevance": 0.669
        },
        {
          "concept": "Faster R-CNN",
          "relevance": 0.668
        },
        {
          "concept": "human pose",
          "relevance": 0.626
        },
        {
          "concept": "keypoint detection",
          "relevance": 0.624
        },
        {
          "concept": "object masks",
          "relevance": 0.622
        },
        {
          "concept": "object detection",
          "relevance": 0.622
        },
        {
          "concept": "instance segmentation",
          "relevance": 0.62
        },
        {
          "concept": "segmentation masks",
          "relevance": 0.616
        },
        {
          "concept": "box recognition",
          "relevance": 0.586
        },
        {
          "concept": "solid baseline",
          "relevance": 0.577
        },
        {
          "concept": "suite of challenges",
          "relevance": 0.573
        },
        {
          "concept": "COCO",
          "relevance": 0.523
        },
        {
          "concept": "task",
          "relevance": 0.509
        },
        {
          "concept": "recognition",
          "relevance": 0.489
        },
        {
          "concept": "mask",
          "relevance": 0.486
        },
        {
          "concept": "pose",
          "relevance": 0.473
        },
        {
          "concept": "Faster",
          "relevance": 0.472
        },
        {
          "concept": "effective approach",
          "relevance": 0.455
        },
        {
          "concept": "objective",
          "relevance": 0.451
        },
        {
          "concept": "detection",
          "relevance": 0.45
        },
        {
          "concept": "code",
          "relevance": 0.447
        },
        {
          "concept": "segments",
          "relevance": 0.441
        },
        {
          "concept": "tricks",
          "relevance": 0.418
        },
        {
          "concept": "tracking",
          "relevance": 0.411
        },
        {
          "concept": "framework",
          "relevance": 0.405
        },
        {
          "concept": "images",
          "relevance": 0.4
        },
        {
          "concept": "box",
          "relevance": 0.367
        },
        {
          "concept": "winners",
          "relevance": 0.366
        },
        {
          "concept": "method",
          "relevance": 0.355
        },
        {
          "concept": "challenges",
          "relevance": 0.346
        },
        {
          "concept": "research",
          "relevance": 0.327
        },
        {
          "concept": "results",
          "relevance": 0.309
        },
        {
          "concept": "branches",
          "relevance": 0.307
        },
        {
          "concept": "persons",
          "relevance": 0.292
        },
        {
          "concept": "entry",
          "relevance": 0.281
        },
        {
          "concept": "baseline",
          "relevance": 0.249
        },
        {
          "concept": "approach",
          "relevance": 0.218
        }
      ]
    },
    {
      "paperId": "pub.1095852454",
      "doi": "10.1109/cvpr.2017.106",
      "title": "Feature Pyramid Networks for Object Detection",
      "year": 2017,
      "citationCount": 22424,
      "fieldCitationRatio": 4520.41,
      "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",
      "reference_ids": [
        "pub.1045321436",
        "pub.1093626237",
        "pub.1017177111",
        "pub.1093997066",
        "pub.1052687286",
        "pub.1001688760",
        "pub.1094046638",
        "pub.1017774818",
        "pub.1094224567",
        "pub.1014346769",
        "pub.1008345178",
        "pub.1030874879",
        "pub.1061744812",
        "pub.1049647714",
        "pub.1009767488",
        "pub.1061745117",
        "pub.1030406568",
        "pub.1093700510",
        "pub.1093359587",
        "pub.1033900312",
        "pub.1085642448",
        "pub.1094727707",
        "pub.1096897144",
        "pub.1061743745",
        "pub.1096897042",
        "pub.1047850584",
        "pub.1095646840",
        "pub.1061744626",
        "pub.1061745147",
        "pub.1096897141",
        "pub.1095379009",
        "pub.1095573598",
        "pub.1095686079",
        "pub.1056860333"
      ],
      "concepts_scores": [
        {
          "concept": "feature pyramid network",
          "relevance": 0.841
        },
        {
          "concept": "feature pyramid",
          "relevance": 0.732
        },
        {
          "concept": "pyramid network",
          "relevance": 0.729
        },
        {
          "concept": "object detection",
          "relevance": 0.727
        },
        {
          "concept": "multi-scale object detection",
          "relevance": 0.7
        },
        {
          "concept": "deep learning object detector",
          "relevance": 0.689
        },
        {
          "concept": "Faster R-CNN system",
          "relevance": 0.687
        },
        {
          "concept": "COCO detection benchmark",
          "relevance": 0.683
        },
        {
          "concept": "single-model entries",
          "relevance": 0.683
        },
        {
          "concept": "semantic feature maps",
          "relevance": 0.68
        },
        {
          "concept": "learning object detectors",
          "relevance": 0.677
        },
        {
          "concept": "single-model results",
          "relevance": 0.662
        },
        {
          "concept": "top-down architecture",
          "relevance": 0.651
        },
        {
          "concept": "detection benchmarks",
          "relevance": 0.631
        },
        {
          "concept": "pyramid representation",
          "relevance": 0.63
        },
        {
          "concept": "object detectors",
          "relevance": 0.629
        },
        {
          "concept": "feature maps",
          "relevance": 0.628
        },
        {
          "concept": "recognition system",
          "relevance": 0.622
        },
        {
          "concept": "marginal extra cost",
          "relevance": 0.619
        },
        {
          "concept": "detect objects",
          "relevance": 0.618
        },
        {
          "concept": "pyramidal hierarchy",
          "relevance": 0.599
        },
        {
          "concept": "multi-scale",
          "relevance": 0.579
        },
        {
          "concept": "lateral connections",
          "relevance": 0.558
        },
        {
          "concept": "architecture",
          "relevance": 0.532
        },
        {
          "concept": "network",
          "relevance": 0.529
        },
        {
          "concept": "COCO",
          "relevance": 0.528
        },
        {
          "concept": "accurate solutions",
          "relevance": 0.516
        },
        {
          "concept": "pyramid",
          "relevance": 0.49
        },
        {
          "concept": "GPU",
          "relevance": 0.488
        },
        {
          "concept": "FPS",
          "relevance": 0.481
        },
        {
          "concept": "features",
          "relevance": 0.477
        },
        {
          "concept": "extra costs",
          "relevance": 0.462
        },
        {
          "concept": "detection",
          "relevance": 0.454
        },
        {
          "concept": "benchmarks",
          "relevance": 0.452
        },
        {
          "concept": "code",
          "relevance": 0.451
        },
        {
          "concept": "objective",
          "relevance": 0.433
        },
        {
          "concept": "representation",
          "relevance": 0.433
        },
        {
          "concept": "system",
          "relevance": 0.429
        },
        {
          "concept": "recognition",
          "relevance": 0.426
        },
        {
          "concept": "method",
          "relevance": 0.415
        },
        {
          "concept": "memory",
          "relevance": 0.393
        },
        {
          "concept": "maps",
          "relevance": 0.393
        },
        {
          "concept": "applications",
          "relevance": 0.384
        },
        {
          "concept": "whistles",
          "relevance": 0.378
        },
        {
          "concept": "cost",
          "relevance": 0.37
        },
        {
          "concept": "winners",
          "relevance": 0.369
        },
        {
          "concept": "connection",
          "relevance": 0.367
        },
        {
          "concept": "detector",
          "relevance": 0.36
        },
        {
          "concept": "solution",
          "relevance": 0.349
        },
        {
          "concept": "improvement",
          "relevance": 0.326
        },
        {
          "concept": "results",
          "relevance": 0.312
        },
        {
          "concept": "components",
          "relevance": 0.287
        },
        {
          "concept": "entry",
          "relevance": 0.283
        },
        {
          "concept": "scale",
          "relevance": 0.273
        },
        {
          "concept": "Bell",
          "relevance": 0.249
        }
      ]
    },
    {
      "paperId": "pub.1151379744",
      "doi": "10.1109/cvpr52688.2022.00135",
      "title": "Masked-attention Mask Transformer for Universal Image Segmentation",
      "year": 2022,
      "citationCount": 1903,
      "fieldCitationRatio": 734.08,
      "abstract": "Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing spe-cialized architectures for each task. We present Masked- attention Mask Transformer (Mask2Former), a new archi-tecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components in-clude masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most no-tably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU onADE20K).",
      "reference_ids": [
        "pub.1142371099",
        "pub.1107463261",
        "pub.1145901676",
        "pub.1146314706",
        "pub.1095839207",
        "pub.1110720947",
        "pub.1110720778",
        "pub.1100060309",
        "pub.1123987989",
        "pub.1145901353",
        "pub.1150407946",
        "pub.1085304410",
        "pub.1138301739",
        "pub.1100060307",
        "pub.1132263657",
        "pub.1145901710",
        "pub.1095851600",
        "pub.1145901083",
        "pub.1125154578",
        "pub.1123987804",
        "pub.1095837190",
        "pub.1009767488",
        "pub.1100060542",
        "pub.1107454614",
        "pub.1123988134",
        "pub.1126062535",
        "pub.1045321436",
        "pub.1145901356",
        "pub.1093838265",
        "pub.1129913477",
        "pub.1123988380",
        "pub.1151379873",
        "pub.1145901352",
        "pub.1061745117",
        "pub.1123988441",
        "pub.1129913574",
        "pub.1093626237",
        "pub.1017073734",
        "pub.1094706336",
        "pub.1093192734",
        "pub.1033900312",
        "pub.1095852454",
        "pub.1132270339",
        "pub.1142386871",
        "pub.1093359587",
        "pub.1094492451"
      ],
      "concepts_scores": [
        {
          "concept": "mask transformer",
          "relevance": 0.7
        },
        {
          "concept": "image segmentation tasks",
          "relevance": 0.648
        },
        {
          "concept": "extract local features",
          "relevance": 0.646
        },
        {
          "concept": "panoptic segmentation",
          "relevance": 0.603
        },
        {
          "concept": "cross-attention",
          "relevance": 0.602
        },
        {
          "concept": "semantic segmentation",
          "relevance": 0.599
        },
        {
          "concept": "segmentation task",
          "relevance": 0.596
        },
        {
          "concept": "image segmentation",
          "relevance": 0.589
        },
        {
          "concept": "specialized architectures",
          "relevance": 0.573
        },
        {
          "concept": "significant margin",
          "relevance": 0.568
        },
        {
          "concept": "mask region",
          "relevance": 0.56
        },
        {
          "concept": "semantics",
          "relevance": 0.557
        },
        {
          "concept": "task",
          "relevance": 0.524
        },
        {
          "concept": "architecture",
          "relevance": 0.508
        },
        {
          "concept": "no-tably",
          "relevance": 0.472
        },
        {
          "concept": "research efforts",
          "relevance": 0.464
        },
        {
          "concept": "segments",
          "relevance": 0.454
        },
        {
          "concept": "images",
          "relevance": 0.446
        },
        {
          "concept": "dataset",
          "relevance": 0.442
        },
        {
          "concept": "segment group",
          "relevance": 0.437
        },
        {
          "concept": "transformation",
          "relevance": 0.409
        },
        {
          "concept": "features",
          "relevance": 0.374
        },
        {
          "concept": "research",
          "relevance": 0.365
        },
        {
          "concept": "in-cluded",
          "relevance": 0.355
        },
        {
          "concept": "efforts",
          "relevance": 0.314
        },
        {
          "concept": "attention",
          "relevance": 0.309
        },
        {
          "concept": "extraction",
          "relevance": 0.304
        },
        {
          "concept": "University",
          "relevance": 0.276
        },
        {
          "concept": "components",
          "relevance": 0.274
        },
        {
          "concept": "margin",
          "relevance": 0.211
        },
        {
          "concept": "region",
          "relevance": 0.189
        },
        {
          "concept": "group",
          "relevance": 0.178
        }
      ]
    },
    {
      "paperId": "pub.1132270339",
      "doi": "10.1007/978-3-030-58452-8_13",
      "title": "End-to-End Object Detection with Transformers",
      "year": 2020,
      "citationCount": 13281,
      "fieldCitationRatio": 3743.11,
      "abstract": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
      "reference_ids": [
        "pub.1100060572",
        "pub.1095738839",
        "pub.1125160668",
        "pub.1120932896",
        "pub.1110720947",
        "pub.1122290650",
        "pub.1095474558",
        "pub.1095839166",
        "pub.1123988134",
        "pub.1093359587",
        "pub.1017177111",
        "pub.1053318791",
        "pub.1095811486",
        "pub.1045321436",
        "pub.1123988380",
        "pub.1122988422",
        "pub.1123988441",
        "pub.1093838265",
        "pub.1125159756",
        "pub.1100060307",
        "pub.1129913473",
        "pub.1094073194",
        "pub.1100060607",
        "pub.1100060309",
        "pub.1125161029",
        "pub.1032778056",
        "pub.1095851355",
        "pub.1126961449",
        "pub.1110720778",
        "pub.1061745117",
        "pub.1095852454",
        "pub.1110720512"
      ],
      "concepts_scores": [
        {
          "concept": "object detection",
          "relevance": 0.726
        },
        {
          "concept": "end-to-end object detection",
          "relevance": 0.708
        },
        {
          "concept": "non-maximum suppression procedure",
          "relevance": 0.701
        },
        {
          "concept": "COCO object detection dataset",
          "relevance": 0.701
        },
        {
          "concept": "transformer encoder-decoder architecture",
          "relevance": 0.698
        },
        {
          "concept": "hand-designed components",
          "relevance": 0.683
        },
        {
          "concept": "forces unique predictions",
          "relevance": 0.683
        },
        {
          "concept": "learned object queries",
          "relevance": 0.683
        },
        {
          "concept": "object detection datasets",
          "relevance": 0.681
        },
        {
          "concept": "global image context",
          "relevance": 0.68
        },
        {
          "concept": "encoder-decoder architecture",
          "relevance": 0.678
        },
        {
          "concept": "run-time performance",
          "relevance": 0.673
        },
        {
          "concept": "end-to-end",
          "relevance": 0.664
        },
        {
          "concept": "anchor generation",
          "relevance": 0.631
        },
        {
          "concept": "object queries",
          "relevance": 0.631
        },
        {
          "concept": "panoptic segmentation",
          "relevance": 0.631
        },
        {
          "concept": "competitive baselines",
          "relevance": 0.631
        },
        {
          "concept": "detection dataset",
          "relevance": 0.63
        },
        {
          "concept": "pretrained models",
          "relevance": 0.625
        },
        {
          "concept": "training code",
          "relevance": 0.623
        },
        {
          "concept": "image context",
          "relevance": 0.619
        },
        {
          "concept": "detection pipeline",
          "relevance": 0.613
        },
        {
          "concept": "prediction problem",
          "relevance": 0.596
        },
        {
          "concept": "bipartite matching",
          "relevance": 0.586
        },
        {
          "concept": "detectable transformation",
          "relevance": 0.574
        },
        {
          "concept": "DETR",
          "relevance": 0.521
        },
        {
          "concept": "modern detectors",
          "relevance": 0.517
        },
        {
          "concept": "query",
          "relevance": 0.489
        },
        {
          "concept": "detection",
          "relevance": 0.484
        },
        {
          "concept": "dataset",
          "relevance": 0.462
        },
        {
          "concept": "architecture",
          "relevance": 0.459
        },
        {
          "concept": "COCO",
          "relevance": 0.455
        },
        {
          "concept": "code",
          "relevance": 0.451
        },
        {
          "concept": "transformation",
          "relevance": 0.45
        },
        {
          "concept": "suppression procedure",
          "relevance": 0.447
        },
        {
          "concept": "task",
          "relevance": 0.443
        },
        {
          "concept": "prediction",
          "relevance": 0.424
        },
        {
          "concept": "accuracy",
          "relevance": 0.423
        },
        {
          "concept": "matching",
          "relevance": 0.414
        },
        {
          "concept": "pipeline",
          "relevance": 0.411
        },
        {
          "concept": "framework",
          "relevance": 0.409
        },
        {
          "concept": "performance",
          "relevance": 0.407
        },
        {
          "concept": "training",
          "relevance": 0.404
        },
        {
          "concept": "model",
          "relevance": 0.392
        },
        {
          "concept": "segments",
          "relevance": 0.384
        },
        {
          "concept": "objective",
          "relevance": 0.374
        },
        {
          "concept": "detector",
          "relevance": 0.36
        },
        {
          "concept": "method",
          "relevance": 0.358
        },
        {
          "concept": "knowledge",
          "relevance": 0.355
        },
        {
          "concept": "reasons",
          "relevance": 0.348
        },
        {
          "concept": "context",
          "relevance": 0.336
        },
        {
          "concept": "anchor",
          "relevance": 0.318
        },
        {
          "concept": "generation",
          "relevance": 0.317
        },
        {
          "concept": "unique prediction",
          "relevance": 0.29
        },
        {
          "concept": "baseline",
          "relevance": 0.29
        },
        {
          "concept": "components",
          "relevance": 0.287
        },
        {
          "concept": "relations",
          "relevance": 0.276
        },
        {
          "concept": "procedure",
          "relevance": 0.253
        },
        {
          "concept": "ingredients",
          "relevance": 0.179
        },
        {
          "concept": "problem",
          "relevance": 0.17
        }
      ]
    },
    {
      "paperId": "pub.1129913574",
      "doi": "10.1109/cvpr42600.2020.01079",
      "title": "EfficientDet: Scalable and Efficient Object Detection",
      "year": 2020,
      "citationCount": 6504,
      "fieldCitationRatio": 1833.09,
      "abstract": "Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDet-D7 achieves state-of-the-art 52.2 AP on COCO t est-dev with 52M parameters and 325B FLOPs11Similar to [12], [36], FLOPs denotes number of multiply-adds., being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detector. Code is available at https://github.com/google/automl/tree/master/efficientdet. Similar to [12], [36], FLOPs denotes number of multiply-adds.",
      "reference_ids": [
        "pub.1095836678",
        "pub.1125160668",
        "pub.1095850372",
        "pub.1045321436",
        "pub.1095851797",
        "pub.1100060307",
        "pub.1110720778",
        "pub.1061745117",
        "pub.1123988134",
        "pub.1125163328",
        "pub.1100319184",
        "pub.1093921969",
        "pub.1017177111",
        "pub.1123988198",
        "pub.1017073734",
        "pub.1107502747",
        "pub.1107463245",
        "pub.1110721047",
        "pub.1107463249",
        "pub.1095852454",
        "pub.1110720196",
        "pub.1095573598",
        "pub.1107454614",
        "pub.1120428549",
        "pub.1120757994",
        "pub.1123987771",
        "pub.1125161029",
        "pub.1093359587"
      ],
      "concepts_scores": [
        {
          "concept": "multiply-add",
          "relevance": 0.672
        },
        {
          "concept": "compound scaling method",
          "relevance": 0.657
        },
        {
          "concept": "architectural design choices",
          "relevance": 0.643
        },
        {
          "concept": "object detectors",
          "relevance": 0.621
        },
        {
          "concept": "computer vision",
          "relevance": 0.62
        },
        {
          "concept": "object detection",
          "relevance": 0.62
        },
        {
          "concept": "feature network",
          "relevance": 0.617
        },
        {
          "concept": "EfficientNet backbone",
          "relevance": 0.616
        },
        {
          "concept": "prediction network",
          "relevance": 0.604
        },
        {
          "concept": "design choices",
          "relevance": 0.576
        },
        {
          "concept": "resource constraints",
          "relevance": 0.562
        },
        {
          "concept": "single-scale",
          "relevance": 0.552
        },
        {
          "concept": "single-model",
          "relevance": 0.55
        },
        {
          "concept": "flop",
          "relevance": 0.549
        },
        {
          "concept": "network",
          "relevance": 0.522
        },
        {
          "concept": "improve efficiency",
          "relevance": 0.498
        },
        {
          "concept": "model efficiency",
          "relevance": 0.491
        },
        {
          "concept": "BiFPN",
          "relevance": 0.488
        },
        {
          "concept": "EfficientDet",
          "relevance": 0.485
        },
        {
          "concept": "optimization",
          "relevance": 0.482
        },
        {
          "concept": "EfficientNet",
          "relevance": 0.481
        },
        {
          "concept": "computer",
          "relevance": 0.45
        },
        {
          "concept": "COCO",
          "relevance": 0.45
        },
        {
          "concept": "code",
          "relevance": 0.445
        },
        {
          "concept": "efficiency",
          "relevance": 0.426
        },
        {
          "concept": "vision",
          "relevance": 0.413
        },
        {
          "concept": "detector",
          "relevance": 0.412
        },
        {
          "concept": "scaling method",
          "relevance": 0.404
        },
        {
          "concept": "constraints",
          "relevance": 0.402
        },
        {
          "concept": "detection",
          "relevance": 0.387
        },
        {
          "concept": "features",
          "relevance": 0.387
        },
        {
          "concept": "backbone",
          "relevance": 0.385
        },
        {
          "concept": "resources",
          "relevance": 0.378
        },
        {
          "concept": "fusion",
          "relevance": 0.375
        },
        {
          "concept": "objective",
          "relevance": 0.369
        },
        {
          "concept": "AP",
          "relevance": 0.36
        },
        {
          "concept": "method",
          "relevance": 0.354
        },
        {
          "concept": "model",
          "relevance": 0.334
        },
        {
          "concept": "resolution",
          "relevance": 0.311
        },
        {
          "concept": "parameters",
          "relevance": 0.292
        },
        {
          "concept": "choice",
          "relevance": 0.273
        },
        {
          "concept": "family",
          "relevance": 0.255
        },
        {
          "concept": "depth",
          "relevance": 0.246
        },
        {
          "concept": "systematically",
          "relevance": 0.223
        },
        {
          "concept": "compounds",
          "relevance": 0.195
        }
      ]
    }
  ],
  "evolution_links": [
    {
      "source": "pub.1182683020",
      "target": "pub.1167960714",
      "source_title": "A comprehensive survey on pretrained foundation models: a history from BERT to ChatGPT",
      "target_title": "Segment Anything"
    },
    {
      "source": "pub.1167960714",
      "target": "pub.1151381159",
      "source_title": "Segment Anything",
      "target_title": "Masked Autoencoders Are Scalable Vision Learners"
    },
    {
      "source": "pub.1151381159",
      "target": "pub.1100060307",
      "source_title": "Masked Autoencoders Are Scalable Vision Learners",
      "target_title": "Mask R-CNN"
    },
    {
      "source": "pub.1151381159",
      "target": "pub.1095852454",
      "source_title": "Masked Autoencoders Are Scalable Vision Learners",
      "target_title": "Feature Pyramid Networks for Object Detection"
    },
    {
      "source": "pub.1167960714",
      "target": "pub.1151379744",
      "source_title": "Segment Anything",
      "target_title": "Masked-attention Mask Transformer for Universal Image Segmentation"
    },
    {
      "source": "pub.1151379744",
      "target": "pub.1132270339",
      "source_title": "Masked-attention Mask Transformer for Universal Image Segmentation",
      "target_title": "End-to-End Object Detection with Transformers"
    },
    {
      "source": "pub.1151379744",
      "target": "pub.1129913574",
      "source_title": "Masked-attention Mask Transformer for Universal Image Segmentation",
      "target_title": "EfficientDet: Scalable and Efficient Object Detection"
    }
  ]
}