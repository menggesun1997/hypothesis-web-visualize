{
  "papers": [
    {
      "paperId": "pub.1170824474",
      "doi": "10.3390/biomedinformatics4020062",
      "title": "Recent Advances in Large Language Models for Healthcare",
      "year": 2024,
      "citationCount": 38,
      "fieldCitationRatio": NaN,
      "abstract": "Recent advances in the field of large language models (LLMs) underline their high potential for applications in a variety of sectors. Their use in healthcare, in particular, holds out promising prospects for improving medical practices. As we highlight in this paper, LLMs have demonstrated remarkable capabilities in language understanding and generation that could indeed be put to good use in the medical field. We also present the main architectures of these models, such as GPT, Bloom, or LLaMA, composed of billions of parameters. We then examine recent trends in the medical datasets used to train these models. We classify them according to different criteria, such as size, source, or subject (patient records, scientific articles, etc.). We mention that LLMs could help improve patient care, accelerate medical research, and optimize the efficiency of healthcare systems such as assisted diagnosis. We also highlight several technical and ethical issues that need to be resolved before LLMs can be used extensively in the medical field. Consequently, we propose a discussion of the capabilities offered by new generations of linguistic models and their limitations when deployed in a domain such as healthcare.",
      "reference_ids": [
        "pub.1155936177",
        "pub.1160571733",
        "pub.1138337551",
        "pub.1041027669",
        "pub.1140743256",
        "pub.1120569918",
        "pub.1162792350",
        "pub.1155270525",
        "pub.1162702121",
        "pub.1159417791",
        "pub.1051828581",
        "pub.1159718833",
        "pub.1121783729",
        "pub.1155687230",
        "pub.1143672116",
        "pub.1121025781",
        "pub.1163044787",
        "pub.1167824010",
        "pub.1061297264",
        "pub.1132085930",
        "pub.1157166150",
        "pub.1110720770",
        "pub.1148391139",
        "pub.1154959868",
        "pub.1086224488",
        "pub.1079394232",
        "pub.1146180570",
        "pub.1155830303",
        "pub.1143948984",
        "pub.1155338534",
        "pub.1156219917",
        "pub.1169742764",
        "pub.1013352032",
        "pub.1138255966",
        "pub.1145846077",
        "pub.1153534647",
        "pub.1163044728",
        "pub.1157014000",
        "pub.1162833440",
        "pub.1110034687",
        "pub.1123309938",
        "pub.1163128628",
        "pub.1001219189",
        "pub.1059743922",
        "pub.1154476459",
        "pub.1129041332",
        "pub.1143481163",
        "pub.1157766869",
        "pub.1168286792",
        "pub.1136739984",
        "pub.1015151655",
        "pub.1163044729",
        "pub.1142372360",
        "pub.1133175157",
        "pub.1170211509",
        "pub.1166226542",
        "pub.1014067078",
        "pub.1144936347",
        "pub.1163044722",
        "pub.1158191617",
        "pub.1043435078",
        "pub.1164705743",
        "pub.1159936421",
        "pub.1158122423",
        "pub.1160635088",
        "pub.1129913554",
        "pub.1113181804",
        "pub.1118170219",
        "pub.1123988572",
        "pub.1160157307",
        "pub.1163044782",
        "pub.1163044786",
        "pub.1165742225",
        "pub.1099201288",
        "pub.1002534250",
        "pub.1160393491",
        "pub.1037494609",
        "pub.1166872595",
        "pub.1158480508",
        "pub.1165991660",
        "pub.1154881766",
        "pub.1166873851",
        "pub.1110614954",
        "pub.1149547805",
        "pub.1148391030",
        "pub.1155001324",
        "pub.1007117520",
        "pub.1148214275",
        "pub.1125166301",
        "pub.1095838544",
        "pub.1168815727",
        "pub.1061745128",
        "pub.1139125422",
        "pub.1148698453",
        "pub.1121617599",
        "pub.1122068745",
        "pub.1154199627",
        "pub.1099106144",
        "pub.1156141039",
        "pub.1163044715",
        "pub.1129757547",
        "pub.1120882528",
        "pub.1155066898",
        "pub.1156710367",
        "pub.1167961018",
        "pub.1098653272",
        "pub.1103305873",
        "pub.1073690512",
        "pub.1119940081",
        "pub.1122290276",
        "pub.1095838414",
        "pub.1158619011",
        "pub.1155921704",
        "pub.1121799052",
        "pub.1157236098",
        "pub.1157389873",
        "pub.1157532001",
        "pub.1167942381",
        "pub.1121093527",
        "pub.1048759234",
        "pub.1159896323",
        "pub.1051365551",
        "pub.1155607279",
        "pub.1171539554",
        "pub.1139648327",
        "pub.1160578353",
        "pub.1160240948",
        "pub.1038351233",
        "pub.1160647589",
        "pub.1167656070",
        "pub.1099110523",
        "pub.1132403015",
        "pub.1129906535",
        "pub.1148391290",
        "pub.1160103012",
        "pub.1156359853",
        "pub.1156378081",
        "pub.1154717373",
        "pub.1169105905",
        "pub.1163045629",
        "pub.1099113839",
        "pub.1163044693",
        "pub.1157184122",
        "pub.1061744267",
        "pub.1094390291",
        "pub.1095844765",
        "pub.1163329267",
        "pub.1144504926",
        "pub.1166339863",
        "pub.1095852149",
        "pub.1133175774",
        "pub.1139280322",
        "pub.1131335796",
        "pub.1104321292",
        "pub.1163041639",
        "pub.1148391293",
        "pub.1131768885",
        "pub.1036391306",
        "pub.1151332162",
        "pub.1125165396",
        "pub.1059743951",
        "pub.1139691916",
        "pub.1163044809",
        "pub.1041818540",
        "pub.1151103052",
        "pub.1168819309",
        "pub.1163376142",
        "pub.1091368727",
        "pub.1163044709",
        "pub.1168777384",
        "pub.1155301318",
        "pub.1154437519",
        "pub.1120096978",
        "pub.1157706922",
        "pub.1152843639",
        "pub.1039633073",
        "pub.1053798956",
        "pub.1003444679",
        "pub.1154392411",
        "pub.1101207119",
        "pub.1155711946",
        "pub.1170666818",
        "pub.1141942664",
        "pub.1048455035",
        "pub.1157335168",
        "pub.1138207338",
        "pub.1166283217",
        "pub.1153982932",
        "pub.1157409990",
        "pub.1135831396",
        "pub.1112645491",
        "pub.1160000503",
        "pub.1078493593",
        "pub.1131419194",
        "pub.1162861961",
        "pub.1167107024",
        "pub.1046681985",
        "pub.1098653816",
        "pub.1158192766",
        "pub.1158502912",
        "pub.1163044720",
        "pub.1107502744",
        "pub.1125547004",
        "pub.1157731270",
        "pub.1046746008",
        "pub.1073150332",
        "pub.1130206193",
        "pub.1107502607",
        "pub.1163533062",
        "pub.1111460221",
        "pub.1150380957",
        "pub.1168994482",
        "pub.1040096592",
        "pub.1157462412",
        "pub.1024359916",
        "pub.1099140269",
        "pub.1009694875",
        "pub.1111810652"
      ],
      "concepts_scores": [
        {
          "concept": "language model",
          "relevance": 0.615
        },
        {
          "concept": "medical datasets",
          "relevance": 0.533
        },
        {
          "concept": "medical field",
          "relevance": 0.532
        },
        {
          "concept": "language understanding",
          "relevance": 0.529
        },
        {
          "concept": "efficiency of healthcare systems",
          "relevance": 0.515
        },
        {
          "concept": "linguistic model",
          "relevance": 0.501
        },
        {
          "concept": "assisted diagnosis",
          "relevance": 0.5
        },
        {
          "concept": "improve patient care",
          "relevance": 0.485
        },
        {
          "concept": "improve medical practice",
          "relevance": 0.461
        },
        {
          "concept": "ethical issues",
          "relevance": 0.454
        },
        {
          "concept": "patient care",
          "relevance": 0.451
        },
        {
          "concept": "healthcare system",
          "relevance": 0.446
        },
        {
          "concept": "medical practice",
          "relevance": 0.428
        },
        {
          "concept": "healthcare",
          "relevance": 0.427
        },
        {
          "concept": "language",
          "relevance": 0.422
        },
        {
          "concept": "capability",
          "relevance": 0.405
        },
        {
          "concept": "medical research",
          "relevance": 0.401
        },
        {
          "concept": "dataset",
          "relevance": 0.395
        },
        {
          "concept": "architecture",
          "relevance": 0.392
        },
        {
          "concept": "LLM",
          "relevance": 0.381
        },
        {
          "concept": "model",
          "relevance": 0.36
        },
        {
          "concept": "care",
          "relevance": 0.351
        },
        {
          "concept": "domain",
          "relevance": 0.329
        },
        {
          "concept": "applications",
          "relevance": 0.327
        },
        {
          "concept": "system",
          "relevance": 0.316
        },
        {
          "concept": "generation",
          "relevance": 0.314
        },
        {
          "concept": "issues",
          "relevance": 0.312
        },
        {
          "concept": "discussion",
          "relevance": 0.308
        },
        {
          "concept": "understanding",
          "relevance": 0.304
        },
        {
          "concept": "efficiency",
          "relevance": 0.303
        },
        {
          "concept": "practice",
          "relevance": 0.299
        },
        {
          "concept": "field",
          "relevance": 0.289
        },
        {
          "concept": "diagnosis",
          "relevance": 0.285
        },
        {
          "concept": "advances",
          "relevance": 0.284
        },
        {
          "concept": "research",
          "relevance": 0.282
        },
        {
          "concept": "criteria",
          "relevance": 0.279
        },
        {
          "concept": "parameters",
          "relevance": 0.252
        },
        {
          "concept": "trends",
          "relevance": 0.251
        },
        {
          "concept": "limitations",
          "relevance": 0.249
        },
        {
          "concept": "source",
          "relevance": 0.248
        },
        {
          "concept": "size",
          "relevance": 0.225
        },
        {
          "concept": "sector",
          "relevance": 0.217
        },
        {
          "concept": "bloom",
          "relevance": 0.195
        },
        {
          "concept": "potential",
          "relevance": 0.185
        },
        {
          "concept": "GPT",
          "relevance": 0.176
        },
        {
          "concept": "llamas",
          "relevance": 0.158
        }
      ]
    },
    {
      "paperId": "pub.1167961018",
      "doi": "10.1109/iccv51070.2023.00675",
      "title": "Structure and Content-Guided Video Synthesis with Diffusion Models",
      "year": 2023,
      "citationCount": 270,
      "fieldCitationRatio": 175.04,
      "abstract": "Text-guided generative diffusion models unlock powerful image creation and editing tools. Recent approaches that edit the content of footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames. In this work, we present a structure and content-guided video diffusion model that edits videos based on descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. A novel guidance method, enabled by joint video and image training, exposes explicit control over temporal consistency. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.",
      "reference_ids": [
        "pub.1130396558",
        "pub.1152138412",
        "pub.1167961044",
        "pub.1049402063",
        "pub.1152342595",
        "pub.1110720299",
        "pub.1151380649",
        "pub.1151380724",
        "pub.1143791344",
        "pub.1149640930",
        "pub.1017774818",
        "pub.1100060089",
        "pub.1100060291",
        "pub.1151379969",
        "pub.1163454104",
        "pub.1044711914",
        "pub.1095850445",
        "pub.1110721051",
        "pub.1130080350"
      ],
      "concepts_scores": [
        {
          "concept": "monocular depth estimation",
          "relevance": 0.689
        },
        {
          "concept": "joint video",
          "relevance": 0.64
        },
        {
          "concept": "video synthesis",
          "relevance": 0.638
        },
        {
          "concept": "user preferences",
          "relevance": 0.631
        },
        {
          "concept": "content editing",
          "relevance": 0.627
        },
        {
          "concept": "depth estimation",
          "relevance": 0.619
        },
        {
          "concept": "propagation of images",
          "relevance": 0.616
        },
        {
          "concept": "temporal consistency",
          "relevance": 0.608
        },
        {
          "concept": "re-training",
          "relevance": 0.602
        },
        {
          "concept": "structural representation",
          "relevance": 0.594
        },
        {
          "concept": "edited video",
          "relevance": 0.59
        },
        {
          "concept": "image training",
          "relevance": 0.569
        },
        {
          "concept": "image creation",
          "relevance": 0.557
        },
        {
          "concept": "video",
          "relevance": 0.549
        },
        {
          "concept": "content fidelity",
          "relevance": 0.535
        },
        {
          "concept": "editing tools",
          "relevance": 0.518
        },
        {
          "concept": "images",
          "relevance": 0.505
        },
        {
          "concept": "users",
          "relevance": 0.481
        },
        {
          "concept": "training",
          "relevance": 0.472
        },
        {
          "concept": "diffusion model",
          "relevance": 0.471
        },
        {
          "concept": "output characteristics",
          "relevance": 0.466
        },
        {
          "concept": "guidance method",
          "relevance": 0.465
        },
        {
          "concept": "output",
          "relevance": 0.454
        },
        {
          "concept": "representation",
          "relevance": 0.438
        },
        {
          "concept": "model",
          "relevance": 0.423
        },
        {
          "concept": "editing",
          "relevance": 0.421
        },
        {
          "concept": "footage",
          "relevance": 0.421
        },
        {
          "concept": "input",
          "relevance": 0.417
        },
        {
          "concept": "customers",
          "relevance": 0.416
        },
        {
          "concept": "disentanglement",
          "relevance": 0.412
        },
        {
          "concept": "frame",
          "relevance": 0.408
        },
        {
          "concept": "tools",
          "relevance": 0.374
        },
        {
          "concept": "fidelity",
          "relevance": 0.366
        },
        {
          "concept": "method",
          "relevance": 0.362
        },
        {
          "concept": "consistency",
          "relevance": 0.361
        },
        {
          "concept": "creation",
          "relevance": 0.353
        },
        {
          "concept": "solution",
          "relevance": 0.352
        },
        {
          "concept": "estimation",
          "relevance": 0.352
        },
        {
          "concept": "structure",
          "relevance": 0.35
        },
        {
          "concept": "experiments",
          "relevance": 0.343
        },
        {
          "concept": "description",
          "relevance": 0.339
        },
        {
          "concept": "success",
          "relevance": 0.315
        },
        {
          "concept": "preferences",
          "relevance": 0.315
        },
        {
          "concept": "results",
          "relevance": 0.315
        },
        {
          "concept": "content",
          "relevance": 0.312
        },
        {
          "concept": "diffusion",
          "relevance": 0.304
        },
        {
          "concept": "control",
          "relevance": 0.299
        },
        {
          "concept": "guidance",
          "relevance": 0.293
        },
        {
          "concept": "characteristics",
          "relevance": 0.278
        },
        {
          "concept": "conflict",
          "relevance": 0.236
        },
        {
          "concept": "synthesis",
          "relevance": 0.235
        },
        {
          "concept": "levels",
          "relevance": 0.208
        }
      ]
    },
    {
      "paperId": "pub.1017774818",
      "doi": "10.1007/978-3-319-24574-4_28",
      "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "year": 2015,
      "citationCount": 83521,
      "fieldCitationRatio": 18661.69,
      "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
      "reference_ids": [
        "pub.1095686079",
        "pub.1037466020",
        "pub.1018267701",
        "pub.1093828312",
        "pub.1095305380",
        "pub.1008345178",
        "pub.1093626237",
        "pub.1052031051",
        "pub.1079004280"
      ],
      "concepts_scores": [
        {
          "concept": "segmentation of neuronal structures",
          "relevance": 0.637
        },
        {
          "concept": "training of deep networks",
          "relevance": 0.634
        },
        {
          "concept": "trained end-to-end",
          "relevance": 0.634
        },
        {
          "concept": "ISBI Cell Tracking Challenge",
          "relevance": 0.622
        },
        {
          "concept": "annotated training samples",
          "relevance": 0.617
        },
        {
          "concept": "biomedical image segmentation",
          "relevance": 0.61
        },
        {
          "concept": "symmetric expanding path",
          "relevance": 0.607
        },
        {
          "concept": "end-to-end",
          "relevance": 0.604
        },
        {
          "concept": "Cell Tracking Challenge",
          "relevance": 0.599
        },
        {
          "concept": "ISBI challenge",
          "relevance": 0.575
        },
        {
          "concept": "deep networks",
          "relevance": 0.571
        },
        {
          "concept": "annotated samples",
          "relevance": 0.569
        },
        {
          "concept": "convolutional network",
          "relevance": 0.568
        },
        {
          "concept": "data augmentation",
          "relevance": 0.563
        },
        {
          "concept": "training samples",
          "relevance": 0.562
        },
        {
          "concept": "image segmentation",
          "relevance": 0.56
        },
        {
          "concept": "tracking challenges",
          "relevance": 0.56
        },
        {
          "concept": "trained network",
          "relevance": 0.554
        },
        {
          "concept": "contracting path",
          "relevance": 0.55
        },
        {
          "concept": "training strategy",
          "relevance": 0.538
        },
        {
          "concept": "network",
          "relevance": 0.518
        },
        {
          "concept": "ISBI",
          "relevance": 0.509
        },
        {
          "concept": "transmitted light microscopy images",
          "relevance": 0.454
        },
        {
          "concept": "images",
          "relevance": 0.446
        },
        {
          "concept": "GPU",
          "relevance": 0.443
        },
        {
          "concept": "training",
          "relevance": 0.425
        },
        {
          "concept": "segments",
          "relevance": 0.424
        },
        {
          "concept": "architecture",
          "relevance": 0.417
        },
        {
          "concept": "path",
          "relevance": 0.413
        },
        {
          "concept": "light microscopy images",
          "relevance": 0.407
        },
        {
          "concept": "data",
          "relevance": 0.402
        },
        {
          "concept": "microscopy images",
          "relevance": 0.402
        },
        {
          "concept": "implementation",
          "relevance": 0.382
        },
        {
          "concept": "electron",
          "relevance": 0.378
        },
        {
          "concept": "neuronal structures",
          "relevance": 0.373
        },
        {
          "concept": "challenges",
          "relevance": 0.367
        },
        {
          "concept": "Biomedical",
          "relevance": 0.354
        },
        {
          "concept": "augmentation",
          "relevance": 0.333
        },
        {
          "concept": "method",
          "relevance": 0.325
        },
        {
          "concept": "stack",
          "relevance": 0.312
        },
        {
          "concept": "localization",
          "relevance": 0.309
        },
        {
          "concept": "context",
          "relevance": 0.305
        },
        {
          "concept": "categories",
          "relevance": 0.284
        },
        {
          "concept": "strategies",
          "relevance": 0.282
        },
        {
          "concept": "structure",
          "relevance": 0.278
        },
        {
          "concept": "samples",
          "relevance": 0.251
        },
        {
          "concept": "contraction",
          "relevance": 0.202
        },
        {
          "concept": "margin",
          "relevance": 0.201
        },
        {
          "concept": "consent",
          "relevance": 0.191
        }
      ]
    },
    {
      "paperId": "pub.1093626237",
      "doi": "10.1109/cvpr.2015.7298965",
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "year": 2015,
      "citationCount": 32637,
      "fieldCitationRatio": 6415.44,
      "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20]), the VGG net [1], and GoogLeNet [2]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IV on 2012), NYVDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",
      "reference_ids": [
        "pub.1061641212",
        "pub.1061743868",
        "pub.1033986161",
        "pub.1003867987",
        "pub.1094083003",
        "pub.1053469442",
        "pub.1095134254",
        "pub.1024540204",
        "pub.1030406568",
        "pub.1095686079",
        "pub.1008345178",
        "pub.1052031051",
        "pub.1016764525",
        "pub.1094727707",
        "pub.1093843992",
        "pub.1094291017",
        "pub.1032233097",
        "pub.1006936750",
        "pub.1015397249",
        "pub.1003201959",
        "pub.1003742061",
        "pub.1093500653"
      ],
      "concepts_scores": [
        {
          "concept": "convolutional network",
          "relevance": 0.779
        },
        {
          "concept": "semantic segmentation",
          "relevance": 0.726
        },
        {
          "concept": "yield hierarchies of features",
          "relevance": 0.7
        },
        {
          "concept": "state-of-the-art segmentation",
          "relevance": 0.698
        },
        {
          "concept": "trained end-to-end",
          "relevance": 0.697
        },
        {
          "concept": "input of arbitrary size",
          "relevance": 0.696
        },
        {
          "concept": "correspondingly-sized output",
          "relevance": 0.682
        },
        {
          "concept": "dense prediction tasks",
          "relevance": 0.682
        },
        {
          "concept": "hierarchies of features",
          "relevance": 0.677
        },
        {
          "concept": "pixels-to-pixels",
          "relevance": 0.676
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.676
        },
        {
          "concept": "fully convolutional network",
          "relevance": 0.671
        },
        {
          "concept": "end-to-end",
          "relevance": 0.663
        },
        {
          "concept": "PASCAL VOC",
          "relevance": 0.631
        },
        {
          "concept": "learned representations",
          "relevance": 0.629
        },
        {
          "concept": "appearance information",
          "relevance": 0.629
        },
        {
          "concept": "classification network",
          "relevance": 0.626
        },
        {
          "concept": "VGG-Net",
          "relevance": 0.626
        },
        {
          "concept": "SIFT flow",
          "relevance": 0.626
        },
        {
          "concept": "segmentation task",
          "relevance": 0.623
        },
        {
          "concept": "semantic information",
          "relevance": 0.619
        },
        {
          "concept": "prediction task",
          "relevance": 0.615
        },
        {
          "concept": "efficient inference",
          "relevance": 0.614
        },
        {
          "concept": "visual model",
          "relevance": 0.593
        },
        {
          "concept": "network",
          "relevance": 0.569
        },
        {
          "concept": "arbitrary size",
          "relevance": 0.562
        },
        {
          "concept": "task",
          "relevance": 0.513
        },
        {
          "concept": "coarse layer",
          "relevance": 0.495
        },
        {
          "concept": "VGG",
          "relevance": 0.491
        },
        {
          "concept": "inference",
          "relevance": 0.487
        },
        {
          "concept": "semantics",
          "relevance": 0.478
        },
        {
          "concept": "segments",
          "relevance": 0.474
        },
        {
          "concept": "information",
          "relevance": 0.474
        },
        {
          "concept": "SIFT",
          "relevance": 0.473
        },
        {
          "concept": "architecture",
          "relevance": 0.459
        },
        {
          "concept": "classification",
          "relevance": 0.439
        },
        {
          "concept": "learning",
          "relevance": 0.436
        },
        {
          "concept": "representation",
          "relevance": 0.432
        },
        {
          "concept": "nets",
          "relevance": 0.413
        },
        {
          "concept": "input",
          "relevance": 0.412
        },
        {
          "concept": "fine layer",
          "relevance": 0.404
        },
        {
          "concept": "images",
          "relevance": 0.403
        },
        {
          "concept": "Fully",
          "relevance": 0.396
        },
        {
          "concept": "model",
          "relevance": 0.391
        },
        {
          "concept": "features",
          "relevance": 0.391
        },
        {
          "concept": "output",
          "relevance": 0.387
        },
        {
          "concept": "connection",
          "relevance": 0.367
        },
        {
          "concept": "space",
          "relevance": 0.359
        },
        {
          "concept": "layer",
          "relevance": 0.323
        },
        {
          "concept": "VOC",
          "relevance": 0.319
        },
        {
          "concept": "spatially",
          "relevance": 0.318
        },
        {
          "concept": "size",
          "relevance": 0.264
        },
        {
          "concept": "appearance",
          "relevance": 0.253
        },
        {
          "concept": "flow",
          "relevance": 0.208
        }
      ]
    },
    {
      "paperId": "pub.1093828312",
      "doi": "10.1109/iccv.2015.123",
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
      "year": 2015,
      "citationCount": 16259,
      "fieldCitationRatio": 3196.02,
      "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first11reported in Feb. 2015. to surpass the reported human-level performance (5.1%, [26]) on this dataset. reported in Feb. 2015.",
      "reference_ids": [
        "pub.1032233097",
        "pub.1033986161",
        "pub.1033229571",
        "pub.1052031051",
        "pub.1093456265",
        "pub.1093968466",
        "pub.1094727707",
        "pub.1052782426",
        "pub.1095573598",
        "pub.1094291017",
        "pub.1099426737",
        "pub.1094646180",
        "pub.1009767488",
        "pub.1008345178",
        "pub.1014796149",
        "pub.1095689025",
        "pub.1030406568"
      ],
      "concepts_scores": [
        {
          "concept": "human-level performance",
          "relevance": 0.761
        },
        {
          "concept": "neural network",
          "relevance": 0.691
        },
        {
          "concept": "parametric rectified linear unit",
          "relevance": 0.679
        },
        {
          "concept": "robust initialization method",
          "relevance": 0.664
        },
        {
          "concept": "rectified linear unit",
          "relevance": 0.658
        },
        {
          "concept": "ImageNet classification",
          "relevance": 0.616
        },
        {
          "concept": "rectifying nonlinearities",
          "relevance": 0.616
        },
        {
          "concept": "classification datasets",
          "relevance": 0.613
        },
        {
          "concept": "image classification",
          "relevance": 0.607
        },
        {
          "concept": "network architecture",
          "relevance": 0.606
        },
        {
          "concept": "linear unit",
          "relevance": 0.602
        },
        {
          "concept": "overfitting risk",
          "relevance": 0.591
        },
        {
          "concept": "computational cost",
          "relevance": 0.579
        },
        {
          "concept": "initialization method",
          "relevance": 0.578
        },
        {
          "concept": "test error",
          "relevance": 0.562
        },
        {
          "concept": "ImageNet",
          "relevance": 0.558
        },
        {
          "concept": "PReLU",
          "relevance": 0.557
        },
        {
          "concept": "network",
          "relevance": 0.54
        },
        {
          "concept": "classification",
          "relevance": 0.521
        },
        {
          "concept": "dataset",
          "relevance": 0.521
        },
        {
          "concept": "ILSVRC",
          "relevance": 0.483
        },
        {
          "concept": "advanced initiatives",
          "relevance": 0.478
        },
        {
          "concept": "overfitting",
          "relevance": 0.47
        },
        {
          "concept": "performance",
          "relevance": 0.459
        },
        {
          "concept": "Deep",
          "relevance": 0.458
        },
        {
          "concept": "architecture",
          "relevance": 0.447
        },
        {
          "concept": "active units",
          "relevance": 0.413
        },
        {
          "concept": "method",
          "relevance": 0.403
        },
        {
          "concept": "error",
          "relevance": 0.395
        },
        {
          "concept": "images",
          "relevance": 0.393
        },
        {
          "concept": "scratch",
          "relevance": 0.383
        },
        {
          "concept": "model",
          "relevance": 0.381
        },
        {
          "concept": "rectifier",
          "relevance": 0.378
        },
        {
          "concept": "cost",
          "relevance": 0.36
        },
        {
          "concept": "knowledge",
          "relevance": 0.346
        },
        {
          "concept": "parametrization",
          "relevance": 0.319
        },
        {
          "concept": "improvement",
          "relevance": 0.317
        },
        {
          "concept": "nonlinearities",
          "relevance": 0.299
        },
        {
          "concept": "units",
          "relevance": 0.284
        },
        {
          "concept": "test",
          "relevance": 0.247
        },
        {
          "concept": "initiation",
          "relevance": 0.222
        },
        {
          "concept": "risk",
          "relevance": 0.171
        },
        {
          "concept": "activity",
          "relevance": 0.162
        }
      ]
    },
    {
      "paperId": "pub.1095850445",
      "doi": "10.1109/cvpr.2017.632",
      "title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "year": 2017,
      "citationCount": 17290,
      "fieldCitationRatio": 3485.46,
      "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
      "reference_ids": [
        "pub.1036703514",
        "pub.1017774818",
        "pub.1061164348",
        "pub.1026908314",
        "pub.1094854085",
        "pub.1095706293",
        "pub.1093406896",
        "pub.1094706336",
        "pub.1044720211",
        "pub.1016085920",
        "pub.1034474844",
        "pub.1095493673",
        "pub.1027979653",
        "pub.1094856798",
        "pub.1000850552",
        "pub.1000375585",
        "pub.1063151969",
        "pub.1046666876",
        "pub.1093626237",
        "pub.1035996172",
        "pub.1040802208",
        "pub.1094045097",
        "pub.1017576733",
        "pub.1051119970",
        "pub.1015291806",
        "pub.1093907742",
        "pub.1095367750",
        "pub.1004607132",
        "pub.1061640964",
        "pub.1093662716",
        "pub.1023031850",
        "pub.1009767488",
        "pub.1035588163"
      ],
      "concepts_scores": [
        {
          "concept": "conditional adversarial network",
          "relevance": 0.782
        },
        {
          "concept": "adversarial network",
          "relevance": 0.725
        },
        {
          "concept": "input image to output image",
          "relevance": 0.709
        },
        {
          "concept": "loss function",
          "relevance": 0.706
        },
        {
          "concept": "image-to-image translation problem",
          "relevance": 0.704
        },
        {
          "concept": "image-to-image translation",
          "relevance": 0.699
        },
        {
          "concept": "Image-to-image",
          "relevance": 0.678
        },
        {
          "concept": "general-purpose solution",
          "relevance": 0.666
        },
        {
          "concept": "hand-engineering",
          "relevance": 0.635
        },
        {
          "concept": "synthesized photos",
          "relevance": 0.629
        },
        {
          "concept": "hand-engineered",
          "relevance": 0.628
        },
        {
          "concept": "edge map",
          "relevance": 0.626
        },
        {
          "concept": "output image",
          "relevance": 0.623
        },
        {
          "concept": "label maps",
          "relevance": 0.623
        },
        {
          "concept": "color images",
          "relevance": 0.618
        },
        {
          "concept": "reconstructed object",
          "relevance": 0.599
        },
        {
          "concept": "Twitter users",
          "relevance": 0.599
        },
        {
          "concept": "mapping function",
          "relevance": 0.594
        },
        {
          "concept": "loss formulation",
          "relevance": 0.592
        },
        {
          "concept": "translation problems",
          "relevance": 0.561
        },
        {
          "concept": "network",
          "relevance": 0.556
        },
        {
          "concept": "maps",
          "relevance": 0.491
        },
        {
          "concept": "Pix2Pix",
          "relevance": 0.485
        },
        {
          "concept": "users",
          "relevance": 0.477
        },
        {
          "concept": "images",
          "relevance": 0.469
        },
        {
          "concept": "Twitter",
          "relevance": 0.468
        },
        {
          "concept": "software",
          "relevance": 0.454
        },
        {
          "concept": "task",
          "relevance": 0.444
        },
        {
          "concept": "input",
          "relevance": 0.414
        },
        {
          "concept": "labeling",
          "relevance": 0.393
        },
        {
          "concept": "photo",
          "relevance": 0.388
        },
        {
          "concept": "edge",
          "relevance": 0.379
        },
        {
          "concept": "objective",
          "relevance": 0.375
        },
        {
          "concept": "system",
          "relevance": 0.371
        },
        {
          "concept": "function",
          "relevance": 0.362
        },
        {
          "concept": "solution",
          "relevance": 0.349
        },
        {
          "concept": "color",
          "relevance": 0.345
        },
        {
          "concept": "experiments",
          "relevance": 0.34
        },
        {
          "concept": "translation",
          "relevance": 0.335
        },
        {
          "concept": "results",
          "relevance": 0.312
        },
        {
          "concept": "loss",
          "relevance": 0.285
        },
        {
          "concept": "formulation",
          "relevance": 0.28
        },
        {
          "concept": "artistic experience",
          "relevance": 0.269
        },
        {
          "concept": "community",
          "relevance": 0.263
        },
        {
          "concept": "conditions",
          "relevance": 0.206
        },
        {
          "concept": "problem",
          "relevance": 0.155
        },
        {
          "concept": "release",
          "relevance": 0.153
        }
      ]
    },
    {
      "paperId": "pub.1061640964",
      "doi": "10.1109/tip.2003.819861",
      "title": "Image Quality Assessment: From Error Visibility to Structural Similarity",
      "year": 2004,
      "citationCount": 44667,
      "fieldCitationRatio": 8842.79,
      "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.",
      "reference_ids": [
        "pub.1061098596",
        "pub.1109699835",
        "pub.1051590917",
        "pub.1061157146",
        "pub.1038074861",
        "pub.1110622471",
        "pub.1042918725",
        "pub.1002450358",
        "pub.1065157151",
        "pub.1061251565",
        "pub.1021151071",
        "pub.1094280748",
        "pub.1010524390",
        "pub.1007353974",
        "pub.1093619469",
        "pub.1061647489",
        "pub.1065210132",
        "pub.1061228473",
        "pub.1061222004",
        "pub.1011624274",
        "pub.1088779547",
        "pub.1005576318",
        "pub.1014836974",
        "pub.1095102111",
        "pub.1016979910",
        "pub.1065158325",
        "pub.1052170656",
        "pub.1065159598",
        "pub.1037873940",
        "pub.1037099914",
        "pub.1020793995",
        "pub.1004119149",
        "pub.1009205613",
        "pub.1061137112",
        "pub.1026562287",
        "pub.1061240373",
        "pub.1094779682",
        "pub.1061240018",
        "pub.1093467274",
        "pub.1061239913",
        "pub.1006831752",
        "pub.1061240395",
        "pub.1003534189",
        "pub.1010313466",
        "pub.1032128907",
        "pub.1061239618",
        "pub.1095644828",
        "pub.1024091291",
        "pub.1020705229"
      ],
      "concepts_scores": [
        {
          "concept": "degradation of structural information",
          "relevance": 0.712
        },
        {
          "concept": "perceptual image quality",
          "relevance": 0.689
        },
        {
          "concept": "visibility of errors",
          "relevance": 0.683
        },
        {
          "concept": "structural similarity index",
          "relevance": 0.683
        },
        {
          "concept": "human visual perception",
          "relevance": 0.681
        },
        {
          "concept": "database of images",
          "relevance": 0.68
        },
        {
          "concept": "human visual system",
          "relevance": 0.679
        },
        {
          "concept": "error visibility",
          "relevance": 0.63
        },
        {
          "concept": "distorted images",
          "relevance": 0.62
        },
        {
          "concept": "structural information",
          "relevance": 0.608
        },
        {
          "concept": "visual system",
          "relevance": 0.579
        },
        {
          "concept": "similarity index",
          "relevance": 0.577
        },
        {
          "concept": "image quality",
          "relevance": 0.57
        },
        {
          "concept": "visual perception",
          "relevance": 0.564
        },
        {
          "concept": "quality assessment",
          "relevance": 0.519
        },
        {
          "concept": "JPEG2000",
          "relevance": 0.505
        },
        {
          "concept": "JPEG",
          "relevance": 0.503
        },
        {
          "concept": "structural similarity",
          "relevance": 0.499
        },
        {
          "concept": "images",
          "relevance": 0.499
        },
        {
          "concept": "information",
          "relevance": 0.482
        },
        {
          "concept": "error",
          "relevance": 0.477
        },
        {
          "concept": "scene",
          "relevance": 0.473
        },
        {
          "concept": "visibility",
          "relevance": 0.462
        },
        {
          "concept": "framework",
          "relevance": 0.415
        },
        {
          "concept": "quality",
          "relevance": 0.409
        },
        {
          "concept": "database",
          "relevance": 0.402
        },
        {
          "concept": "complementary framework",
          "relevance": 0.401
        },
        {
          "concept": "examples",
          "relevance": 0.382
        },
        {
          "concept": "similarity",
          "relevance": 0.376
        },
        {
          "concept": "system",
          "relevance": 0.376
        },
        {
          "concept": "method",
          "relevance": 0.363
        },
        {
          "concept": "concept",
          "relevance": 0.361
        },
        {
          "concept": "properties",
          "relevance": 0.329
        },
        {
          "concept": "degradation",
          "relevance": 0.322
        },
        {
          "concept": "structure",
          "relevance": 0.302
        },
        {
          "concept": "perception",
          "relevance": 0.294
        },
        {
          "concept": "comparison",
          "relevance": 0.283
        },
        {
          "concept": "rate",
          "relevance": 0.257
        },
        {
          "concept": "index",
          "relevance": 0.232
        },
        {
          "concept": "assessment",
          "relevance": 0.225
        },
        {
          "concept": "differences",
          "relevance": 0.153
        }
      ]
    },
    {
      "paperId": "pub.1167107024",
      "doi": "10.1016/s2589-7500(23)00225-x",
      "title": "Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study",
      "year": 2024,
      "citationCount": 269,
      "fieldCitationRatio": NaN,
      "abstract": "BACKGROUND: Large language models (LLMs) such as GPT-4 hold great promise as transformative tools in health care, ranging from automating administrative tasks to augmenting clinical decision making. However, these models also pose a danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care. We aimed to assess whether GPT-4 encodes racial and gender biases that impact its use in health care.\nMETHODS: Using the Azure OpenAI application interface, this model evaluation study tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain-namely, medical education, diagnostic reasoning, clinical plan generation, and subjective patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in health care. GPT-4 estimates of the demographic distribution of medical conditions were compared with true US prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups.\nFINDINGS: We found that GPT-4 did not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardised clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and genders. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception.\nINTERPRETATION: Our findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools such as GPT-4 for intended use cases before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies before clinical implementation.\nFUNDING: Priscilla Chan and Mark Zuckerberg.",
      "reference_ids": [
        "pub.1131796937",
        "pub.1155844504",
        "pub.1102193513",
        "pub.1163043631",
        "pub.1157982273",
        "pub.1019676655",
        "pub.1125494725",
        "pub.1128159671",
        "pub.1125798967",
        "pub.1035677984",
        "pub.1092675337",
        "pub.1148391035",
        "pub.1026244738",
        "pub.1149741329",
        "pub.1037355815",
        "pub.1093083832",
        "pub.1160766715",
        "pub.1156371110",
        "pub.1156602823",
        "pub.1099927333",
        "pub.1146917543",
        "pub.1154268382",
        "pub.1152952399",
        "pub.1111494982",
        "pub.1155035437",
        "pub.1150828835",
        "pub.1138942250",
        "pub.1157705785",
        "pub.1143980655",
        "pub.1158819277",
        "pub.1068844823",
        "pub.1155270525",
        "pub.1139947461",
        "pub.1043410507",
        "pub.1163045663",
        "pub.1103057519",
        "pub.1159832465",
        "pub.1135710434"
      ],
      "concepts_scores": [
        {
          "concept": "health care",
          "relevance": 0.681
        },
        {
          "concept": "diversity of medical conditions",
          "relevance": 0.623
        },
        {
          "concept": "clinical vignettes",
          "relevance": 0.616
        },
        {
          "concept": "medical conditions",
          "relevance": 0.607
        },
        {
          "concept": "evaluation studies",
          "relevance": 0.569
        },
        {
          "concept": "clinical decision making",
          "relevance": 0.559
        },
        {
          "concept": "patients' perceptions",
          "relevance": 0.549
        },
        {
          "concept": "clinical care",
          "relevance": 0.541
        },
        {
          "concept": "medical care",
          "relevance": 0.539
        },
        {
          "concept": "US prevalence",
          "relevance": 0.538
        },
        {
          "concept": "bias assessment",
          "relevance": 0.537
        },
        {
          "concept": "medical education",
          "relevance": 0.535
        },
        {
          "concept": "patient assessment",
          "relevance": 0.534
        },
        {
          "concept": "care",
          "relevance": 0.531
        },
        {
          "concept": "health",
          "relevance": 0.516
        },
        {
          "concept": "demographic groups",
          "relevance": 0.502
        },
        {
          "concept": "administrative tasks",
          "relevance": 0.493
        },
        {
          "concept": "implicit bias",
          "relevance": 0.493
        },
        {
          "concept": "standard statistical tests",
          "relevance": 0.484
        },
        {
          "concept": "demographic distribution",
          "relevance": 0.481
        },
        {
          "concept": "gender bias",
          "relevance": 0.469
        },
        {
          "concept": "vignettes",
          "relevance": 0.469
        },
        {
          "concept": "published research",
          "relevance": 0.465
        },
        {
          "concept": "diagnostic reasoning",
          "relevance": 0.462
        },
        {
          "concept": "gender",
          "relevance": 0.461
        },
        {
          "concept": "medical diagnosis",
          "relevance": 0.461
        },
        {
          "concept": "medical education applications",
          "relevance": 0.454
        },
        {
          "concept": "incorrect medical diagnosis",
          "relevance": 0.452
        },
        {
          "concept": "decision making",
          "relevance": 0.442
        },
        {
          "concept": "clinical implementation",
          "relevance": 0.441
        },
        {
          "concept": "statistical tests",
          "relevance": 0.44
        },
        {
          "concept": "assessment",
          "relevance": 0.432
        },
        {
          "concept": "diagnosis",
          "relevance": 0.428
        },
        {
          "concept": "treatment planning",
          "relevance": 0.428
        },
        {
          "concept": "language model",
          "relevance": 0.421
        },
        {
          "concept": "application interface",
          "relevance": 0.418
        },
        {
          "concept": "domain-names",
          "relevance": 0.414
        },
        {
          "concept": "demographic attributes",
          "relevance": 0.414
        },
        {
          "concept": "demographic presentation",
          "relevance": 0.413
        },
        {
          "concept": "patients",
          "relevance": 0.406
        },
        {
          "concept": "healers",
          "relevance": 0.406
        },
        {
          "concept": "bias",
          "relevance": 0.404
        },
        {
          "concept": "transformation tools",
          "relevance": 0.401
        },
        {
          "concept": "ethnicity",
          "relevance": 0.401
        },
        {
          "concept": "plan generation",
          "relevance": 0.4
        },
        {
          "concept": "prevalence",
          "relevance": 0.398
        },
        {
          "concept": "differential diagnosis",
          "relevance": 0.398
        },
        {
          "concept": "education",
          "relevance": 0.382
        },
        {
          "concept": "association",
          "relevance": 0.382
        },
        {
          "concept": "group",
          "relevance": 0.379
        },
        {
          "concept": "planning",
          "relevance": 0.377
        },
        {
          "concept": "educational applications",
          "relevance": 0.375
        },
        {
          "concept": "NEJM",
          "relevance": 0.375
        },
        {
          "concept": "race",
          "relevance": 0.374
        },
        {
          "concept": "recommendations",
          "relevance": 0.373
        },
        {
          "concept": "demographic diversity",
          "relevance": 0.367
        },
        {
          "concept": "perception",
          "relevance": 0.365
        },
        {
          "concept": "prompts",
          "relevance": 0.361
        },
        {
          "concept": "study",
          "relevance": 0.355
        },
        {
          "concept": "impact",
          "relevance": 0.351
        },
        {
          "concept": "tools",
          "relevance": 0.34
        },
        {
          "concept": "making",
          "relevance": 0.338
        },
        {
          "concept": "potential sources",
          "relevance": 0.333
        },
        {
          "concept": "potential mitigation strategies",
          "relevance": 0.33
        },
        {
          "concept": "stereotypes",
          "relevance": 0.321
        },
        {
          "concept": "implementation",
          "relevance": 0.317
        },
        {
          "concept": "task",
          "relevance": 0.315
        },
        {
          "concept": "findings",
          "relevance": 0.313
        },
        {
          "concept": "presentation",
          "relevance": 0.312
        },
        {
          "concept": "Azure",
          "relevance": 0.308
        },
        {
          "concept": "implicit",
          "relevance": 0.302
        },
        {
          "concept": "test",
          "relevance": 0.302
        },
        {
          "concept": "language",
          "relevance": 0.301
        },
        {
          "concept": "reasons",
          "relevance": 0.301
        },
        {
          "concept": "strategies",
          "relevance": 0.301
        },
        {
          "concept": "LLM",
          "relevance": 0.297
        },
        {
          "concept": "research",
          "relevance": 0.296
        },
        {
          "concept": "mitigation strategies",
          "relevance": 0.292
        },
        {
          "concept": "treatment",
          "relevance": 0.289
        },
        {
          "concept": "model",
          "relevance": 0.285
        },
        {
          "concept": "significance",
          "relevance": 0.281
        },
        {
          "concept": "danger",
          "relevance": 0.267
        },
        {
          "concept": "attributes",
          "relevance": 0.266
        },
        {
          "concept": "conditions",
          "relevance": 0.262
        },
        {
          "concept": "applications",
          "relevance": 0.259
        },
        {
          "concept": "interface",
          "relevance": 0.255
        },
        {
          "concept": "cases",
          "relevance": 0.248
        },
        {
          "concept": "procedure",
          "relevance": 0.245
        },
        {
          "concept": "potential applications",
          "relevance": 0.238
        },
        {
          "concept": "experiments",
          "relevance": 0.234
        },
        {
          "concept": "generation",
          "relevance": 0.214
        },
        {
          "concept": "model evaluation studies",
          "relevance": 0.203
        },
        {
          "concept": "source",
          "relevance": 0.196
        },
        {
          "concept": "potential",
          "relevance": 0.194
        }
      ]
    },
    {
      "paperId": "pub.1155270525",
      "doi": "10.1371/journal.pdig.0000198",
      "title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models",
      "year": 2023,
      "citationCount": 2693,
      "fieldCitationRatio": 2145.24,
      "abstract": "We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.",
      "reference_ids": [
        "pub.1121025018",
        "pub.1100900092",
        "pub.1130542575",
        "pub.1117041738",
        "pub.1117939920",
        "pub.1136656239",
        "pub.1092352256",
        "pub.1127685771",
        "pub.1121129078",
        "pub.1045049575",
        "pub.1151451023",
        "pub.1113525484",
        "pub.1126027253",
        "pub.1139648327",
        "pub.1116881202",
        "pub.1169290908",
        "pub.1112585067",
        "pub.1033178586",
        "pub.1078412683",
        "pub.1129260878",
        "pub.1147955703",
        "pub.1093497718"
      ],
      "concepts_scores": [
        {
          "concept": "language model",
          "relevance": 0.682
        },
        {
          "concept": "ChatGPT",
          "relevance": 0.546
        },
        {
          "concept": "United States Medical Licensing Exam",
          "relevance": 0.536
        },
        {
          "concept": "medical education",
          "relevance": 0.476
        },
        {
          "concept": "language",
          "relevance": 0.445
        },
        {
          "concept": "clinical decision-making",
          "relevance": 0.428
        },
        {
          "concept": "Medical Licensing Exam",
          "relevance": 0.427
        },
        {
          "concept": "performance",
          "relevance": 0.424
        },
        {
          "concept": "decision-making",
          "relevance": 0.412
        },
        {
          "concept": "level of concordance",
          "relevance": 0.405
        },
        {
          "concept": "licensing exam",
          "relevance": 0.394
        },
        {
          "concept": "specialized training",
          "relevance": 0.392
        },
        {
          "concept": "model",
          "relevance": 0.37
        },
        {
          "concept": "training",
          "relevance": 0.363
        },
        {
          "concept": "education",
          "relevance": 0.34
        },
        {
          "concept": "exam",
          "relevance": 0.339
        },
        {
          "concept": "reinforcement",
          "relevance": 0.324
        },
        {
          "concept": "threshold",
          "relevance": 0.296
        },
        {
          "concept": "results",
          "relevance": 0.28
        },
        {
          "concept": "concordance",
          "relevance": 0.263
        },
        {
          "concept": "units",
          "relevance": 0.247
        },
        {
          "concept": "levels",
          "relevance": 0.216
        },
        {
          "concept": "potential",
          "relevance": 0.196
        }
      ]
    },
    {
      "paperId": "pub.1093497718",
      "doi": "10.1109/cvpr.2016.308",
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "year": 2016,
      "citationCount": 26343,
      "fieldCitationRatio": 5148.56,
      "abstract": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error and 17.3% top-1 error on the validation set and 3.6% top-5 error on the official test set.",
      "reference_ids": [
        "pub.1094727707",
        "pub.1095510970",
        "pub.1037471929",
        "pub.1061218465",
        "pub.1094291017",
        "pub.1095072940",
        "pub.1093828312",
        "pub.1022837810",
        "pub.1085642448",
        "pub.1093626237",
        "pub.1095738839",
        "pub.1094869203"
      ],
      "concepts_scores": [
        {
          "concept": "convolutional network",
          "relevance": 0.733
        },
        {
          "concept": "computational cost",
          "relevance": 0.697
        },
        {
          "concept": "top-1 error",
          "relevance": 0.691
        },
        {
          "concept": "computer vision solutions",
          "relevance": 0.688
        },
        {
          "concept": "Big Data scenarios",
          "relevance": 0.687
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.685
        },
        {
          "concept": "deep convolutional networks",
          "relevance": 0.683
        },
        {
          "concept": "increasing model size",
          "relevance": 0.663
        },
        {
          "concept": "low parameter count",
          "relevance": 0.661
        },
        {
          "concept": "computer vision",
          "relevance": 0.635
        },
        {
          "concept": "top-1",
          "relevance": 0.635
        },
        {
          "concept": "Inception architecture",
          "relevance": 0.631
        },
        {
          "concept": "vision solutions",
          "relevance": 0.629
        },
        {
          "concept": "factorized convolution",
          "relevance": 0.626
        },
        {
          "concept": "parameter count",
          "relevance": 0.623
        },
        {
          "concept": "model size",
          "relevance": 0.618
        },
        {
          "concept": "mobile vision",
          "relevance": 0.611
        },
        {
          "concept": "quality gains",
          "relevance": 0.61
        },
        {
          "concept": "multiply-add",
          "relevance": 0.595
        },
        {
          "concept": "computational efficiency",
          "relevance": 0.574
        },
        {
          "concept": "network",
          "relevance": 0.572
        },
        {
          "concept": "computer",
          "relevance": 0.534
        },
        {
          "concept": "task",
          "relevance": 0.52
        },
        {
          "concept": "error",
          "relevance": 0.508
        },
        {
          "concept": "ILSVRC",
          "relevance": 0.502
        },
        {
          "concept": "vision",
          "relevance": 0.49
        },
        {
          "concept": "validation set",
          "relevance": 0.486
        },
        {
          "concept": "convolution",
          "relevance": 0.481
        },
        {
          "concept": "architecture",
          "relevance": 0.465
        },
        {
          "concept": "benchmarks",
          "relevance": 0.457
        },
        {
          "concept": "classification",
          "relevance": 0.445
        },
        {
          "concept": "cost",
          "relevance": 0.434
        },
        {
          "concept": "scenarios",
          "relevance": 0.429
        },
        {
          "concept": "inference",
          "relevance": 0.427
        },
        {
          "concept": "regularization",
          "relevance": 0.421
        },
        {
          "concept": "evaluation",
          "relevance": 0.42
        },
        {
          "concept": "ensemble",
          "relevance": 0.401
        },
        {
          "concept": "official tests",
          "relevance": 0.387
        },
        {
          "concept": "sets",
          "relevance": 0.373
        },
        {
          "concept": "method",
          "relevance": 0.362
        },
        {
          "concept": "efficiency",
          "relevance": 0.359
        },
        {
          "concept": "solution",
          "relevance": 0.353
        },
        {
          "concept": "gain",
          "relevance": 0.347
        },
        {
          "concept": "model",
          "relevance": 0.343
        },
        {
          "concept": "validity",
          "relevance": 0.333
        },
        {
          "concept": "mainstream",
          "relevance": 0.308
        },
        {
          "concept": "core",
          "relevance": 0.302
        },
        {
          "concept": "parameters",
          "relevance": 0.299
        },
        {
          "concept": "inception",
          "relevance": 0.267
        },
        {
          "concept": "size",
          "relevance": 0.267
        },
        {
          "concept": "cases",
          "relevance": 0.261
        },
        {
          "concept": "test",
          "relevance": 0.257
        },
        {
          "concept": "state",
          "relevance": 0.226
        },
        {
          "concept": "count",
          "relevance": 0.204
        },
        {
          "concept": "factors",
          "relevance": 0.193
        }
      ]
    },
    {
      "paperId": "pub.1127685771",
      "doi": "10.1038/s41591-020-0842-3",
      "title": "A deep learning system for differential diagnosis of skin diseases",
      "year": 2020,
      "citationCount": 616,
      "fieldCitationRatio": 175.6,
      "abstract": "Skin conditions affect 1.9 billion people. Because of a shortage of dermatologists, most cases are seen instead by general practitioners with lower diagnostic accuracy. We present a deep learning system (DLS) to provide a differential diagnosis of skin conditions using 16,114 de-identified cases (photographs and clinical data) from a teledermatology practice serving 17 sites. The DLS distinguishes between 26 common skin conditions, representing 80% of cases seen in primary care, while also providing a secondary prediction covering 419 skin conditions. On 963 validation cases, where a rotating panel of three board-certified dermatologists defined the reference standard, the DLS was non-inferior to six other dermatologists and superior to six primary care physicians (PCPs) and six nurse practitioners (NPs) (top-1 accuracy: 0.66 DLS, 0.63 dermatologists, 0.44 PCPs and 0.40 NPs). These results highlight the potential of the DLS to assist general practitioners in diagnosing skin conditions.",
      "reference_ids": [
        "pub.1009634351",
        "pub.1077793311",
        "pub.1106835128",
        "pub.1112466976",
        "pub.1104293396",
        "pub.1052314382",
        "pub.1009767488",
        "pub.1120312910",
        "pub.1084948468",
        "pub.1091080781",
        "pub.1021839880",
        "pub.1019018231",
        "pub.1084246301",
        "pub.1100565524",
        "pub.1074344209",
        "pub.1045074199",
        "pub.1051728167",
        "pub.1003788636",
        "pub.1100484852",
        "pub.1121783884",
        "pub.1105579486",
        "pub.1039230587",
        "pub.1000798719",
        "pub.1105977295",
        "pub.1103818178",
        "pub.1034543347",
        "pub.1044058999",
        "pub.1054104140",
        "pub.1084005844",
        "pub.1021955972",
        "pub.1031248877",
        "pub.1126584795",
        "pub.1059744049",
        "pub.1003052029",
        "pub.1007815996",
        "pub.1032664445",
        "pub.1148955875",
        "pub.1043162503",
        "pub.1074217286",
        "pub.1113359490",
        "pub.1046704911",
        "pub.1117069073",
        "pub.1043265979",
        "pub.1110720271",
        "pub.1008508369",
        "pub.1079025257"
      ],
      "concepts_scores": [
        {
          "concept": "primary care physicians",
          "relevance": 0.645
        },
        {
          "concept": "nurse practitioners",
          "relevance": 0.604
        },
        {
          "concept": "general practitioners",
          "relevance": 0.598
        },
        {
          "concept": "skin conditions",
          "relevance": 0.579
        },
        {
          "concept": "differential diagnosis",
          "relevance": 0.571
        },
        {
          "concept": "differential diagnosis of skin diseases",
          "relevance": 0.547
        },
        {
          "concept": "shortage of dermatologists",
          "relevance": 0.546
        },
        {
          "concept": "primary care",
          "relevance": 0.522
        },
        {
          "concept": "board-certified dermatologists",
          "relevance": 0.521
        },
        {
          "concept": "care physicians",
          "relevance": 0.516
        },
        {
          "concept": "diagnosis of skin conditions",
          "relevance": 0.508
        },
        {
          "concept": "deep learning system",
          "relevance": 0.506
        },
        {
          "concept": "diagnosing skin conditions",
          "relevance": 0.503
        },
        {
          "concept": "diagnostic accuracy",
          "relevance": 0.489
        },
        {
          "concept": "non-inferiority",
          "relevance": 0.488
        },
        {
          "concept": "teledermatology practice",
          "relevance": 0.48
        },
        {
          "concept": "reference standard",
          "relevance": 0.471
        },
        {
          "concept": "skin diseases",
          "relevance": 0.464
        },
        {
          "concept": "diagnosis of skin diseases",
          "relevance": 0.458
        },
        {
          "concept": "dermatologists",
          "relevance": 0.453
        },
        {
          "concept": "practitioners",
          "relevance": 0.444
        },
        {
          "concept": "skin",
          "relevance": 0.432
        },
        {
          "concept": "learning system",
          "relevance": 0.427
        },
        {
          "concept": "nurses",
          "relevance": 0.408
        },
        {
          "concept": "care",
          "relevance": 0.396
        },
        {
          "concept": "teledermatology",
          "relevance": 0.39
        },
        {
          "concept": "physicians",
          "relevance": 0.389
        },
        {
          "concept": "cases",
          "relevance": 0.376
        },
        {
          "concept": "billion people",
          "relevance": 0.369
        },
        {
          "concept": "disease",
          "relevance": 0.364
        },
        {
          "concept": "secondary prediction",
          "relevance": 0.352
        },
        {
          "concept": "people",
          "relevance": 0.342
        },
        {
          "concept": "practice",
          "relevance": 0.336
        },
        {
          "concept": "rotating panel",
          "relevance": 0.334
        },
        {
          "concept": "shortage",
          "relevance": 0.323
        },
        {
          "concept": "validity",
          "relevance": 0.295
        },
        {
          "concept": "billion",
          "relevance": 0.275
        },
        {
          "concept": "validation cases",
          "relevance": 0.273
        },
        {
          "concept": "standards",
          "relevance": 0.267
        },
        {
          "concept": "sites",
          "relevance": 0.266
        },
        {
          "concept": "conditions",
          "relevance": 0.263
        },
        {
          "concept": "system",
          "relevance": 0.262
        },
        {
          "concept": "accuracy",
          "relevance": 0.258
        },
        {
          "concept": "panel",
          "relevance": 0.258
        },
        {
          "concept": "potential",
          "relevance": 0.252
        },
        {
          "concept": "reference",
          "relevance": 0.242
        },
        {
          "concept": "results",
          "relevance": 0.22
        },
        {
          "concept": "prediction",
          "relevance": 0.213
        }
      ]
    },
    {
      "paperId": "pub.1155844504",
      "doi": "10.3322/caac.21772",
      "title": "Colorectal cancer statistics, 2023",
      "year": 2023,
      "citationCount": 2144,
      "fieldCitationRatio": 1224.49,
      "abstract": "Colorectal cancer (CRC) is the second most common cause of cancer death in the United States. Every 3 years, the American Cancer Society provides an update of CRC statistics based on incidence from population-based cancer registries and mortality from the National Center for Health Statistics. In 2023, approximately 153,020 individuals will be diagnosed with CRC and 52,550 will die from the disease, including 19,550 cases and 3750 deaths in individuals younger than 50 years. The decline in CRC incidence slowed from 3%-4% annually during the 2000s to 1% annually during 2011-2019, driven partly by an increase in individuals younger than 55 years of 1%-2% annually since the mid-1990s. Consequently, the proportion of cases among those younger than 55 years increased from 11% in 1995 to 20% in 2019. Incidence since circa 2010 increased in those younger than 65 years for regional-stage disease by about 2%-3% annually and for distant-stage disease by 0.5%-3% annually, reversing the overall shift to earlier stage diagnosis that occurred during 1995 through 2005. For example, 60% of all new cases were advanced in 2019 versus 52% in the mid-2000s and 57% in 1995, before widespread screening. There is also a shift to left-sided tumors, with the proportion of rectal cancer increasing from 27% in 1995 to 31% in 2019. CRC mortality declined by 2% annually from 2011-2020 overall but increased by 0.5%-3% annually in individuals younger than 50 years and in Native Americans younger than 65 years. In summary, despite continued overall declines, CRC is rapidly shifting to diagnosis at a younger age, at a more advanced stage, and in the left colon/rectum. Progress against CRC could be accelerated by uncovering the etiology of rising incidence in generations born since 1950 and increasing access to high-quality screening and treatment among all populations, especially Native Americans.",
      "reference_ids": [
        "pub.1009243015",
        "pub.1107431890",
        "pub.1120441272",
        "pub.1139863587",
        "pub.1009875825",
        "pub.1152573761",
        "pub.1010387769",
        "pub.1114503782",
        "pub.1150401054",
        "pub.1012877974",
        "pub.1016975294",
        "pub.1002808712",
        "pub.1033880503",
        "pub.1092692269",
        "pub.1139627505",
        "pub.1147627661",
        "pub.1042615284",
        "pub.1137345683",
        "pub.1151917889",
        "pub.1021716314",
        "pub.1027737459",
        "pub.1031793963",
        "pub.1139043786",
        "pub.1032310654",
        "pub.1135369245",
        "pub.1040428642",
        "pub.1083932849",
        "pub.1137805685",
        "pub.1153204671",
        "pub.1008331133",
        "pub.1149508739",
        "pub.1104290717",
        "pub.1143972339",
        "pub.1026682634",
        "pub.1035693028",
        "pub.1121255686",
        "pub.1016499409",
        "pub.1084521549",
        "pub.1008963276",
        "pub.1013380402",
        "pub.1153449961",
        "pub.1143452654",
        "pub.1010133804",
        "pub.1033182204",
        "pub.1134182446",
        "pub.1044276858",
        "pub.1032688576",
        "pub.1017117182",
        "pub.1034555697",
        "pub.1047235138",
        "pub.1101315766",
        "pub.1019654552",
        "pub.1120874443",
        "pub.1032384183",
        "pub.1029562127",
        "pub.1139068035",
        "pub.1001173514",
        "pub.1007288835",
        "pub.1077912113",
        "pub.1134999563",
        "pub.1013507168",
        "pub.1100348075",
        "pub.1104042286",
        "pub.1016683971",
        "pub.1090877805",
        "pub.1112676709",
        "pub.1135280966",
        "pub.1023110676",
        "pub.1092829901",
        "pub.1035635231",
        "pub.1023529189",
        "pub.1150676848",
        "pub.1078573568",
        "pub.1027056811",
        "pub.1071273728",
        "pub.1107668674",
        "pub.1050806968",
        "pub.1123144955",
        "pub.1046334485",
        "pub.1138104031",
        "pub.1152915382",
        "pub.1005650232",
        "pub.1107120970",
        "pub.1141727326",
        "pub.1142498968",
        "pub.1039764056",
        "pub.1008811062",
        "pub.1068846841",
        "pub.1139752207",
        "pub.1029185920",
        "pub.1151960236",
        "pub.1148900016",
        "pub.1148914263",
        "pub.1028125401",
        "pub.1062814354",
        "pub.1104290082",
        "pub.1039939815",
        "pub.1148404649",
        "pub.1085470454",
        "pub.1147882619",
        "pub.1027785064",
        "pub.1001093648",
        "pub.1085615702",
        "pub.1047164847",
        "pub.1140315737",
        "pub.1084109249",
        "pub.1059821874",
        "pub.1033426756",
        "pub.1146510796",
        "pub.1059818721",
        "pub.1002394888",
        "pub.1123795024",
        "pub.1134558207",
        "pub.1078641090",
        "pub.1039373975",
        "pub.1012487722",
        "pub.1140475538",
        "pub.1150067827",
        "pub.1138102811",
        "pub.1122254519",
        "pub.1133399990",
        "pub.1140133594",
        "pub.1131547923",
        "pub.1092239851",
        "pub.1151063426",
        "pub.1145786878",
        "pub.1050993858",
        "pub.1031961740",
        "pub.1137501347"
      ],
      "concepts_scores": [
        {
          "concept": "colorectal cancer",
          "relevance": 0.684
        },
        {
          "concept": "population-based cancer registries",
          "relevance": 0.664
        },
        {
          "concept": "proportion of rectal cancer",
          "relevance": 0.664
        },
        {
          "concept": "National Center for Health Statistics",
          "relevance": 0.659
        },
        {
          "concept": "Center for Health Statistics",
          "relevance": 0.653
        },
        {
          "concept": "colorectal cancer mortality",
          "relevance": 0.651
        },
        {
          "concept": "colorectal cancer incidence",
          "relevance": 0.65
        },
        {
          "concept": "distant-stage disease",
          "relevance": 0.644
        },
        {
          "concept": "regional-stage disease",
          "relevance": 0.637
        },
        {
          "concept": "American Cancer Society",
          "relevance": 0.636
        },
        {
          "concept": "Cancer Registry",
          "relevance": 0.596
        },
        {
          "concept": "Health Statistics",
          "relevance": 0.588
        },
        {
          "concept": "cause of cancer death",
          "relevance": 0.585
        },
        {
          "concept": "Cancer Society",
          "relevance": 0.585
        },
        {
          "concept": "Native Americans",
          "relevance": 0.576
        },
        {
          "concept": "increase access",
          "relevance": 0.562
        },
        {
          "concept": "widespread screening",
          "relevance": 0.559
        },
        {
          "concept": "cancer death",
          "relevance": 0.558
        },
        {
          "concept": "proportion of cases",
          "relevance": 0.551
        },
        {
          "concept": "stage diagnosis",
          "relevance": 0.551
        },
        {
          "concept": "younger age",
          "relevance": 0.539
        },
        {
          "concept": "early stage diagnosis",
          "relevance": 0.521
        },
        {
          "concept": "left-sided tumors",
          "relevance": 0.518
        },
        {
          "concept": "American",
          "relevance": 0.504
        },
        {
          "concept": "United States",
          "relevance": 0.486
        },
        {
          "concept": "rectal cancer",
          "relevance": 0.486
        },
        {
          "concept": "left colon/rectum",
          "relevance": 0.485
        },
        {
          "concept": "individuals",
          "relevance": 0.481
        },
        {
          "concept": "incidence",
          "relevance": 0.469
        },
        {
          "concept": "mortality",
          "relevance": 0.468
        },
        {
          "concept": "years",
          "relevance": 0.467
        },
        {
          "concept": "screening",
          "relevance": 0.466
        },
        {
          "concept": "advanced stage",
          "relevance": 0.458
        },
        {
          "concept": "colon/rectum",
          "relevance": 0.451
        },
        {
          "concept": "registry",
          "relevance": 0.445
        },
        {
          "concept": "cancer",
          "relevance": 0.439
        },
        {
          "concept": "colorectal",
          "relevance": 0.438
        },
        {
          "concept": "death",
          "relevance": 0.438
        },
        {
          "concept": "statistically",
          "relevance": 0.438
        },
        {
          "concept": "disease",
          "relevance": 0.429
        },
        {
          "concept": "diagnosis",
          "relevance": 0.427
        },
        {
          "concept": "proportion",
          "relevance": 0.421
        },
        {
          "concept": "decline",
          "relevance": 0.398
        },
        {
          "concept": "age",
          "relevance": 0.396
        },
        {
          "concept": "National",
          "relevance": 0.395
        },
        {
          "concept": "population",
          "relevance": 0.386
        },
        {
          "concept": "tumor",
          "relevance": 0.369
        },
        {
          "concept": "cause",
          "relevance": 0.364
        },
        {
          "concept": "access",
          "relevance": 0.363
        },
        {
          "concept": "etiology",
          "relevance": 0.361
        },
        {
          "concept": "cases",
          "relevance": 0.351
        },
        {
          "concept": "units",
          "relevance": 0.343
        },
        {
          "concept": "treatment",
          "relevance": 0.339
        },
        {
          "concept": "will",
          "relevance": 0.326
        },
        {
          "concept": "mid-2000s",
          "relevance": 0.321
        },
        {
          "concept": "progression",
          "relevance": 0.315
        },
        {
          "concept": "society",
          "relevance": 0.288
        },
        {
          "concept": "update",
          "relevance": 0.279
        },
        {
          "concept": "increase",
          "relevance": 0.275
        },
        {
          "concept": "stage",
          "relevance": 0.271
        },
        {
          "concept": "state",
          "relevance": 0.252
        },
        {
          "concept": "shift",
          "relevance": 0.225
        },
        {
          "concept": "generation",
          "relevance": 0.175
        }
      ]
    },
    {
      "paperId": "pub.1148900016",
      "doi": "10.3322/caac.21731",
      "title": "Cancer treatment and survivorship statistics, 2022",
      "year": 2022,
      "citationCount": 2194,
      "fieldCitationRatio": 797.4,
      "abstract": "The number of cancer survivors continues to increase in the United States due to the growth and aging of the population as well as advances in early detection and treatment. To assist the public health community in better serving these individuals, the American Cancer Society and the National Cancer Institute collaborate triennially to estimate cancer prevalence in the United States using incidence and survival data from the Surveillance, Epidemiology, and End Results cancer registries, vital statistics from the Centers for Disease Control and Prevention's National Center for Health Statistics, and population projections from the US Census Bureau. Current treatment patterns based on information in the National Cancer Database are presented for the most prevalent cancer types by race, and cancer-related and treatment-related side-effects are also briefly described. More than 18 million Americans (8.3 million males and 9.7 million females) with a history of cancer were alive on January 1, 2022. The 3 most prevalent cancers are prostate (3,523,230), melanoma of the skin (760,640), and colon and rectum (726,450) among males and breast (4,055,770), uterine corpus (891,560), and thyroid (823,800) among females. More than one-half (53%) of survivors were diagnosed within the past 10 years, and two-thirds (67%) were aged 65 years or older. One of the largest racial disparities in treatment is for rectal cancer, for which 41% of Black patients with stage I disease receive proctectomy or proctocolectomy compared to 66% of White patients. Surgical receipt is also substantially lower among Black patients with non-small cell lung cancer, 49% for stages I-II and 16% for stage III versus 55% and 22% for White patients, respectively. These treatment disparities are exacerbated by the fact that Black patients continue to be less likely to be diagnosed with stage I disease than White patients for most cancers, with some of the largest disparities for female breast (53% vs 68%) and endometrial (59% vs 73%). Although there are a growing number of tools that can assist patients, caregivers, and clinicians in navigating the various phases of cancer survivorship, further evidence-based strategies and equitable access to available resources are needed to mitigate disparities for communities of color and optimize care for people with a history of cancer. CA Cancer J Clin. 2022;72:409-436.",
      "reference_ids": [
        "pub.1024209358",
        "pub.1141244085",
        "pub.1079363808",
        "pub.1138030703",
        "pub.1049185215",
        "pub.1092405762",
        "pub.1139077509",
        "pub.1011848494",
        "pub.1037096124",
        "pub.1003741861",
        "pub.1040283091",
        "pub.1072920300",
        "pub.1052981885",
        "pub.1053613603",
        "pub.1135671402",
        "pub.1103658909",
        "pub.1038838231",
        "pub.1146158443",
        "pub.1041788748",
        "pub.1101820878",
        "pub.1011419396",
        "pub.1134785376",
        "pub.1135369245",
        "pub.1019391424",
        "pub.1079218512",
        "pub.1142293566",
        "pub.1002803538",
        "pub.1139068035",
        "pub.1025656563",
        "pub.1134557622",
        "pub.1126921698",
        "pub.1000665329",
        "pub.1106970466",
        "pub.1042673214",
        "pub.1099637814",
        "pub.1123948938",
        "pub.1004401373",
        "pub.1141915440",
        "pub.1014692346",
        "pub.1021935355",
        "pub.1011567930",
        "pub.1016232487",
        "pub.1041490891",
        "pub.1009735523",
        "pub.1091890473",
        "pub.1041003792",
        "pub.1005946923",
        "pub.1025650838",
        "pub.1106012864",
        "pub.1134480754",
        "pub.1047686618",
        "pub.1132942947",
        "pub.1053685222",
        "pub.1121958118",
        "pub.1034364010",
        "pub.1104437044",
        "pub.1008520644",
        "pub.1017836241",
        "pub.1123752799",
        "pub.1029325643",
        "pub.1017019981",
        "pub.1142148249",
        "pub.1040871179",
        "pub.1127841029",
        "pub.1092173517",
        "pub.1045945595",
        "pub.1014754437",
        "pub.1043954179",
        "pub.1047202094",
        "pub.1037975934",
        "pub.1041230584",
        "pub.1033499169",
        "pub.1100091117",
        "pub.1007626247",
        "pub.1002511902",
        "pub.1113529536",
        "pub.1145209363",
        "pub.1123183680",
        "pub.1010415551",
        "pub.1011367041",
        "pub.1000173437",
        "pub.1135036081",
        "pub.1106150993",
        "pub.1002282507",
        "pub.1143690092",
        "pub.1125350719",
        "pub.1112712546",
        "pub.1124018716",
        "pub.1016676592",
        "pub.1140208330",
        "pub.1041950077",
        "pub.1111571891",
        "pub.1008626354",
        "pub.1034543175",
        "pub.1077957080",
        "pub.1011081933",
        "pub.1033145416",
        "pub.1107232277",
        "pub.1013064214",
        "pub.1084006245",
        "pub.1106621112",
        "pub.1134761919",
        "pub.1140475538",
        "pub.1005924120",
        "pub.1101412896",
        "pub.1090353316",
        "pub.1027566638",
        "pub.1144442842",
        "pub.1024414682",
        "pub.1131339306",
        "pub.1008727796",
        "pub.1099644889",
        "pub.1013756781",
        "pub.1092692269",
        "pub.1103250294",
        "pub.1050664711",
        "pub.1125327519",
        "pub.1134288834",
        "pub.1084738840",
        "pub.1000199331",
        "pub.1132458235",
        "pub.1007355467",
        "pub.1103163568",
        "pub.1135361346",
        "pub.1024843336",
        "pub.1125029533",
        "pub.1005550247",
        "pub.1139249790",
        "pub.1130127274",
        "pub.1028260717",
        "pub.1053099243",
        "pub.1144930685",
        "pub.1130763564",
        "pub.1049936375",
        "pub.1092500767",
        "pub.1133568870",
        "pub.1014071672",
        "pub.1029425708",
        "pub.1052212829",
        "pub.1013702789",
        "pub.1005912207",
        "pub.1112057736",
        "pub.1085471737",
        "pub.1130874654",
        "pub.1137352840",
        "pub.1107361296",
        "pub.1092239851",
        "pub.1009435698",
        "pub.1052266422",
        "pub.1033414807",
        "pub.1112024379",
        "pub.1135111203",
        "pub.1003194620",
        "pub.1140992017",
        "pub.1050113442",
        "pub.1022130089",
        "pub.1042229500",
        "pub.1091552162",
        "pub.1125558661",
        "pub.1011676133",
        "pub.1051468955",
        "pub.1111218818",
        "pub.1133345675",
        "pub.1030610306",
        "pub.1103272712",
        "pub.1141332718",
        "pub.1104270786",
        "pub.1110954516",
        "pub.1144470394",
        "pub.1115169407",
        "pub.1141522890",
        "pub.1085864115",
        "pub.1037226731",
        "pub.1030983220",
        "pub.1023293062",
        "pub.1144556330",
        "pub.1049590131",
        "pub.1081877197",
        "pub.1133121022",
        "pub.1106024384",
        "pub.1137496424",
        "pub.1013526624",
        "pub.1017610848",
        "pub.1146310596",
        "pub.1046706203",
        "pub.1134499260",
        "pub.1014014595",
        "pub.1092245493",
        "pub.1037374675",
        "pub.1139068738",
        "pub.1028680509",
        "pub.1004461485",
        "pub.1078982170",
        "pub.1139070214",
        "pub.1026847587",
        "pub.1134040512",
        "pub.1020098002",
        "pub.1039338385",
        "pub.1014370191",
        "pub.1038207197",
        "pub.1121435918",
        "pub.1064204244",
        "pub.1011899386",
        "pub.1051064277",
        "pub.1016567841",
        "pub.1128199898",
        "pub.1100292997",
        "pub.1142196275",
        "pub.1005209599",
        "pub.1064203387",
        "pub.1134860776",
        "pub.1000157711",
        "pub.1091533958",
        "pub.1125117396",
        "pub.1033201525",
        "pub.1101219226",
        "pub.1042607992",
        "pub.1140942721",
        "pub.1041864150",
        "pub.1103218019",
        "pub.1125831075",
        "pub.1038260596",
        "pub.1027220823",
        "pub.1014234053",
        "pub.1049360824",
        "pub.1104385459",
        "pub.1002756334",
        "pub.1036540095",
        "pub.1032849612",
        "pub.1078690791",
        "pub.1046120066",
        "pub.1034022202",
        "pub.1082130059",
        "pub.1013947668",
        "pub.1123509509",
        "pub.1144985812",
        "pub.1127979520",
        "pub.1051365811",
        "pub.1029453056",
        "pub.1144614181",
        "pub.1028319837",
        "pub.1027524924",
        "pub.1129904179",
        "pub.1005475008"
      ],
      "concepts_scores": [
        {
          "concept": "history of cancer",
          "relevance": 0.784
        },
        {
          "concept": "white patients",
          "relevance": 0.72
        },
        {
          "concept": "phases of cancer survivorship",
          "relevance": 0.716
        },
        {
          "concept": "Centers for Disease Control and Prevention's National Center for Health Statistics",
          "relevance": 0.713
        },
        {
          "concept": "black patients",
          "relevance": 0.713
        },
        {
          "concept": "National Cancer Institute collaborate",
          "relevance": 0.709
        },
        {
          "concept": "End Results cancer registry",
          "relevance": 0.707
        },
        {
          "concept": "National Center for Health Statistics",
          "relevance": 0.703
        },
        {
          "concept": "Center for Health Statistics",
          "relevance": 0.697
        },
        {
          "concept": "estimate cancer prevalence",
          "relevance": 0.693
        },
        {
          "concept": "American Cancer Society",
          "relevance": 0.678
        },
        {
          "concept": "evidence-based strategies",
          "relevance": 0.67
        },
        {
          "concept": "public health community",
          "relevance": 0.666
        },
        {
          "concept": "stage I disease",
          "relevance": 0.655
        },
        {
          "concept": "US Census Bureau",
          "relevance": 0.645
        },
        {
          "concept": "cancer survivorship",
          "relevance": 0.642
        },
        {
          "concept": "cancer survivors",
          "relevance": 0.639
        },
        {
          "concept": "Cancer Registry",
          "relevance": 0.636
        },
        {
          "concept": "mitigate disparities",
          "relevance": 0.63
        },
        {
          "concept": "Health Statistics",
          "relevance": 0.627
        },
        {
          "concept": "Cancer Society",
          "relevance": 0.624
        },
        {
          "concept": "cancer prevalence",
          "relevance": 0.623
        },
        {
          "concept": "treatment disparities",
          "relevance": 0.621
        },
        {
          "concept": "vital statistics",
          "relevance": 0.62
        },
        {
          "concept": "current treatment patterns",
          "relevance": 0.619
        },
        {
          "concept": "racial disparities",
          "relevance": 0.618
        },
        {
          "concept": "optimal care",
          "relevance": 0.616
        },
        {
          "concept": "communities of color",
          "relevance": 0.613
        },
        {
          "concept": "cancer-related",
          "relevance": 0.609
        },
        {
          "concept": "health community",
          "relevance": 0.607
        },
        {
          "concept": "I disease",
          "relevance": 0.605
        },
        {
          "concept": "United States",
          "relevance": 0.6
        },
        {
          "concept": "equitable access",
          "relevance": 0.599
        },
        {
          "concept": "female breast",
          "relevance": 0.595
        },
        {
          "concept": "non-small cell lung cancer",
          "relevance": 0.58
        },
        {
          "concept": "prevalent cancers",
          "relevance": 0.579
        },
        {
          "concept": "disparities",
          "relevance": 0.578
        },
        {
          "concept": "Census Bureau",
          "relevance": 0.565
        },
        {
          "concept": "National Cancer Database",
          "relevance": 0.565
        },
        {
          "concept": "cell lung cancer",
          "relevance": 0.56
        },
        {
          "concept": "treatment patterns",
          "relevance": 0.545
        },
        {
          "concept": "two-thirds",
          "relevance": 0.54
        },
        {
          "concept": "survivors",
          "relevance": 0.534
        },
        {
          "concept": "early detection",
          "relevance": 0.534
        },
        {
          "concept": "cancer treatment",
          "relevance": 0.527
        },
        {
          "concept": "surgical receipt",
          "relevance": 0.527
        },
        {
          "concept": "uterine corpus",
          "relevance": 0.525
        },
        {
          "concept": "cancer types",
          "relevance": 0.523
        },
        {
          "concept": "population projections",
          "relevance": 0.52
        },
        {
          "concept": "rectal cancer",
          "relevance": 0.519
        },
        {
          "concept": "Cancer Database",
          "relevance": 0.518
        },
        {
          "concept": "American",
          "relevance": 0.512
        },
        {
          "concept": "lung cancer",
          "relevance": 0.509
        },
        {
          "concept": "cancer",
          "relevance": 0.506
        },
        {
          "concept": "breast",
          "relevance": 0.504
        },
        {
          "concept": "stage III",
          "relevance": 0.503
        },
        {
          "concept": "patients",
          "relevance": 0.499
        },
        {
          "concept": "caregivers",
          "relevance": 0.494
        },
        {
          "concept": "institutional collaboration",
          "relevance": 0.492
        },
        {
          "concept": "statistically",
          "relevance": 0.49
        },
        {
          "concept": "National",
          "relevance": 0.488
        },
        {
          "concept": "care",
          "relevance": 0.484
        },
        {
          "concept": "population",
          "relevance": 0.477
        },
        {
          "concept": "survival data",
          "relevance": 0.476
        },
        {
          "concept": "registry",
          "relevance": 0.475
        },
        {
          "concept": "community",
          "relevance": 0.469
        },
        {
          "concept": "years",
          "relevance": 0.462
        },
        {
          "concept": "treatment",
          "relevance": 0.458
        },
        {
          "concept": "receipt",
          "relevance": 0.457
        },
        {
          "concept": "clinicians",
          "relevance": 0.457
        },
        {
          "concept": "epidemiology",
          "relevance": 0.454
        },
        {
          "concept": "prevalence",
          "relevance": 0.453
        },
        {
          "concept": "survivorship",
          "relevance": 0.451
        },
        {
          "concept": "race",
          "relevance": 0.426
        },
        {
          "concept": "units",
          "relevance": 0.424
        },
        {
          "concept": "age",
          "relevance": 0.422
        },
        {
          "concept": "people",
          "relevance": 0.419
        },
        {
          "concept": "history",
          "relevance": 0.415
        },
        {
          "concept": "individuals",
          "relevance": 0.414
        },
        {
          "concept": "surveillance",
          "relevance": 0.413
        },
        {
          "concept": "database",
          "relevance": 0.406
        },
        {
          "concept": "prostate",
          "relevance": 0.406
        },
        {
          "concept": "collaboration",
          "relevance": 0.405
        },
        {
          "concept": "incidence",
          "relevance": 0.404
        },
        {
          "concept": "proctectomy",
          "relevance": 0.404
        },
        {
          "concept": "melanoma",
          "relevance": 0.403
        },
        {
          "concept": "thyroid",
          "relevance": 0.4
        },
        {
          "concept": "proctocolectomy",
          "relevance": 0.4
        },
        {
          "concept": "rectum",
          "relevance": 0.396
        },
        {
          "concept": "male",
          "relevance": 0.39
        },
        {
          "concept": "center",
          "relevance": 0.389
        },
        {
          "concept": "females",
          "relevance": 0.389
        },
        {
          "concept": "access",
          "relevance": 0.387
        },
        {
          "concept": "survival",
          "relevance": 0.381
        },
        {
          "concept": "Bureau",
          "relevance": 0.374
        },
        {
          "concept": "colon",
          "relevance": 0.37
        },
        {
          "concept": "skin",
          "relevance": 0.368
        },
        {
          "concept": "stage",
          "relevance": 0.365
        },
        {
          "concept": "III",
          "relevance": 0.355
        },
        {
          "concept": "resources",
          "relevance": 0.344
        },
        {
          "concept": "strategies",
          "relevance": 0.343
        },
        {
          "concept": "data",
          "relevance": 0.34
        },
        {
          "concept": "information",
          "relevance": 0.334
        },
        {
          "concept": "tools",
          "relevance": 0.334
        },
        {
          "concept": "state",
          "relevance": 0.311
        },
        {
          "concept": "project",
          "relevance": 0.308
        },
        {
          "concept": "society",
          "relevance": 0.307
        },
        {
          "concept": "type",
          "relevance": 0.307
        },
        {
          "concept": "patterns",
          "relevance": 0.276
        },
        {
          "concept": "corpus",
          "relevance": 0.269
        },
        {
          "concept": "detection",
          "relevance": 0.262
        },
        {
          "concept": "growth",
          "relevance": 0.253
        },
        {
          "concept": "phase",
          "relevance": 0.203
        },
        {
          "concept": "color",
          "relevance": 0.17
        }
      ]
    },
    {
      "paperId": "pub.1151960236",
      "doi": "10.1200/jco.22.01690",
      "title": "Treatment of Metastatic Colorectal Cancer: ASCO Guideline",
      "year": 2022,
      "citationCount": 378,
      "fieldCitationRatio": 161.17,
      "abstract": "PURPOSE: To develop recommendations for treatment of patients with metastatic colorectal cancer (mCRC).\nMETHODS: ASCO convened an Expert Panel to conduct a systematic review of relevant studies and develop recommendations for clinical practice.\nRESULTS: Five systematic reviews and 10 randomized controlled trials met the systematic review inclusion criteria.\nRECOMMENDATIONS: Doublet chemotherapy should be offered, or triplet therapy may be offered to patients with previously untreated, initially unresectable mCRC, on the basis of included studies of chemotherapy in combination with anti-vascular endothelial growth factor antibodies. In the first-line setting, pembrolizumab is recommended for patients with mCRC and microsatellite instability-high or deficient mismatch repair tumors; chemotherapy and anti-epidermal growth factor receptor therapy is recommended for microsatellite stable or proficient mismatch repair left-sided treatment-naive <i>RAS</i> wild-type mCRC; chemotherapy and anti-vascular endothelial growth factor therapy is recommended for microsatellite stable or proficient mismatch repair <i>RAS</i> wild-type right-sided mCRC. Encorafenib plus cetuximab is recommended for patients with previously treated <i>BRAF</i> V600E-mutant mCRC that has progressed after at least one previous line of therapy. Cytoreductive surgery plus systemic chemotherapy may be recommended for selected patients with colorectal peritoneal metastases; however, the addition of hyperthermic intraperitoneal chemotherapy is not recommended. Stereotactic body radiation therapy may be recommended following systemic therapy for patients with oligometastases of the liver who are not considered candidates for resection. Selective internal radiation therapy is not routinely recommended for patients with unilobar or bilobar metastases of the liver. Perioperative chemotherapy or surgery alone should be offered to patients with mCRC who are candidates for potentially curative resection of liver metastases. Multidisciplinary team management and shared decision making are recommended. Qualifying statements with further details related to implementation of guideline recommendations are also included.Additional information is available at www.asco.org/gastrointestinal-cancer-guidelines.",
      "reference_ids": [
        "pub.1017105693",
        "pub.1015374378",
        "pub.1133077261",
        "pub.1029533783",
        "pub.1068661719",
        "pub.1121390027",
        "pub.1101003219",
        "pub.1091353557",
        "pub.1148464960",
        "pub.1053140905",
        "pub.1014382035",
        "pub.1130241826",
        "pub.1105988006",
        "pub.1086087326",
        "pub.1014051315",
        "pub.1083763634",
        "pub.1136646837",
        "pub.1041360516",
        "pub.1141237567",
        "pub.1110635124",
        "pub.1122316177",
        "pub.1125511122",
        "pub.1038419533",
        "pub.1105020014",
        "pub.1106909905",
        "pub.1041695189",
        "pub.1141284141",
        "pub.1010204612",
        "pub.1124280446",
        "pub.1134499260",
        "pub.1143332487",
        "pub.1025262969",
        "pub.1134932511",
        "pub.1130077353",
        "pub.1014945327",
        "pub.1105424666",
        "pub.1019437953",
        "pub.1041283275",
        "pub.1090992088",
        "pub.1135108163",
        "pub.1101397559",
        "pub.1000194595",
        "pub.1138549690",
        "pub.1144120162",
        "pub.1143332488",
        "pub.1030588737",
        "pub.1121800921",
        "pub.1112901595",
        "pub.1123795024",
        "pub.1032780345",
        "pub.1028309268",
        "pub.1091554191",
        "pub.1030091417",
        "pub.1022578634",
        "pub.1025548121",
        "pub.1045468727",
        "pub.1122193068",
        "pub.1134681979",
        "pub.1121122379",
        "pub.1064203564",
        "pub.1010823014",
        "pub.1136867424",
        "pub.1128170420",
        "pub.1145095521",
        "pub.1048994422",
        "pub.1051210499",
        "pub.1028959624",
        "pub.1132360625",
        "pub.1125497068",
        "pub.1117502483",
        "pub.1100551947",
        "pub.1026513410",
        "pub.1113479869",
        "pub.1148524810",
        "pub.1079251948",
        "pub.1085935534",
        "pub.1134557622",
        "pub.1039227127",
        "pub.1128496878",
        "pub.1015072088",
        "pub.1045167855",
        "pub.1141124499",
        "pub.1138549770"
      ],
      "concepts_scores": [
        {
          "concept": "metastatic colorectal cancer",
          "relevance": 0.75
        },
        {
          "concept": "anti-vascular endothelial growth factor therapy",
          "relevance": 0.627
        },
        {
          "concept": "anti-epidermal growth factor receptor therapy",
          "relevance": 0.626
        },
        {
          "concept": "wild-type metastatic colorectal cancer",
          "relevance": 0.625
        },
        {
          "concept": "right-sided metastatic colorectal cancer",
          "relevance": 0.625
        },
        {
          "concept": "curative resection of liver metastases",
          "relevance": 0.625
        },
        {
          "concept": "anti-vascular endothelial growth factor antibody",
          "relevance": 0.621
        },
        {
          "concept": "unresectable metastatic colorectal cancer",
          "relevance": 0.619
        },
        {
          "concept": "stereotactic body radiation therapy",
          "relevance": 0.619
        },
        {
          "concept": "resection of liver metastases",
          "relevance": 0.619
        },
        {
          "concept": "first-line setting",
          "relevance": 0.603
        },
        {
          "concept": "colorectal peritoneal metastases",
          "relevance": 0.603
        },
        {
          "concept": "hyperthermic intraperitoneal chemotherapy",
          "relevance": 0.601
        },
        {
          "concept": "studies of chemotherapy",
          "relevance": 0.598
        },
        {
          "concept": "deficient mismatch repair tumors",
          "relevance": 0.595
        },
        {
          "concept": "growth factor therapy",
          "relevance": 0.594
        },
        {
          "concept": "growth factor antibody",
          "relevance": 0.591
        },
        {
          "concept": "treatment of patients",
          "relevance": 0.589
        },
        {
          "concept": "systematic review",
          "relevance": 0.577
        },
        {
          "concept": "multidisciplinary team management",
          "relevance": 0.577
        },
        {
          "concept": "microsatellite instability-high",
          "relevance": 0.575
        },
        {
          "concept": "implementation of guideline recommendations",
          "relevance": 0.561
        },
        {
          "concept": "triplet therapy",
          "relevance": 0.559
        },
        {
          "concept": "cytoreductive surgery",
          "relevance": 0.559
        },
        {
          "concept": "bilobar metastases",
          "relevance": 0.559
        },
        {
          "concept": "randomized controlled trials",
          "relevance": 0.558
        },
        {
          "concept": "doublet chemotherapy",
          "relevance": 0.558
        },
        {
          "concept": "systemic chemotherapy",
          "relevance": 0.558
        },
        {
          "concept": "intraperitoneal chemotherapy",
          "relevance": 0.557
        },
        {
          "concept": "perioperative chemotherapy",
          "relevance": 0.557
        },
        {
          "concept": "curative resection",
          "relevance": 0.557
        },
        {
          "concept": "peritoneal metastasis",
          "relevance": 0.556
        },
        {
          "concept": "radiation therapy",
          "relevance": 0.556
        },
        {
          "concept": "systemic therapy",
          "relevance": 0.555
        },
        {
          "concept": "liver metastases",
          "relevance": 0.555
        },
        {
          "concept": "systematic review inclusion criteria",
          "relevance": 0.553
        },
        {
          "concept": "factor therapy",
          "relevance": 0.552
        },
        {
          "concept": "first-line",
          "relevance": 0.551
        },
        {
          "concept": "ASCO guidelines",
          "relevance": 0.549
        },
        {
          "concept": "chemotherapy",
          "relevance": 0.541
        },
        {
          "concept": "colorectal cancer",
          "relevance": 0.538
        },
        {
          "concept": "guideline recommendations",
          "relevance": 0.535
        },
        {
          "concept": "therapy",
          "relevance": 0.529
        },
        {
          "concept": "patients",
          "relevance": 0.527
        },
        {
          "concept": "controlled trials",
          "relevance": 0.523
        },
        {
          "concept": "metastasis",
          "relevance": 0.523
        },
        {
          "concept": "inclusion criteria",
          "relevance": 0.522
        },
        {
          "concept": "clinical practice",
          "relevance": 0.513
        },
        {
          "concept": "review inclusion criteria",
          "relevance": 0.498
        },
        {
          "concept": "surgery",
          "relevance": 0.496
        },
        {
          "concept": "expert panel",
          "relevance": 0.479
        },
        {
          "concept": "liver",
          "relevance": 0.466
        },
        {
          "concept": "qualifying statements",
          "relevance": 0.444
        },
        {
          "concept": "pembrolizumab",
          "relevance": 0.44
        },
        {
          "concept": "oligometastases",
          "relevance": 0.44
        },
        {
          "concept": "encorafenib",
          "relevance": 0.437
        },
        {
          "concept": "cetuximab",
          "relevance": 0.437
        },
        {
          "concept": "resection",
          "relevance": 0.434
        },
        {
          "concept": "tumor",
          "relevance": 0.427
        },
        {
          "concept": "ASCO",
          "relevance": 0.422
        },
        {
          "concept": "cancer",
          "relevance": 0.416
        },
        {
          "concept": "review",
          "relevance": 0.416
        },
        {
          "concept": "antibodies",
          "relevance": 0.405
        },
        {
          "concept": "trials",
          "relevance": 0.397
        },
        {
          "concept": "treatment",
          "relevance": 0.393
        },
        {
          "concept": "recommendations",
          "relevance": 0.378
        },
        {
          "concept": "study",
          "relevance": 0.374
        },
        {
          "concept": "guidelines",
          "relevance": 0.36
        },
        {
          "concept": "microsatellite",
          "relevance": 0.344
        },
        {
          "concept": "team management",
          "relevance": 0.334
        },
        {
          "concept": "criteria",
          "relevance": 0.333
        },
        {
          "concept": "combination",
          "relevance": 0.312
        },
        {
          "concept": "decision making",
          "relevance": 0.312
        },
        {
          "concept": "management",
          "relevance": 0.311
        },
        {
          "concept": "www",
          "relevance": 0.296
        },
        {
          "concept": "sets",
          "relevance": 0.271
        },
        {
          "concept": "panel",
          "relevance": 0.265
        },
        {
          "concept": "mismatch",
          "relevance": 0.26
        },
        {
          "concept": "practice",
          "relevance": 0.257
        },
        {
          "concept": "experts",
          "relevance": 0.242
        },
        {
          "concept": "making",
          "relevance": 0.239
        },
        {
          "concept": "statements",
          "relevance": 0.225
        },
        {
          "concept": "implementation",
          "relevance": 0.224
        },
        {
          "concept": "information",
          "relevance": 0.215
        },
        {
          "concept": "decision",
          "relevance": 0.211
        },
        {
          "concept": "triplet",
          "relevance": 0.17
        }
      ]
    }
  ],
  "evolution_links": [
    {
      "source": "pub.1170824474",
      "target": "pub.1167961018",
      "source_title": "Recent Advances in Large Language Models for Healthcare",
      "target_title": "Structure and Content-Guided Video Synthesis with Diffusion Models"
    },
    {
      "source": "pub.1167961018",
      "target": "pub.1017774818",
      "source_title": "Structure and Content-Guided Video Synthesis with Diffusion Models",
      "target_title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
    },
    {
      "source": "pub.1017774818",
      "target": "pub.1093626237",
      "source_title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "target_title": "Fully Convolutional Networks for Semantic Segmentation"
    },
    {
      "source": "pub.1017774818",
      "target": "pub.1093828312",
      "source_title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "target_title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
    },
    {
      "source": "pub.1167961018",
      "target": "pub.1095850445",
      "source_title": "Structure and Content-Guided Video Synthesis with Diffusion Models",
      "target_title": "Image-to-Image Translation with Conditional Adversarial Networks"
    },
    {
      "source": "pub.1095850445",
      "target": "pub.1061640964",
      "source_title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "target_title": "Image Quality Assessment: From Error Visibility to Structural Similarity"
    },
    {
      "source": "pub.1095850445",
      "target": "pub.1093626237",
      "source_title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "target_title": "Fully Convolutional Networks for Semantic Segmentation"
    },
    {
      "source": "pub.1170824474",
      "target": "pub.1167107024",
      "source_title": "Recent Advances in Large Language Models for Healthcare",
      "target_title": "Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study"
    },
    {
      "source": "pub.1167107024",
      "target": "pub.1155270525",
      "source_title": "Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study",
      "target_title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models"
    },
    {
      "source": "pub.1155270525",
      "target": "pub.1093497718",
      "source_title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models",
      "target_title": "Rethinking the Inception Architecture for Computer Vision"
    },
    {
      "source": "pub.1155270525",
      "target": "pub.1127685771",
      "source_title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models",
      "target_title": "A deep learning system for differential diagnosis of skin diseases"
    },
    {
      "source": "pub.1167107024",
      "target": "pub.1155844504",
      "source_title": "Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study",
      "target_title": "Colorectal cancer statistics, 2023"
    },
    {
      "source": "pub.1155844504",
      "target": "pub.1148900016",
      "source_title": "Colorectal cancer statistics, 2023",
      "target_title": "Cancer treatment and survivorship statistics, 2022"
    },
    {
      "source": "pub.1155844504",
      "target": "pub.1151960236",
      "source_title": "Colorectal cancer statistics, 2023",
      "target_title": "Treatment of Metastatic Colorectal Cancer: ASCO Guideline"
    }
  ]
}