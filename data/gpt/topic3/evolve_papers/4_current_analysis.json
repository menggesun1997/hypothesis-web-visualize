{
  "prompt": "You are a world-class research strategist and data synthesizer. Your mission is to analyze a curated set of research papers and their underlying conceptual structure to produce a comprehensive 'Landscape Map' that reveals the current state, critical gaps, and novel opportunities in the field of **Optimizing Computational Efficiency of LLMs While Maintaining Reliability in NLP Task Performance**.\n\n### Input: The Evolutionary Research Trajectory\nYou are provided with a curated set of research papers that form an evolutionary path on the topic. This data is structured as a knowledge graph with nodes (the papers) and edges (their citation links).\n\n**Part A.1: The Papers (Nodes in the Knowledge Graph):**\nThese are the key publications that act as milestones along the research path. They are selected for their high citations count and represent significant steps in the evolution of the topic.\n```json[{'paper_id': 1, 'title': 'Large Language Models for Software Engineering: A Systematic Literature Review', 'abstract': ' Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at https://github.com/security-pride/LLM4SE_SLR . '}, {'paper_id': 2, 'title': 'ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design', 'abstract': 'This chapter presents design techniques for software engineering, in the form of prompt patterns, to solve common problems that arise when using large language models (LLMs) to automate common software engineering activities, such as ensuring code is decoupled from third-party libraries and creating API specifications from lists of requirements. This chapter provides two contributions to research on using LLMs for software engineering. First, it provides a catalog of patterns for software engineering that classifies patterns according to the types of problems they solve. Second, it explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, deployment, and testing.'}, {'paper_id': 3, 'title': 'Generalizing from a Few Examples', 'abstract': ' Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research. 1 '}, {'paper_id': 4, 'title': 'Long Short-Term Memory', 'abstract': \"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.\"}, {'paper_id': 5, 'title': 'Learning from Imbalanced Data', 'abstract': 'With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.'}, {'paper_id': 6, 'title': \"Guest Editor's Introduction: Model-Driven Engineering\", 'abstract': 'Model-driven engineering technologies offer a promising approach to address the inability of third-generation languages to alleviate the complexity of platforms and express domain concepts effectively.'}, {'paper_id': 7, 'title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'abstract': 'Large Language Models (LLM) are a new class of computation engines, \"programmed\" via prompt engineering. Researchers are still learning how to best \"program\" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc. One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of \"code analysis\" and extracting such information, while processing code: but are they, really? If they aren\\'t, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM\\'s prompt with semantic facts explicitly, actually helps. Prior work shows that LLM performance on code summarization benefits from embedding a few code & summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization. We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU1. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs\\' line completion performance.'}, {'paper_id': 8, 'title': 'Deep code comment generation', 'abstract': 'During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.'}, {'paper_id': 9, 'title': 'BLEU: a method for automatic evaluation of machine translation', 'abstract': 'Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.'}, {'paper_id': 10, 'title': 'ChatGPT: Jack of all trades, master of none', 'abstract': 'OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. The first contact with the chatbot reveals its ability to provide detailed and precise answers in various areas. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT’s capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed that the more difficult the task (lower SOTA performance), the higher the ChatGPT loss. It especially refers to pragmatic NLP problems like emotion recognition. We also tested the ability to personalize ChatGPT responses for selected subjective tasks via Random Contextual Few-Shot Personalization, and we obtained significantly better user-based predictions. Additional qualitative analysis revealed a ChatGPT bias, most likely due to the rules imposed on human trainers by OpenAI. Our results provide the basis for a fundamental discussion of whether the high quality of recent predictive NLP models can indicate a tool’s usefulness to society and how the learning and validation procedures for such systems should be established.'}]\n```\n\n**Part A.2: The Evolution Links (Edges of the Graph):**\nThe following list defines the citation relationships between the papers in Part A. Each link means that 'the source paper' cites and builds upon the work of 'the target paper'(the earlier paper).\n```list[{'source': 'pub.1175845134', 'target': 'pub.1172304190', 'source_title': 'Large Language Models for Software Engineering: A Systematic Literature Review', 'target_title': 'ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design'}, {'source': 'pub.1172304190', 'target': 'pub.1128474475', 'source_title': 'ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design', 'target_title': 'Generalizing from a Few Examples'}, {'source': 'pub.1128474475', 'target': 'pub.1038140272', 'source_title': 'Generalizing from a Few Examples', 'target_title': 'Long Short-Term Memory'}, {'source': 'pub.1128474475', 'target': 'pub.1061661916', 'source_title': 'Generalizing from a Few Examples', 'target_title': 'Learning from Imbalanced Data'}, {'source': 'pub.1172304190', 'target': 'pub.1061387764', 'source_title': 'ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design', 'target_title': \"Guest Editor's Introduction: Model-Driven Engineering\"}, {'source': 'pub.1175845134', 'target': 'pub.1170644911', 'source_title': 'Large Language Models for Software Engineering: A Systematic Literature Review', 'target_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)'}, {'source': 'pub.1170644911', 'target': 'pub.1105723410', 'source_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'target_title': 'Deep code comment generation'}, {'source': 'pub.1105723410', 'target': 'pub.1038140272', 'source_title': 'Deep code comment generation', 'target_title': 'Long Short-Term Memory'}, {'source': 'pub.1105723410', 'target': 'pub.1099239594', 'source_title': 'Deep code comment generation', 'target_title': 'BLEU: a method for automatic evaluation of machine translation'}, {'source': 'pub.1170644911', 'target': 'pub.1158600650', 'source_title': 'Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)', 'target_title': 'ChatGPT: Jack of all trades, master of none'}, {'source': 'pub.1158600650', 'target': 'pub.1164705743', 'source_title': 'ChatGPT: Jack of all trades, master of none', 'target_title': 'ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports'}, {'source': 'pub.1158600650', 'target': 'pub.1157891790', 'source_title': 'ChatGPT: Jack of all trades, master of none', 'target_title': 'A Categorical Archive of ChatGPT Failures'}]\n```\n\n### Part B: Local Knowledge Skeleton\nThis is the topological analysis of the local concept network built from the above papers. It reveals the internal structure of this specific research cluster.\n**B1. Central Nodes (The Core Focus):**\nThese are the most central concepts, representing the main focus of this research area.\n```list\n['requirements elicitation', 'code quality', 'catalog of patterns', 'improve requirements elicitation', 'software engineering activities', 'third-party libraries', 'improving code quality']\n```\n\n**B2. Thematic Islands (Concept Clusters):**\nThese are clusters of closely related concepts, representing the key sub-themes or research paradigms.\n```list\n[['improve requirements elicitation', 'requirements elicitation', 'catalog of patterns', 'software engineering activities', 'third-party libraries', 'code quality', 'improving code quality']]\n```\n\n**B3. Bridge Nodes (The Connectors):**\nThese concepts connect different clusters within the local network, indicating potential inter-topic relationships.\n```list\n['requirements elicitation', 'code quality', 'catalog of patterns', 'improve requirements elicitation', 'software engineering activities']\n```\n\n### Part C: Global Context & Hidden Bridges (Analysis of the entire database)\nThis is the 'GPS' analysis using second-order co-occurrence to find 'hidden bridges' between the local thematic islands. It points to potential cross-disciplinary opportunities not present in the 10 papers.\n```json\n[]\n```\n\n### Your Task: A Two-Step Process\nYour task involves an internal analysis step followed by a final report generation step.\n\n**Step 1: Internal Analysis & Synthesis (Your thought process - DO NOT include this in the final output)**\nFirst, mentally trace the 'Evolution Links' (A.2) to understand the narrative of scientific progress. Analyze how the core problems and methods have evolved through the papers (A.1). Synthesize this evolutionary understanding with the local (Part B) and global (Part C) context. This deep understanding will be the foundation for your final report.\n\n**Step 2: Generate the Final Research Landscape Map (This is your ONLY output)**\nNow, based on the deep insights gained from your internal analysis in Step 1, generate a concise and insightful report. The report **MUST** contain ONLY the following three sections:\n\n**1. Current Research Landscape & Core Consensus:**\n- What is the central problem and dominant methodology of the **current** state-of-the-art? Your summary should reflect the insights from the evolutionary analysis (e.g., 'the current focus on X is a direct response to the limitations of the earlier Y').\n- Use the Central Nodes (B.1) and Thematic Islands (B.2) to support your summary.\n\n**2. Critical Gaps & Unexplored Frontiers:**\n- **Internal Gaps:** What limitations are present in the recent papers? What weaknesses are revealed by the overall evolutionary trajectory (e.g., 'the field has consistently overlooked problem Z throughout its evolution')?\n- **External/Novel Gaps:** Based on the Global Context (Part C), what novel connections or cross-disciplinary approaches have been overlooked?\n\n**3. High-Potential Innovation Opportunities:**\n- Based on the identified gaps, propose 2-3 high-level, innovative research directions.\n- Each opportunity should be justified by linking a Global Opportunity (from Part C) with a problem or limitation identified in your analysis.\n",
  "analysis": {
    "title": "Landscape Map for Optimizing Computational Efficiency of LLMs While Maintaining Reliability in NLP Task Performance",
    "current_research_landscape": "The current research landscape on optimizing computational efficiency of Large Language Models (LLMs) while maintaining reliability in NLP tasks is dominated by the integration of LLMs into software engineering activities, especially focusing on improving requirements elicitation and code quality. The state-of-the-art reflects a progression from foundational neural architectures like LSTM, through challenges of learning with limited or imbalanced data, towards prompt engineering and semantic augmentation techniques for LLMs applied to software tasks such as code summarization and quality improvement. Central to this landscape is the utilization of cataloged prompt patterns that systematically enhance LLM outputs in software engineering by addressing common problems like decoupling code from third-party libraries and refining API design. This reflects a direct response to earlier limitations of LLMs struggling with data scarcity, task generalization, and noisy inputs, by leveraging few-shot learning, semantic context enrichment, and systematic prompt design. Consequently, the emphasis on requirements elicitation and code quality, supported by a rich catalog of patterns, forms the core thematic cluster driving advances in computational efficiency and reliability within this domain.",
    "critical_gaps": "Internally, the field exhibits several notable limitations: 1) Despite advances in prompt pattern design and semantic augmentation, there remains a significant performance drop of models like ChatGPT on complex, subjective NLP tasks (e.g., emotion recognition), indicating a reliability gap when optimizing for efficiency; 2) The evolutionary trajectory suggests insufficient exploration of model adaptation and robustness in imbalanced or low-resource data scenarios specific to software engineering applications; 3) Existing approaches predominantly address improving software-related NLP tasks but have neglected cross-task transferability and adaptability to diverse NLP domains that challenge computational efficiency versus reliability trade-offs. Externally, the global analysis reveals a lack of cross-disciplinary bridges engaging broader cognitive science principles, human-in-the-loop optimization strategies, or hybrid symbolic-neural methods that could provide novel mechanisms to balance efficiency and reliability. No hidden bridges or second-order co-occurrence suggest underexploited interdisciplinary opportunities such as integrating formal methods from model-driven engineering with neural prompt optimization or leveraging insights from imbalanced learning to refine prompt generalization more systematically.",
    "high_potential_innovation_opportunities": "(1) Development of adaptive prompt engineering frameworks that dynamically tailor semantic augmentation strategies based on task complexity and data representativeness, aiming to mitigate performance degradation in subjective or low-resource NLP tasks within software engineering. This would bridge the internal gap in reliability by leveraging few-shot and imbalanced learning methods more deeply integrated into prompt pattern catalogs. (2) Exploration of hybrid model architectures combining symbolic program analysis (inspired by model-driven engineering) with neural LLMs, to improve computational efficiency by pruning irrelevant model pathways while maintaining task-specific reliability, especially for code summarization and quality assurance tasks. This introduces a cross-disciplinary dimension lacking in the current literature. (3) Integration of human-in-the-loop feedback loops and interactive learning paradigms to iteratively refine prompt design and LLM outputs, fostering robust generalization across imbalanced and evolving software engineering datasets. This approach draws on external cognitive and interactive AI frameworks, potentially unlocking improvements in both computational resource use and output trustworthiness overlooked by current fully automated pipelines."
  }
}