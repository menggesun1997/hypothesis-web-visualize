{
  "before_idea": {
    "title": "PsyFairEval: Psychologically Grounded Fairness Metrics for Human-Level LLM Evaluation",
    "Problem_Statement": "Current fairness evaluation metrics for LLMs focus on statistical parity or benchmark scores, failing to capture nuanced human cognitive responses to biased or unfair outputs, limiting true human-model alignment.",
    "Motivation": "Directly addresses Opportunity 1 by integrating cognitive psychology constructs into evaluation methodology, filling the gap in stable fairness detection by creating metrics that reflect human cognitive fairness perceptions beyond benchmark-centric measures.",
    "Proposed_Method": "Develop new fairness evaluation metrics grounded in cognitive load theory and moral judgment frameworks. This includes measuring cognitive dissonance induced by biased model outputs using user attention tracking, response latency, and sentiment analysis on human feedback. Incorporate these metrics into a composite psychological fairness score for LLM outputs.",
    "Step_by_Step_Experiment_Plan": "1. Recruit human participants from diverse groups and collect responses to LLM outputs with varying bias levels. 2. Track physiological and behavioral indicators (eye-tracking, response time). 3. Correlate these with existing statistical fairness metrics and build predictive models mapping outputs to cognitive fairness scores. 4. Validate metrics on unseen tasks and demographic groups. 5. Release a benchmarking suite for psychological fairness evaluation.",
    "Test_Case_Examples": "Input: LLM-generated responses to politically sensitive questions. Human evaluators show increased cognitive dissonance indicators for biased outputs, reflected by longer response times and negative sentiment, enabling calibration of the new fairness score.",
    "Fallback_Plan": "If physiological measures are noisy, rely on crowd-sourced qualitative fairness ratings combined with NLP sentiment analysis as proxy signals. Alternatively, simplify metrics to textual features correlated with human bias perceptions."
  },
  "after_idea": {
    "ideas": [
      {
        "title": "PsyFairEval v2: Empirically Validated Psychologically Grounded Fairness Metrics for Human-Level LLM Evaluation",
        "Problem_Statement": "Existing fairness evaluation metrics for large language models (LLMs) predominantly rely on statistical parity measures or benchmark scores, which inadequately capture nuanced human cognitive and affective responses to biased or unfair outputs. While prior proposals suggest leveraging cognitive load theory and moral judgment frameworks to reflect psychological fairness perceptions, these constructs risk confounding fairness-specific reactions with other factors like complexity or interest. This proposal addresses the critical gap by integrating empirical psychological evidence and pilot validations that explicitly link cognitive dissonance, attentional deployment, and moral appraisal mechanisms with human fairness perceptions in LLM interactions. By doing so, it aims to isolate fairness-specific cognitive and affective signals from general cognitive load or unrelated moral responses, enabling robust, psychologically grounded fairness metrics that better align with human fairness judgments in diverse contexts.",
        "Motivation": "In light of the NOV-COMPETITIVE verdict and critiques of prior assumptions, this work advances the state-of-the-art by rigorously grounding fairness evaluation metrics in empirically supported psychological constructs with demonstrated specificity to fairness perception. It leverages multidisciplinary cognitive neuroscience and social psychology literature that distinguishes fairness-related cognitive responses from general complexity processing (e.g., controlled experimental studies on cognitive dissonance and fairness judgements) to justify chosen metrics. The motivation is to create metrics that are not only novel in integrating cognitive and moral psychological frameworks but also superior in validity and interpretability compared to purely statistical or proxy measures. This approach is designed to yield human-aligned metrics that better predict actual perceived fairness across demographic and cultural groups, offering a transformative improvement in LLM fairness evaluation.",
        "Proposed_Method": "1. Conduct a comprehensive literature review and meta-analysis of psychological and neuroscience studies that link cognitive load components and moral judgment frameworks specifically to fairness perception, differentiating these from general cognitive processing and non-fairness moral dissonance. 2. Design controlled lab-based pilot studies with validated psychological tasks to preliminarily test and isolate cognitive dissonance and attention patterns elicited specifically by fairness violations in LLM outputs versus confounding factors like complexity or novelty, using a combination of eye-tracking, pupillometry, and concurrent subjective fairness rating protocols. 3. Develop new composite fairness metrics integrating these rigorously validated cognitive and affective signals with sentiment analysis of human feedback, including methods to statistically control for confounds identified in pilot phases. 4. Employ advanced statistical and machine learning models that incorporate demographic, cultural, and task-context variables as covariates to enhance interpretability and generalization of the psychological fairness score. 5. Iteratively refine metrics via an incremental pipeline involving increasingly diverse participant samples and real-world LLM use-cases to ensure scalability and ecological validity. This approach honors both the psychological rigor and practical feasibility imperative for reliable fairness evaluation beyond current benchmarks.",
        "Step_by_Step_Experiment_Plan": "1. Perform meta-analysis to extract psychological evidences linking cognitive/moral mechanisms and fairness perception. 2. Design and run pilot experiments in controlled environments with a small, demographically balanced cohort, incorporating:   - Presentation of LLM outputs varying in bias and complexity   - Measurement via eye-tracking, pupillometry, response latency, and self-reported fairness judgments 3. Analyze pilot data to isolate signals uniquely correlated with fairness perception, controlling for complexity and interest variables using regression and causal inference methods. 4. Based on pilot findings, develop composite psychological fairness metrics incorporating multi-modal signals and demographic covariates. 5. Scale up experiments to a larger, diverse population using a hybrid approach: remote crowd-sourced fairness ratings supplemented by wearable eye-tracking glasses in select subgroups to validate proxy signals. 6. Employ cross-validation across tasks and demographics to test metric robustness and generalizability. 7. Release an open benchmarking suite alongside detailed experimental protocols, allowing replication and wider adoption of validated metrics.",
        "Test_Case_Examples": "Example 1: Politically charged LLM responses exhibiting implicit bias trigger longer fixation durations and higher pupil dilation specifically in fairness-primed participants, correlating strongly with their lower fairness ratings after statistically adjusting for response complexity. Example 2: Culturally distinct participants show varied response latencies and cognitive load patterns to gender-biased outputs, but after controlling for baseline complexity sensitivity, the psychological fairness score remains predictive of fairness perception across groups. These examples demonstrate that the refined metrics capture fairness-specific cognitive and affective responses that align with human judgments beyond confounding factors.",
        "Fallback_Plan": "In the event physiological and behavioral measures prove too noisy or impractical for large-scale deployment, we will pivot to a robust hybrid fallback approach: (i) use well-validated, crowdsourced qualitative fairness ratings augmented by advanced NLP-based sentiment and emotion analyses fine-tuned to detect fairness-related language; (ii) incorporate textual and contextual feature engineering to identify fairness signals in LLM outputs correlated with human fairness judgments; (iii) apply statistical controls and causal modeling to disentangle fairness perception from other cognitive and interest drivers; (iv) implement incremental metric development using iterative crowdsourced feedback cycles to triangulate and refine fairness proxies. This staged fallback ensures psychological grounding is maintained as much as feasible while addressing scalability and noise concerns."
      }
    ]
  },
  "feedback_results": {
    "keywords_query": [
      "PsyFairEval",
      "fairness metrics",
      "cognitive psychology",
      "LLM evaluation",
      "human cognitive fairness",
      "bias detection"
    ],
    "direct_cooccurrence_count": 0,
    "min_pmi_score_value": 2.943702041215401,
    "avg_pmi_score_value": 4.812989329886601,
    "novelty": "NOV-COMPETITIVE",
    "future_suggestions_categories": [],
    "future_suggestions_concepts": [],
    "internal_review": {
      "critiques": [
        {
          "feedback_code": "SOU-ASSUMPTION",
          "feedback_content": "The core assumption that cognitive load theory and moral judgment frameworks can robustly capture human perceptions of fairness in LLM outputs requires stronger justification. It is unclear whether cognitive dissonance measured via attention tracking, response latency, and sentiment analysis sufficiently differentiates between fairness-related cognitive responses and other confounding factors like complexity or interest. The proposal should clarify and possibly preliminarily validate these assumptions or reference prior psychological studies supporting this mapping to strengthen soundness and avoid confounding interpretations of fairness signals in human cognition and behavior metrics. This is critical before fully committing to metric development and model building steps that rely on these constructs as ground truths of psychological fairness perception.\n\nPlease augment the Problem_Statement and Proposed_Method sections with empirical or theoretical backing that explicitly connects the chosen psychological constructs to fairness perception in LLM outputs, anticipating alternative explanations and outlining how to isolate fairness-specific signals from general cognitive load or moral dissonance unrelated to fairness judgments."
        },
        {
          "feedback_code": "FEA-EXPERIMENT",
          "feedback_content": "The experimental plan is ambitious but contains practical challenges that may threaten the feasibility and validity of the results. For instance, eye-tracking and physiological measures to capture cognitive dissonance and attention require careful calibration and controlled environments, which may conflict with recruiting diverse participants and scaling data collection. Moreover, tracking subtle cognitive and affective responses to biased outputs could be highly noisy and influenced by demographic, cultural, and task-specific variables.\n\nThe fallback plan mentions relying on crowdsourced qualitative ratings and NLP sentiment analysis, which may not strongly correlate to cognitive dissonance signals, risking metric dilution or weakening the psychological grounding. The proposal should expand its contingency plans, for example, by integrating pilot studies or incremental metric development phases and providing more detailed protocols addressing noise reduction, participant diversity, environment standardization, and variable control.\n\nClarify how the experiment design will ensure statistical power, demographic representation, and cross-task generalization especially when physiological and behavioral indicators may produce noisy or ambiguous data."
        }
      ]
    }
  }
}