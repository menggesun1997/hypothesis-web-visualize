{
  "papers": [
    {
      "paperId": "pub.1157336399",
      "doi": "10.1016/j.inffus.2023.101805",
      "title": "Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence",
      "year": 2023,
      "citationCount": 813,
      "fieldCitationRatio": 514.5,
      "abstract": "Artificial intelligence (AI) is currently being utilized in a wide range of sophisticated applications, but the outcomes of many AI models are challenging to comprehend and trust due to their black-box nature. Usually, it is essential to understand the reasoning behind an AI model’s decision-making. Thus, the need for eXplainable AI (XAI) methods for improving trust in AI models has arisen. XAI has become a popular research subject within the AI field in recent years. Existing survey papers have tackled the concepts of XAI, its general terms, and post-hoc explainability methods but there have not been any reviews that have looked at the assessment methods, available tools, XAI datasets, and other related aspects. Therefore, in this comprehensive study, we provide readers with an overview of the current research and trends in this rapidly emerging area with a case study example. The study starts by explaining the background of XAI, common definitions, and summarizing recently proposed techniques in XAI for supervised machine learning. The review divides XAI techniques into four axes using a hierarchical categorization system: (i) data explainability, (ii) model explainability, (iii) post-hoc explainability, and (iv) assessment of explanations. We also introduce available evaluation metrics as well as open-source packages and datasets with future research directions. Then, the significance of explainability in terms of legal demands, user viewpoints, and application orientation is outlined, termed as XAI concerns. This paper advocates for tailoring explanation content to specific user types. An examination of XAI techniques and evaluation was conducted by looking at 410 critical articles, published between January 2016 and October 2022, in reputed journals and using a wide range of research databases as a source of information. The article is aimed at XAI researchers who are interested in making their AI models more trustworthy, as well as towards researchers from other disciplines who are looking for effective XAI methods to complete tasks with confidence while communicating meaning from data.",
      "reference_ids": [
        "pub.1061814664",
        "pub.1025763367",
        "pub.1100060663",
        "pub.1143336252",
        "pub.1140105771",
        "pub.1127840628",
        "pub.1034827084",
        "pub.1152164966",
        "pub.1061814324",
        "pub.1110893725",
        "pub.1128394885",
        "pub.1034925587",
        "pub.1147164034",
        "pub.1012204494",
        "pub.1061213767",
        "pub.1094986107",
        "pub.1104451629",
        "pub.1110893724",
        "pub.1112676463",
        "pub.1113814266",
        "pub.1138787573",
        "pub.1085055442",
        "pub.1140876140",
        "pub.1148220063",
        "pub.1134089846",
        "pub.1040622814",
        "pub.1138781225",
        "pub.1106328016",
        "pub.1147164030",
        "pub.1053251882",
        "pub.1010127757",
        "pub.1120516167",
        "pub.1073328979",
        "pub.1111918634",
        "pub.1107865769",
        "pub.1037292812",
        "pub.1061662310",
        "pub.1139669196",
        "pub.1008311198",
        "pub.1154206297",
        "pub.1049861419",
        "pub.1113500190",
        "pub.1093403985",
        "pub.1002229238",
        "pub.1107292064",
        "pub.1034441287",
        "pub.1135223075",
        "pub.1137892255",
        "pub.1122087661",
        "pub.1119901816",
        "pub.1064451185",
        "pub.1138302375",
        "pub.1112085726",
        "pub.1021582385",
        "pub.1094742232",
        "pub.1051347740",
        "pub.1150997559",
        "pub.1114160691",
        "pub.1061219349",
        "pub.1093281053",
        "pub.1107064439",
        "pub.1125837468",
        "pub.1027811023",
        "pub.1106706056",
        "pub.1142759704",
        "pub.1134960870",
        "pub.1044161852",
        "pub.1147164023",
        "pub.1023156425",
        "pub.1017950549",
        "pub.1044560613",
        "pub.1147505309",
        "pub.1061247920",
        "pub.1136926156",
        "pub.1120914928",
        "pub.1148956119",
        "pub.1006616507",
        "pub.1106440987",
        "pub.1133593651",
        "pub.1113813992",
        "pub.1101456014",
        "pub.1130542975",
        "pub.1013909162",
        "pub.1135672253",
        "pub.1105370637",
        "pub.1135227217",
        "pub.1106289667",
        "pub.1150049748",
        "pub.1150127393",
        "pub.1101490909",
        "pub.1133601209",
        "pub.1095018452",
        "pub.1061219401",
        "pub.1134913709",
        "pub.1018748994",
        "pub.1147400226",
        "pub.1152337281",
        "pub.1147456842",
        "pub.1091437760",
        "pub.1142226766",
        "pub.1139076270",
        "pub.1152010588",
        "pub.1116118745",
        "pub.1150084943",
        "pub.1093996183",
        "pub.1107264104",
        "pub.1123502786",
        "pub.1095577162",
        "pub.1058368938",
        "pub.1100060220",
        "pub.1110721049",
        "pub.1103568793",
        "pub.1094856798",
        "pub.1142568599",
        "pub.1139629150",
        "pub.1064449684",
        "pub.1093223794",
        "pub.1122819943",
        "pub.1123242545",
        "pub.1032233097",
        "pub.1084900704",
        "pub.1044325917",
        "pub.1037449119",
        "pub.1148956109",
        "pub.1155720431",
        "pub.1103568851",
        "pub.1130501370",
        "pub.1092229321",
        "pub.1137620924",
        "pub.1084112523",
        "pub.1140672281",
        "pub.1051299379",
        "pub.1024765710",
        "pub.1096024029",
        "pub.1032448819",
        "pub.1135876213",
        "pub.1153794472",
        "pub.1130839933",
        "pub.1135476171",
        "pub.1132656141",
        "pub.1000456826",
        "pub.1064392198",
        "pub.1111607515",
        "pub.1135365217",
        "pub.1139314845",
        "pub.1053662457",
        "pub.1134809936",
        "pub.1045055595",
        "pub.1095437913",
        "pub.1095746446",
        "pub.1136693845",
        "pub.1098652906",
        "pub.1094006321",
        "pub.1123579034",
        "pub.1106333613",
        "pub.1130208127",
        "pub.1110721054",
        "pub.1135137861",
        "pub.1105185896",
        "pub.1085113364",
        "pub.1136369767",
        "pub.1139100959",
        "pub.1038436955",
        "pub.1124138062",
        "pub.1155575924",
        "pub.1142393342",
        "pub.1130041357",
        "pub.1140619700",
        "pub.1142161007",
        "pub.1150076284",
        "pub.1094619162",
        "pub.1100687488",
        "pub.1147164022",
        "pub.1117820769",
        "pub.1120413212",
        "pub.1042615414",
        "pub.1141003671",
        "pub.1017737903",
        "pub.1091437753",
        "pub.1101647668",
        "pub.1113814398",
        "pub.1061719264",
        "pub.1122346024",
        "pub.1152010612",
        "pub.1127385865",
        "pub.1135552875",
        "pub.1111334723",
        "pub.1139338581",
        "pub.1147616318",
        "pub.1151524350",
        "pub.1061814814",
        "pub.1125554669",
        "pub.1135634031",
        "pub.1147164031",
        "pub.1096024430",
        "pub.1108236709",
        "pub.1009640697",
        "pub.1139947460",
        "pub.1154423176",
        "pub.1133339796",
        "pub.1142133182",
        "pub.1148488860",
        "pub.1147164024",
        "pub.1091522528",
        "pub.1045871111",
        "pub.1124215602",
        "pub.1105267403",
        "pub.1124145969",
        "pub.1135187565",
        "pub.1093270996",
        "pub.1094591331",
        "pub.1150355389",
        "pub.1091604205",
        "pub.1092081310",
        "pub.1117925590",
        "pub.1027812501",
        "pub.1061121888",
        "pub.1120522246",
        "pub.1113842568",
        "pub.1145662974",
        "pub.1103568647",
        "pub.1151380649",
        "pub.1091048267",
        "pub.1140638313",
        "pub.1027897187",
        "pub.1092353421",
        "pub.1033364394",
        "pub.1103067323",
        "pub.1127797754",
        "pub.1124784557",
        "pub.1074552015",
        "pub.1118089548",
        "pub.1026490706",
        "pub.1008148086",
        "pub.1030320059",
        "pub.1037469097",
        "pub.1008175277",
        "pub.1151128913",
        "pub.1064449155",
        "pub.1023173124",
        "pub.1023195388",
        "pub.1147164021",
        "pub.1132044794",
        "pub.1061204958",
        "pub.1129237626",
        "pub.1002371134",
        "pub.1064450037",
        "pub.1072617113",
        "pub.1091437739",
        "pub.1131214231",
        "pub.1140862494",
        "pub.1135364489",
        "pub.1028787133",
        "pub.1140544714",
        "pub.1018537934",
        "pub.1152787441",
        "pub.1038696717",
        "pub.1149310112",
        "pub.1053592938",
        "pub.1131889876",
        "pub.1112250084",
        "pub.1111349438",
        "pub.1129662423",
        "pub.1061663019",
        "pub.1129134703",
        "pub.1129723829",
        "pub.1049307256",
        "pub.1012542794",
        "pub.1091107064",
        "pub.1124608023",
        "pub.1120509143",
        "pub.1136573879",
        "pub.1023481852",
        "pub.1117925602",
        "pub.1151002119",
        "pub.1111349511",
        "pub.1036446674",
        "pub.1147164029",
        "pub.1061814557",
        "pub.1047909927",
        "pub.1073402687",
        "pub.1026138623",
        "pub.1136984523",
        "pub.1042154811",
        "pub.1130537759",
        "pub.1092345755",
        "pub.1113832977",
        "pub.1011651510",
        "pub.1084847747",
        "pub.1085136067",
        "pub.1035352508",
        "pub.1131245952",
        "pub.1138465716",
        "pub.1152451603",
        "pub.1092723101",
        "pub.1041608034",
        "pub.1141275958",
        "pub.1083810751",
        "pub.1120935856",
        "pub.1007137628",
        "pub.1134748325",
        "pub.1110300502",
        "pub.1143628288",
        "pub.1153439795",
        "pub.1134389326",
        "pub.1099117725",
        "pub.1119942183",
        "pub.1127446953",
        "pub.1103449795",
        "pub.1024871308",
        "pub.1106005146",
        "pub.1152983485",
        "pub.1144561428",
        "pub.1137371397",
        "pub.1101554124",
        "pub.1002292014",
        "pub.1093194973",
        "pub.1064198742",
        "pub.1061792730",
        "pub.1123669031",
        "pub.1149514501",
        "pub.1134080356",
        "pub.1032944518",
        "pub.1102472475",
        "pub.1090427213",
        "pub.1084541082",
        "pub.1016984185",
        "pub.1152010611",
        "pub.1024136459",
        "pub.1120357365",
        "pub.1095432752",
        "pub.1143465879",
        "pub.1121858826",
        "pub.1052608692",
        "pub.1113814588",
        "pub.1046304259",
        "pub.1141847525",
        "pub.1148956122",
        "pub.1061789059",
        "pub.1036963313",
        "pub.1120935872",
        "pub.1141122143",
        "pub.1091437722",
        "pub.1139834233",
        "pub.1063897184",
        "pub.1112979858",
        "pub.1050594373",
        "pub.1030399495",
        "pub.1111334729",
        "pub.1151670517",
        "pub.1138574334",
        "pub.1123448608",
        "pub.1028574329",
        "pub.1143700610",
        "pub.1147164020",
        "pub.1138039429",
        "pub.1093428229"
      ],
      "concepts_scores": [
        {
          "concept": "artificial intelligence",
          "relevance": 0.755
        },
        {
          "concept": "AI models",
          "relevance": 0.733
        },
        {
          "concept": "XAI techniques",
          "relevance": 0.721
        },
        {
          "concept": "post-hoc explainability methods",
          "relevance": 0.693
        },
        {
          "concept": "concept of XAI",
          "relevance": 0.685
        },
        {
          "concept": "post-hoc explainability",
          "relevance": 0.679
        },
        {
          "concept": "Explainable Artificial Intelligence",
          "relevance": 0.675
        },
        {
          "concept": "black-box nature",
          "relevance": 0.671
        },
        {
          "concept": "trustworthy artificial intelligence",
          "relevance": 0.668
        },
        {
          "concept": "supervised machine learning",
          "relevance": 0.645
        },
        {
          "concept": "open-source package",
          "relevance": 0.634
        },
        {
          "concept": "XAI methods",
          "relevance": 0.628
        },
        {
          "concept": "model decision-making",
          "relevance": 0.626
        },
        {
          "concept": "user's viewpoint",
          "relevance": 0.623
        },
        {
          "concept": "XAI research",
          "relevance": 0.622
        },
        {
          "concept": "evaluation metrics",
          "relevance": 0.621
        },
        {
          "concept": "explainability methods",
          "relevance": 0.62
        },
        {
          "concept": "model explainability",
          "relevance": 0.615
        },
        {
          "concept": "XAI",
          "relevance": 0.613
        },
        {
          "concept": "AI field",
          "relevance": 0.613
        },
        {
          "concept": "user types",
          "relevance": 0.609
        },
        {
          "concept": "machine learning",
          "relevance": 0.608
        },
        {
          "concept": "survey paper",
          "relevance": 0.605
        },
        {
          "concept": "explainability",
          "relevance": 0.592
        },
        {
          "concept": "reputed journals",
          "relevance": 0.573
        },
        {
          "concept": "complete tasks",
          "relevance": 0.564
        },
        {
          "concept": "application orientation",
          "relevance": 0.561
        },
        {
          "concept": "intelligence",
          "relevance": 0.557
        },
        {
          "concept": "research directions",
          "relevance": 0.549
        },
        {
          "concept": "categorization system",
          "relevance": 0.546
        },
        {
          "concept": "dataset",
          "relevance": 0.537
        },
        {
          "concept": "communication means",
          "relevance": 0.537
        },
        {
          "concept": "decision-making",
          "relevance": 0.531
        },
        {
          "concept": "source of information",
          "relevance": 0.521
        },
        {
          "concept": "improve trust",
          "relevance": 0.517
        },
        {
          "concept": "users",
          "relevance": 0.477
        },
        {
          "concept": "trust",
          "relevance": 0.471
        },
        {
          "concept": "case study example",
          "relevance": 0.471
        },
        {
          "concept": "trustworthiness",
          "relevance": 0.451
        },
        {
          "concept": "metrics",
          "relevance": 0.45
        },
        {
          "concept": "applications",
          "relevance": 0.446
        },
        {
          "concept": "task",
          "relevance": 0.445
        },
        {
          "concept": "technique",
          "relevance": 0.444
        },
        {
          "concept": "data",
          "relevance": 0.444
        },
        {
          "concept": "learning",
          "relevance": 0.438
        },
        {
          "concept": "method",
          "relevance": 0.437
        },
        {
          "concept": "study examples",
          "relevance": 0.43
        },
        {
          "concept": "model",
          "relevance": 0.42
        },
        {
          "concept": "assessment methods",
          "relevance": 0.418
        },
        {
          "concept": "evaluation",
          "relevance": 0.417
        },
        {
          "concept": "research",
          "relevance": 0.413
        },
        {
          "concept": "information",
          "relevance": 0.411
        },
        {
          "concept": "database",
          "relevance": 0.398
        },
        {
          "concept": "viewpoint",
          "relevance": 0.394
        },
        {
          "concept": "examples",
          "relevance": 0.377
        },
        {
          "concept": "tools",
          "relevance": 0.372
        },
        {
          "concept": "system",
          "relevance": 0.372
        },
        {
          "concept": "journals",
          "relevance": 0.372
        },
        {
          "concept": "comprehensive study",
          "relevance": 0.362
        },
        {
          "concept": "concept",
          "relevance": 0.357
        },
        {
          "concept": "reasons",
          "relevance": 0.349
        },
        {
          "concept": "research subjects",
          "relevance": 0.348
        },
        {
          "concept": "definition",
          "relevance": 0.348
        },
        {
          "concept": "package",
          "relevance": 0.34
        },
        {
          "concept": "disciplines",
          "relevance": 0.332
        },
        {
          "concept": "post-hoc",
          "relevance": 0.329
        },
        {
          "concept": "overview",
          "relevance": 0.329
        },
        {
          "concept": "assessment of explanations",
          "relevance": 0.327
        },
        {
          "concept": "data",
          "relevance": 0.324
        },
        {
          "concept": "readers",
          "relevance": 0.322
        },
        {
          "concept": "demand",
          "relevance": 0.318
        },
        {
          "concept": "aspects",
          "relevance": 0.314
        },
        {
          "concept": "article",
          "relevance": 0.307
        },
        {
          "concept": "legal demands",
          "relevance": 0.306
        },
        {
          "concept": "direction",
          "relevance": 0.303
        },
        {
          "concept": "background",
          "relevance": 0.301
        },
        {
          "concept": "field",
          "relevance": 0.294
        },
        {
          "concept": "source",
          "relevance": 0.291
        },
        {
          "concept": "concerns",
          "relevance": 0.279
        },
        {
          "concept": "confidence",
          "relevance": 0.27
        },
        {
          "concept": "Research Database",
          "relevance": 0.269
        },
        {
          "concept": "mean",
          "relevance": 0.268
        },
        {
          "concept": "orientation",
          "relevance": 0.267
        },
        {
          "concept": "nature",
          "relevance": 0.265
        },
        {
          "concept": "cases",
          "relevance": 0.258
        },
        {
          "concept": "assessment",
          "relevance": 0.257
        },
        {
          "concept": "trends",
          "relevance": 0.256
        },
        {
          "concept": "survey",
          "relevance": 0.244
        },
        {
          "concept": "explanation",
          "relevance": 0.244
        },
        {
          "concept": "critical articles",
          "relevance": 0.241
        },
        {
          "concept": "review",
          "relevance": 0.224
        },
        {
          "concept": "study",
          "relevance": 0.215
        },
        {
          "concept": "type",
          "relevance": 0.214
        },
        {
          "concept": "subjects",
          "relevance": 0.207
        },
        {
          "concept": "years",
          "relevance": 0.193
        },
        {
          "concept": "significance",
          "relevance": 0.185
        },
        {
          "concept": "paper",
          "relevance": 0.181
        },
        {
          "concept": "outcomes",
          "relevance": 0.167
        },
        {
          "concept": "axis",
          "relevance": 0.158
        },
        {
          "concept": "examination",
          "relevance": 0.145
        }
      ]
    },
    {
      "paperId": "pub.1151380649",
      "doi": "10.1109/cvpr52688.2022.01042",
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "year": 2022,
      "citationCount": 10384,
      "fieldCitationRatio": 4077.61,
      "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
      "reference_ids": [
        "pub.1125160158",
        "pub.1095689025",
        "pub.1142386675",
        "pub.1094016389",
        "pub.1110720202",
        "pub.1123987931",
        "pub.1090555548",
        "pub.1095850445",
        "pub.1150931789",
        "pub.1017774818",
        "pub.1138871251",
        "pub.1150866620",
        "pub.1094476634",
        "pub.1123987722",
        "pub.1145901468",
        "pub.1129913318",
        "pub.1124467986",
        "pub.1128857248",
        "pub.1142373977",
        "pub.1129913308",
        "pub.1110720266",
        "pub.1145639545"
      ],
      "concepts_scores": [
        {
          "concept": "image synthesis",
          "relevance": 0.731
        },
        {
          "concept": "text-to-image synthesis",
          "relevance": 0.709
        },
        {
          "concept": "high-resolution image synthesis",
          "relevance": 0.688
        },
        {
          "concept": "cross-attention layer",
          "relevance": 0.687
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.686
        },
        {
          "concept": "image generation process",
          "relevance": 0.679
        },
        {
          "concept": "GPU days",
          "relevance": 0.641
        },
        {
          "concept": "pretrained autoencoder",
          "relevance": 0.641
        },
        {
          "concept": "image inpainting",
          "relevance": 0.639
        },
        {
          "concept": "bounding boxes",
          "relevance": 0.636
        },
        {
          "concept": "image formation process",
          "relevance": 0.635
        },
        {
          "concept": "latent space",
          "relevance": 0.633
        },
        {
          "concept": "visual fidelity",
          "relevance": 0.632
        },
        {
          "concept": "super-resolution",
          "relevance": 0.627
        },
        {
          "concept": "image generation",
          "relevance": 0.626
        },
        {
          "concept": "convolutional manner",
          "relevance": 0.625
        },
        {
          "concept": "complexity reduction",
          "relevance": 0.624
        },
        {
          "concept": "competitive performance",
          "relevance": 0.623
        },
        {
          "concept": "model architecture",
          "relevance": 0.622
        },
        {
          "concept": "computational resources",
          "relevance": 0.621
        },
        {
          "concept": "high-resolution synthesis",
          "relevance": 0.617
        },
        {
          "concept": "conditional input",
          "relevance": 0.605
        },
        {
          "concept": "computational requirements",
          "relevance": 0.605
        },
        {
          "concept": "pixel space",
          "relevance": 0.601
        },
        {
          "concept": "autoencoder",
          "relevance": 0.576
        },
        {
          "concept": "image data",
          "relevance": 0.573
        },
        {
          "concept": "generation process",
          "relevance": 0.531
        },
        {
          "concept": "images",
          "relevance": 0.509
        },
        {
          "concept": "inpainting",
          "relevance": 0.501
        },
        {
          "concept": "flexible generation",
          "relevance": 0.495
        },
        {
          "concept": "GPU",
          "relevance": 0.494
        },
        {
          "concept": "pixel",
          "relevance": 0.476
        },
        {
          "concept": "training",
          "relevance": 0.474
        },
        {
          "concept": "architecture",
          "relevance": 0.465
        },
        {
          "concept": "high-resolution",
          "relevance": 0.456
        },
        {
          "concept": "task",
          "relevance": 0.449
        },
        {
          "concept": "representation",
          "relevance": 0.439
        },
        {
          "concept": "retraining",
          "relevance": 0.434
        },
        {
          "concept": "DM training",
          "relevance": 0.433
        },
        {
          "concept": "model",
          "relevance": 0.428
        },
        {
          "concept": "optimization",
          "relevance": 0.427
        },
        {
          "concept": "inference",
          "relevance": 0.427
        },
        {
          "concept": "diffusion model",
          "relevance": 0.425
        },
        {
          "concept": "space",
          "relevance": 0.422
        },
        {
          "concept": "input",
          "relevance": 0.418
        },
        {
          "concept": "sequential evaluation",
          "relevance": 0.415
        },
        {
          "concept": "text",
          "relevance": 0.413
        },
        {
          "concept": "performance",
          "relevance": 0.412
        },
        {
          "concept": "requirements",
          "relevance": 0.41
        },
        {
          "concept": "flexibility",
          "relevance": 0.397
        },
        {
          "concept": "applications",
          "relevance": 0.389
        },
        {
          "concept": "resources",
          "relevance": 0.387
        },
        {
          "concept": "process",
          "relevance": 0.376
        },
        {
          "concept": "box",
          "relevance": 0.375
        },
        {
          "concept": "generation",
          "relevance": 0.372
        },
        {
          "concept": "fidelity",
          "relevance": 0.367
        },
        {
          "concept": "evaluation",
          "relevance": 0.363
        },
        {
          "concept": "quality",
          "relevance": 0.353
        },
        {
          "concept": "art",
          "relevance": 0.353
        },
        {
          "concept": "complex",
          "relevance": 0.343
        },
        {
          "concept": "state-of-the-art synthesis",
          "relevance": 0.342
        },
        {
          "concept": "data",
          "relevance": 0.328
        },
        {
          "concept": "manner",
          "relevance": 0.318
        },
        {
          "concept": "ART score",
          "relevance": 0.314
        },
        {
          "concept": "formulation",
          "relevance": 0.283
        },
        {
          "concept": "layer",
          "relevance": 0.283
        },
        {
          "concept": "formation process",
          "relevance": 0.282
        },
        {
          "concept": "preservation",
          "relevance": 0.276
        },
        {
          "concept": "diffusion",
          "relevance": 0.256
        },
        {
          "concept": "mechanism",
          "relevance": 0.24
        },
        {
          "concept": "synthesis",
          "relevance": 0.238
        },
        {
          "concept": "scores",
          "relevance": 0.222
        },
        {
          "concept": "reduction",
          "relevance": 0.205
        },
        {
          "concept": "days",
          "relevance": 0.11
        }
      ]
    },
    {
      "paperId": "pub.1095689025",
      "doi": "10.1109/cvpr.2009.5206848",
      "title": "ImageNet: A large-scale hierarchical image database",
      "year": 2009,
      "citationCount": 48560,
      "fieldCitationRatio": 11425.11,
      "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",
      "reference_ids": [
        "pub.1002200216",
        "pub.1002707761",
        "pub.1093839826",
        "pub.1094055038",
        "pub.1045945012",
        "pub.1093423587",
        "pub.1094700637",
        "pub.1093888066",
        "pub.1061743490",
        "pub.1022501172",
        "pub.1093301542",
        "pub.1017544873",
        "pub.1027534025",
        "pub.1038699157",
        "pub.1061743121",
        "pub.1093557144",
        "pub.1021652857",
        "pub.1052687286",
        "pub.1045723254"
      ],
      "concepts_scores": [
        {
          "concept": "semantic hierarchy of WordNet",
          "relevance": 0.695
        },
        {
          "concept": "hierarchical structure of ImageNet",
          "relevance": 0.695
        },
        {
          "concept": "explosion of image data",
          "relevance": 0.676
        },
        {
          "concept": "large-scale ontologies",
          "relevance": 0.673
        },
        {
          "concept": "computer vision community",
          "relevance": 0.673
        },
        {
          "concept": "synsets of WordNet",
          "relevance": 0.665
        },
        {
          "concept": "data collection scheme",
          "relevance": 0.658
        },
        {
          "concept": "large-scale databases",
          "relevance": 0.644
        },
        {
          "concept": "multimedia data",
          "relevance": 0.625
        },
        {
          "concept": "vision community",
          "relevance": 0.624
        },
        {
          "concept": "annotated images",
          "relevance": 0.619
        },
        {
          "concept": "image classification",
          "relevance": 0.618
        },
        {
          "concept": "semantic hierarchy",
          "relevance": 0.617
        },
        {
          "concept": "WordNet structure",
          "relevance": 0.616
        },
        {
          "concept": "image datasets",
          "relevance": 0.615
        },
        {
          "concept": "object clustering",
          "relevance": 0.613
        },
        {
          "concept": "ImageNet",
          "relevance": 0.611
        },
        {
          "concept": "object recognition",
          "relevance": 0.599
        },
        {
          "concept": "WordNet",
          "relevance": 0.593
        },
        {
          "concept": "Amazon Mechanical Turk",
          "relevance": 0.59
        },
        {
          "concept": "collection scheme",
          "relevance": 0.588
        },
        {
          "concept": "image data",
          "relevance": 0.56
        },
        {
          "concept": "synsets",
          "relevance": 0.556
        },
        {
          "concept": "Mechanical Turk",
          "relevance": 0.547
        },
        {
          "concept": "ontology of images",
          "relevance": 0.536
        },
        {
          "concept": "robust model",
          "relevance": 0.527
        },
        {
          "concept": "hierarchical structure",
          "relevance": 0.518
        },
        {
          "concept": "images",
          "relevance": 0.499
        },
        {
          "concept": "multimedia",
          "relevance": 0.477
        },
        {
          "concept": "Internet",
          "relevance": 0.468
        },
        {
          "concept": "algorithm",
          "relevance": 0.461
        },
        {
          "concept": "dataset",
          "relevance": 0.458
        },
        {
          "concept": "subtrees",
          "relevance": 0.457
        },
        {
          "concept": "database",
          "relevance": 0.455
        },
        {
          "concept": "computer",
          "relevance": 0.451
        },
        {
          "concept": "task",
          "relevance": 0.439
        },
        {
          "concept": "scheme",
          "relevance": 0.439
        },
        {
          "concept": "classification",
          "relevance": 0.436
        },
        {
          "concept": "recognition",
          "relevance": 0.422
        },
        {
          "concept": "accuracy",
          "relevance": 0.419
        },
        {
          "concept": "data",
          "relevance": 0.396
        },
        {
          "concept": "Amazon",
          "relevance": 0.387
        },
        {
          "concept": "applications",
          "relevance": 0.38
        },
        {
          "concept": "clusters",
          "relevance": 0.374
        },
        {
          "concept": "objective",
          "relevance": 0.37
        },
        {
          "concept": "Turks",
          "relevance": 0.355
        },
        {
          "concept": "model",
          "relevance": 0.335
        },
        {
          "concept": "diversity",
          "relevance": 0.335
        },
        {
          "concept": "backbone",
          "relevance": 0.333
        },
        {
          "concept": "research",
          "relevance": 0.327
        },
        {
          "concept": "explosion",
          "relevance": 0.321
        },
        {
          "concept": "opportunities",
          "relevance": 0.295
        },
        {
          "concept": "scale",
          "relevance": 0.28
        },
        {
          "concept": "average",
          "relevance": 0.273
        },
        {
          "concept": "community",
          "relevance": 0.26
        },
        {
          "concept": "structure",
          "relevance": 0.257
        },
        {
          "concept": "analysis",
          "relevance": 0.254
        },
        {
          "concept": "use",
          "relevance": 0.251
        },
        {
          "concept": "index",
          "relevance": 0.226
        },
        {
          "concept": "potential",
          "relevance": 0.215
        }
      ]
    },
    {
      "paperId": "pub.1052687286",
      "doi": "10.1023/b:visi.0000029664.99615.94",
      "title": "Distinctive Image Features from Scale-Invariant Keypoints",
      "year": 2004,
      "citationCount": 45113,
      "fieldCitationRatio": 8931.09,
      "abstract": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.",
      "reference_ids": [
        "pub.1040477036",
        "pub.1014228158",
        "pub.1061156202",
        "pub.1037494632",
        "pub.1003744571",
        "pub.1024856005",
        "pub.1094986307",
        "pub.1004259985",
        "pub.1022603165",
        "pub.1061156611",
        "pub.1099383030",
        "pub.1094775144",
        "pub.1099368997",
        "pub.1021081072",
        "pub.1094093086",
        "pub.1007590088",
        "pub.1034101072",
        "pub.1051377384",
        "pub.1035426628",
        "pub.1046596731",
        "pub.1027706028",
        "pub.1024177337",
        "pub.1061742025",
        "pub.1037481186",
        "pub.1093624919",
        "pub.1045823787",
        "pub.1061155653",
        "pub.1026320994",
        "pub.1094949846"
      ],
      "concepts_scores": [
        {
          "concept": "database of features",
          "relevance": 0.754
        },
        {
          "concept": "near real-time performance",
          "relevance": 0.667
        },
        {
          "concept": "scale-invariant keypoints",
          "relevance": 0.656
        },
        {
          "concept": "real-time performance",
          "relevance": 0.642
        },
        {
          "concept": "Distinctive imaging features",
          "relevance": 0.611
        },
        {
          "concept": "affine distortion",
          "relevance": 0.607
        },
        {
          "concept": "robust matching",
          "relevance": 0.599
        },
        {
          "concept": "Hough transform",
          "relevance": 0.586
        },
        {
          "concept": "object recognition",
          "relevance": 0.585
        },
        {
          "concept": "least-squares solution",
          "relevance": 0.572
        },
        {
          "concept": "image features",
          "relevance": 0.552
        },
        {
          "concept": "image scale",
          "relevance": 0.54
        },
        {
          "concept": "recognition",
          "relevance": 0.501
        },
        {
          "concept": "keypoints",
          "relevance": 0.476
        },
        {
          "concept": "images",
          "relevance": 0.475
        },
        {
          "concept": "features",
          "relevance": 0.474
        },
        {
          "concept": "scale invariance",
          "relevance": 0.474
        },
        {
          "concept": "matching",
          "relevance": 0.464
        },
        {
          "concept": "objective",
          "relevance": 0.45
        },
        {
          "concept": "scene",
          "relevance": 0.45
        },
        {
          "concept": "database",
          "relevance": 0.444
        },
        {
          "concept": "clutter",
          "relevance": 0.435
        },
        {
          "concept": "verification",
          "relevance": 0.421
        },
        {
          "concept": "noise",
          "relevance": 0.418
        },
        {
          "concept": "performance",
          "relevance": 0.394
        },
        {
          "concept": "viewpoint",
          "relevance": 0.38
        },
        {
          "concept": "illumination",
          "relevance": 0.38
        },
        {
          "concept": "probability",
          "relevance": 0.374
        },
        {
          "concept": "distortion",
          "relevance": 0.369
        },
        {
          "concept": "clusters",
          "relevance": 0.365
        },
        {
          "concept": "transformation",
          "relevance": 0.358
        },
        {
          "concept": "method",
          "relevance": 0.346
        },
        {
          "concept": "views",
          "relevance": 0.342
        },
        {
          "concept": "solution",
          "relevance": 0.337
        },
        {
          "concept": "parameters",
          "relevance": 0.335
        },
        {
          "concept": "occlusion",
          "relevance": 0.314
        },
        {
          "concept": "rotation",
          "relevance": 0.305
        },
        {
          "concept": "scale",
          "relevance": 0.241
        },
        {
          "concept": "changes",
          "relevance": 0.205
        }
      ]
    },
    {
      "paperId": "pub.1027534025",
      "doi": "10.1007/s11263-007-0090-8",
      "title": "LabelMe: A Database and Web-Based Tool for Image Annotation",
      "year": 2007,
      "citationCount": 3093,
      "fieldCitationRatio": 663.1,
      "abstract": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.",
      "reference_ids": [
        "pub.1094301320",
        "pub.1015899908",
        "pub.1021318481",
        "pub.1061742623",
        "pub.1061743121",
        "pub.1094132829",
        "pub.1094707806",
        "pub.1008598522",
        "pub.1094251884",
        "pub.1110625185",
        "pub.1043225769",
        "pub.1094870330",
        "pub.1019562355",
        "pub.1061743101",
        "pub.1095195005",
        "pub.1094136659",
        "pub.1095068040",
        "pub.1095164167",
        "pub.1093279315",
        "pub.1093955201",
        "pub.1093187020",
        "pub.1034831185",
        "pub.1094053049",
        "pub.1021652857",
        "pub.1063151975",
        "pub.1094847977",
        "pub.1032051655",
        "pub.1026428551",
        "pub.1091765347",
        "pub.1094700637",
        "pub.1034091054",
        "pub.1093305113"
      ],
      "concepts_scores": [
        {
          "concept": "collection of images",
          "relevance": 0.678
        },
        {
          "concept": "minimal user supervision",
          "relevance": 0.669
        },
        {
          "concept": "image annotation",
          "relevance": 0.639
        },
        {
          "concept": "object detection",
          "relevance": 0.636
        },
        {
          "concept": "supervised learning",
          "relevance": 0.634
        },
        {
          "concept": "recognition research",
          "relevance": 0.629
        },
        {
          "concept": "object parts",
          "relevance": 0.625
        },
        {
          "concept": "user supervision",
          "relevance": 0.624
        },
        {
          "concept": "depth order",
          "relevance": 0.621
        },
        {
          "concept": "object categories",
          "relevance": 0.619
        },
        {
          "concept": "object labels",
          "relevance": 0.619
        },
        {
          "concept": "art datasets",
          "relevance": 0.618
        },
        {
          "concept": "object recognition",
          "relevance": 0.613
        },
        {
          "concept": "multiple instances",
          "relevance": 0.612
        },
        {
          "concept": "annotation tool",
          "relevance": 0.596
        },
        {
          "concept": "web-based tool",
          "relevance": 0.581
        },
        {
          "concept": "dataset",
          "relevance": 0.58
        },
        {
          "concept": "annotation",
          "relevance": 0.557
        },
        {
          "concept": "quantitative evaluation",
          "relevance": 0.511
        },
        {
          "concept": "images",
          "relevance": 0.506
        },
        {
          "concept": "recognition",
          "relevance": 0.499
        },
        {
          "concept": "WordNet",
          "relevance": 0.498
        },
        {
          "concept": "scene",
          "relevance": 0.472
        },
        {
          "concept": "instances",
          "relevance": 0.471
        },
        {
          "concept": "automatically",
          "relevance": 0.471
        },
        {
          "concept": "objective",
          "relevance": 0.469
        },
        {
          "concept": "detection",
          "relevance": 0.46
        },
        {
          "concept": "labeling",
          "relevance": 0.46
        },
        {
          "concept": "Web",
          "relevance": 0.46
        },
        {
          "concept": "learning",
          "relevance": 0.442
        },
        {
          "concept": "tools",
          "relevance": 0.435
        },
        {
          "concept": "sharing",
          "relevance": 0.425
        },
        {
          "concept": "database",
          "relevance": 0.402
        },
        {
          "concept": "collection",
          "relevance": 0.366
        },
        {
          "concept": "supervision",
          "relevance": 0.366
        },
        {
          "concept": "evaluation",
          "relevance": 0.363
        },
        {
          "concept": "art",
          "relevance": 0.353
        },
        {
          "concept": "Abstract",
          "relevance": 0.346
        },
        {
          "concept": "research",
          "relevance": 0.335
        },
        {
          "concept": "data",
          "relevance": 0.328
        },
        {
          "concept": "categories",
          "relevance": 0.317
        },
        {
          "concept": "order",
          "relevance": 0.307
        },
        {
          "concept": "parts",
          "relevance": 0.298
        },
        {
          "concept": "content",
          "relevance": 0.258
        },
        {
          "concept": "depth",
          "relevance": 0.252
        }
      ]
    },
    {
      "paperId": "pub.1095850445",
      "doi": "10.1109/cvpr.2017.632",
      "title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "year": 2017,
      "citationCount": 17290,
      "fieldCitationRatio": 3485.46,
      "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
      "reference_ids": [
        "pub.1036703514",
        "pub.1017774818",
        "pub.1061164348",
        "pub.1026908314",
        "pub.1094854085",
        "pub.1095706293",
        "pub.1093406896",
        "pub.1094706336",
        "pub.1044720211",
        "pub.1016085920",
        "pub.1034474844",
        "pub.1095493673",
        "pub.1027979653",
        "pub.1094856798",
        "pub.1000850552",
        "pub.1000375585",
        "pub.1063151969",
        "pub.1046666876",
        "pub.1093626237",
        "pub.1035996172",
        "pub.1040802208",
        "pub.1094045097",
        "pub.1017576733",
        "pub.1051119970",
        "pub.1015291806",
        "pub.1093907742",
        "pub.1095367750",
        "pub.1004607132",
        "pub.1061640964",
        "pub.1093662716",
        "pub.1023031850",
        "pub.1009767488",
        "pub.1035588163"
      ],
      "concepts_scores": [
        {
          "concept": "conditional adversarial network",
          "relevance": 0.782
        },
        {
          "concept": "adversarial network",
          "relevance": 0.725
        },
        {
          "concept": "input image to output image",
          "relevance": 0.709
        },
        {
          "concept": "loss function",
          "relevance": 0.706
        },
        {
          "concept": "image-to-image translation problem",
          "relevance": 0.704
        },
        {
          "concept": "image-to-image translation",
          "relevance": 0.699
        },
        {
          "concept": "Image-to-image",
          "relevance": 0.678
        },
        {
          "concept": "general-purpose solution",
          "relevance": 0.666
        },
        {
          "concept": "hand-engineering",
          "relevance": 0.635
        },
        {
          "concept": "synthesized photos",
          "relevance": 0.629
        },
        {
          "concept": "hand-engineered",
          "relevance": 0.628
        },
        {
          "concept": "edge map",
          "relevance": 0.626
        },
        {
          "concept": "output image",
          "relevance": 0.623
        },
        {
          "concept": "label maps",
          "relevance": 0.623
        },
        {
          "concept": "color images",
          "relevance": 0.618
        },
        {
          "concept": "reconstructed object",
          "relevance": 0.599
        },
        {
          "concept": "Twitter users",
          "relevance": 0.599
        },
        {
          "concept": "mapping function",
          "relevance": 0.594
        },
        {
          "concept": "loss formulation",
          "relevance": 0.592
        },
        {
          "concept": "translation problems",
          "relevance": 0.561
        },
        {
          "concept": "network",
          "relevance": 0.556
        },
        {
          "concept": "maps",
          "relevance": 0.491
        },
        {
          "concept": "Pix2Pix",
          "relevance": 0.485
        },
        {
          "concept": "users",
          "relevance": 0.477
        },
        {
          "concept": "images",
          "relevance": 0.469
        },
        {
          "concept": "Twitter",
          "relevance": 0.468
        },
        {
          "concept": "software",
          "relevance": 0.454
        },
        {
          "concept": "task",
          "relevance": 0.444
        },
        {
          "concept": "input",
          "relevance": 0.414
        },
        {
          "concept": "labeling",
          "relevance": 0.393
        },
        {
          "concept": "photo",
          "relevance": 0.388
        },
        {
          "concept": "edge",
          "relevance": 0.379
        },
        {
          "concept": "objective",
          "relevance": 0.375
        },
        {
          "concept": "system",
          "relevance": 0.371
        },
        {
          "concept": "function",
          "relevance": 0.362
        },
        {
          "concept": "solution",
          "relevance": 0.349
        },
        {
          "concept": "color",
          "relevance": 0.345
        },
        {
          "concept": "experiments",
          "relevance": 0.34
        },
        {
          "concept": "translation",
          "relevance": 0.335
        },
        {
          "concept": "results",
          "relevance": 0.312
        },
        {
          "concept": "loss",
          "relevance": 0.285
        },
        {
          "concept": "formulation",
          "relevance": 0.28
        },
        {
          "concept": "artistic experience",
          "relevance": 0.269
        },
        {
          "concept": "community",
          "relevance": 0.263
        },
        {
          "concept": "conditions",
          "relevance": 0.206
        },
        {
          "concept": "problem",
          "relevance": 0.155
        },
        {
          "concept": "release",
          "relevance": 0.153
        }
      ]
    },
    {
      "paperId": "pub.1061640964",
      "doi": "10.1109/tip.2003.819861",
      "title": "Image Quality Assessment: From Error Visibility to Structural Similarity",
      "year": 2004,
      "citationCount": 44667,
      "fieldCitationRatio": 8842.79,
      "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.",
      "reference_ids": [
        "pub.1061098596",
        "pub.1109699835",
        "pub.1051590917",
        "pub.1061157146",
        "pub.1038074861",
        "pub.1110622471",
        "pub.1042918725",
        "pub.1002450358",
        "pub.1065157151",
        "pub.1061251565",
        "pub.1021151071",
        "pub.1094280748",
        "pub.1010524390",
        "pub.1007353974",
        "pub.1093619469",
        "pub.1061647489",
        "pub.1065210132",
        "pub.1061228473",
        "pub.1061222004",
        "pub.1011624274",
        "pub.1088779547",
        "pub.1005576318",
        "pub.1014836974",
        "pub.1095102111",
        "pub.1016979910",
        "pub.1065158325",
        "pub.1052170656",
        "pub.1065159598",
        "pub.1037873940",
        "pub.1037099914",
        "pub.1020793995",
        "pub.1004119149",
        "pub.1009205613",
        "pub.1061137112",
        "pub.1026562287",
        "pub.1061240373",
        "pub.1094779682",
        "pub.1061240018",
        "pub.1093467274",
        "pub.1061239913",
        "pub.1006831752",
        "pub.1061240395",
        "pub.1003534189",
        "pub.1010313466",
        "pub.1032128907",
        "pub.1061239618",
        "pub.1095644828",
        "pub.1024091291",
        "pub.1020705229"
      ],
      "concepts_scores": [
        {
          "concept": "degradation of structural information",
          "relevance": 0.712
        },
        {
          "concept": "perceptual image quality",
          "relevance": 0.689
        },
        {
          "concept": "visibility of errors",
          "relevance": 0.683
        },
        {
          "concept": "structural similarity index",
          "relevance": 0.683
        },
        {
          "concept": "human visual perception",
          "relevance": 0.681
        },
        {
          "concept": "database of images",
          "relevance": 0.68
        },
        {
          "concept": "human visual system",
          "relevance": 0.679
        },
        {
          "concept": "error visibility",
          "relevance": 0.63
        },
        {
          "concept": "distorted images",
          "relevance": 0.62
        },
        {
          "concept": "structural information",
          "relevance": 0.608
        },
        {
          "concept": "visual system",
          "relevance": 0.579
        },
        {
          "concept": "similarity index",
          "relevance": 0.577
        },
        {
          "concept": "image quality",
          "relevance": 0.57
        },
        {
          "concept": "visual perception",
          "relevance": 0.564
        },
        {
          "concept": "quality assessment",
          "relevance": 0.519
        },
        {
          "concept": "JPEG2000",
          "relevance": 0.505
        },
        {
          "concept": "JPEG",
          "relevance": 0.503
        },
        {
          "concept": "structural similarity",
          "relevance": 0.499
        },
        {
          "concept": "images",
          "relevance": 0.499
        },
        {
          "concept": "information",
          "relevance": 0.482
        },
        {
          "concept": "error",
          "relevance": 0.477
        },
        {
          "concept": "scene",
          "relevance": 0.473
        },
        {
          "concept": "visibility",
          "relevance": 0.462
        },
        {
          "concept": "framework",
          "relevance": 0.415
        },
        {
          "concept": "quality",
          "relevance": 0.409
        },
        {
          "concept": "database",
          "relevance": 0.402
        },
        {
          "concept": "complementary framework",
          "relevance": 0.401
        },
        {
          "concept": "examples",
          "relevance": 0.382
        },
        {
          "concept": "similarity",
          "relevance": 0.376
        },
        {
          "concept": "system",
          "relevance": 0.376
        },
        {
          "concept": "method",
          "relevance": 0.363
        },
        {
          "concept": "concept",
          "relevance": 0.361
        },
        {
          "concept": "properties",
          "relevance": 0.329
        },
        {
          "concept": "degradation",
          "relevance": 0.322
        },
        {
          "concept": "structure",
          "relevance": 0.302
        },
        {
          "concept": "perception",
          "relevance": 0.294
        },
        {
          "concept": "comparison",
          "relevance": 0.283
        },
        {
          "concept": "rate",
          "relevance": 0.257
        },
        {
          "concept": "index",
          "relevance": 0.232
        },
        {
          "concept": "assessment",
          "relevance": 0.225
        },
        {
          "concept": "differences",
          "relevance": 0.153
        }
      ]
    },
    {
      "paperId": "pub.1093626237",
      "doi": "10.1109/cvpr.2015.7298965",
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "year": 2015,
      "citationCount": 32637,
      "fieldCitationRatio": 6415.44,
      "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20]), the VGG net [1], and GoogLeNet [2]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IV on 2012), NYVDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",
      "reference_ids": [
        "pub.1061641212",
        "pub.1061743868",
        "pub.1033986161",
        "pub.1003867987",
        "pub.1094083003",
        "pub.1053469442",
        "pub.1095134254",
        "pub.1024540204",
        "pub.1030406568",
        "pub.1095686079",
        "pub.1008345178",
        "pub.1052031051",
        "pub.1016764525",
        "pub.1094727707",
        "pub.1093843992",
        "pub.1094291017",
        "pub.1032233097",
        "pub.1006936750",
        "pub.1015397249",
        "pub.1003201959",
        "pub.1003742061",
        "pub.1093500653"
      ],
      "concepts_scores": [
        {
          "concept": "convolutional network",
          "relevance": 0.779
        },
        {
          "concept": "semantic segmentation",
          "relevance": 0.726
        },
        {
          "concept": "yield hierarchies of features",
          "relevance": 0.7
        },
        {
          "concept": "state-of-the-art segmentation",
          "relevance": 0.698
        },
        {
          "concept": "trained end-to-end",
          "relevance": 0.697
        },
        {
          "concept": "input of arbitrary size",
          "relevance": 0.696
        },
        {
          "concept": "correspondingly-sized output",
          "relevance": 0.682
        },
        {
          "concept": "dense prediction tasks",
          "relevance": 0.682
        },
        {
          "concept": "hierarchies of features",
          "relevance": 0.677
        },
        {
          "concept": "pixels-to-pixels",
          "relevance": 0.676
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.676
        },
        {
          "concept": "fully convolutional network",
          "relevance": 0.671
        },
        {
          "concept": "end-to-end",
          "relevance": 0.663
        },
        {
          "concept": "PASCAL VOC",
          "relevance": 0.631
        },
        {
          "concept": "learned representations",
          "relevance": 0.629
        },
        {
          "concept": "appearance information",
          "relevance": 0.629
        },
        {
          "concept": "classification network",
          "relevance": 0.626
        },
        {
          "concept": "VGG-Net",
          "relevance": 0.626
        },
        {
          "concept": "SIFT flow",
          "relevance": 0.626
        },
        {
          "concept": "segmentation task",
          "relevance": 0.623
        },
        {
          "concept": "semantic information",
          "relevance": 0.619
        },
        {
          "concept": "prediction task",
          "relevance": 0.615
        },
        {
          "concept": "efficient inference",
          "relevance": 0.614
        },
        {
          "concept": "visual model",
          "relevance": 0.593
        },
        {
          "concept": "network",
          "relevance": 0.569
        },
        {
          "concept": "arbitrary size",
          "relevance": 0.562
        },
        {
          "concept": "task",
          "relevance": 0.513
        },
        {
          "concept": "coarse layer",
          "relevance": 0.495
        },
        {
          "concept": "VGG",
          "relevance": 0.491
        },
        {
          "concept": "inference",
          "relevance": 0.487
        },
        {
          "concept": "semantics",
          "relevance": 0.478
        },
        {
          "concept": "segments",
          "relevance": 0.474
        },
        {
          "concept": "information",
          "relevance": 0.474
        },
        {
          "concept": "SIFT",
          "relevance": 0.473
        },
        {
          "concept": "architecture",
          "relevance": 0.459
        },
        {
          "concept": "classification",
          "relevance": 0.439
        },
        {
          "concept": "learning",
          "relevance": 0.436
        },
        {
          "concept": "representation",
          "relevance": 0.432
        },
        {
          "concept": "nets",
          "relevance": 0.413
        },
        {
          "concept": "input",
          "relevance": 0.412
        },
        {
          "concept": "fine layer",
          "relevance": 0.404
        },
        {
          "concept": "images",
          "relevance": 0.403
        },
        {
          "concept": "Fully",
          "relevance": 0.396
        },
        {
          "concept": "model",
          "relevance": 0.391
        },
        {
          "concept": "features",
          "relevance": 0.391
        },
        {
          "concept": "output",
          "relevance": 0.387
        },
        {
          "concept": "connection",
          "relevance": 0.367
        },
        {
          "concept": "space",
          "relevance": 0.359
        },
        {
          "concept": "layer",
          "relevance": 0.323
        },
        {
          "concept": "VOC",
          "relevance": 0.319
        },
        {
          "concept": "spatially",
          "relevance": 0.318
        },
        {
          "concept": "size",
          "relevance": 0.264
        },
        {
          "concept": "appearance",
          "relevance": 0.253
        },
        {
          "concept": "flow",
          "relevance": 0.208
        }
      ]
    },
    {
      "paperId": "pub.1150997559",
      "doi": "10.1007/s10115-022-01756-8",
      "title": "Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond",
      "year": 2022,
      "citationCount": 319,
      "fieldCitationRatio": 127.53,
      "abstract": "Deep neural networks have been well-known for their superb handling of various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal how deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we first introduce and clarify two basic concepts—interpretations and interpretability—that people usually get confused about. To address the research efforts in interpretations, we elaborate the designs of a number of interpretation algorithms, from different perspectives, by proposing a new taxonomy. Then, to understand the interpretation results, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the current works in evaluating models’ interpretability using “trustworthy” interpretation algorithms. Finally, we review and discuss the connections between deep models’ interpretations and other factors, such as adversarial robustness and learning from interpretations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.",
      "reference_ids": [
        "pub.1129913365",
        "pub.1125554669",
        "pub.1150329651",
        "pub.1109352328",
        "pub.1122290019",
        "pub.1120990524",
        "pub.1120401846",
        "pub.1105538429",
        "pub.1096023938",
        "pub.1018298256",
        "pub.1092261951",
        "pub.1090904008",
        "pub.1127645248",
        "pub.1093270996",
        "pub.1109352387",
        "pub.1127249624",
        "pub.1121025172",
        "pub.1150867120",
        "pub.1092625289",
        "pub.1144479479",
        "pub.1115961194",
        "pub.1135553216",
        "pub.1130208127",
        "pub.1092723101",
        "pub.1142385965",
        "pub.1092345755",
        "pub.1148956109",
        "pub.1121024874",
        "pub.1124267276",
        "pub.1121858826",
        "pub.1142383734",
        "pub.1142378451",
        "pub.1148956387",
        "pub.1095689025",
        "pub.1130698369",
        "pub.1064392198",
        "pub.1099151330",
        "pub.1084013014",
        "pub.1135710967",
        "pub.1125458335",
        "pub.1145901817",
        "pub.1053251882",
        "pub.1128280227",
        "pub.1099930454",
        "pub.1086009261",
        "pub.1145901038",
        "pub.1107454571",
        "pub.1148956122",
        "pub.1129756981",
        "pub.1128855985",
        "pub.1129757281",
        "pub.1031679233",
        "pub.1107022940",
        "pub.1103853893",
        "pub.1125161077",
        "pub.1009495412",
        "pub.1039427823",
        "pub.1010020120",
        "pub.1128708772",
        "pub.1129757409",
        "pub.1094357983",
        "pub.1122790958",
        "pub.1117659183",
        "pub.1126277589",
        "pub.1110721054",
        "pub.1122194839",
        "pub.1127315758",
        "pub.1107691072",
        "pub.1038436955",
        "pub.1043419628",
        "pub.1095849131",
        "pub.1136009482",
        "pub.1140387296",
        "pub.1121715269",
        "pub.1123988120",
        "pub.1121025152",
        "pub.1107264104",
        "pub.1026271373",
        "pub.1091495976",
        "pub.1107865769",
        "pub.1135293067",
        "pub.1148845264",
        "pub.1030517994",
        "pub.1111349486",
        "pub.1093687173",
        "pub.1122707489",
        "pub.1118155103",
        "pub.1100060361",
        "pub.1120935860",
        "pub.1123988581",
        "pub.1061719264",
        "pub.1095620157",
        "pub.1036446674",
        "pub.1133174459",
        "pub.1129723829",
        "pub.1043777853",
        "pub.1036183961",
        "pub.1110720657",
        "pub.1127840628",
        "pub.1119923412",
        "pub.1132472148",
        "pub.1131933523",
        "pub.1002371134",
        "pub.1139691916",
        "pub.1129756980",
        "pub.1100885465",
        "pub.1008096476"
      ],
      "concepts_scores": [
        {
          "concept": "deep models",
          "relevance": 0.733
        },
        {
          "concept": "artificial intelligence tasks",
          "relevance": 0.686
        },
        {
          "concept": "interpretation algorithms",
          "relevance": 0.682
        },
        {
          "concept": "deep neural networks",
          "relevance": 0.681
        },
        {
          "concept": "black-box nature",
          "relevance": 0.674
        },
        {
          "concept": "open-source library",
          "relevance": 0.672
        },
        {
          "concept": "adversarial robustness",
          "relevance": 0.636
        },
        {
          "concept": "neural network",
          "relevance": 0.618
        },
        {
          "concept": "intelligence tasks",
          "relevance": 0.618
        },
        {
          "concept": "machine learning",
          "relevance": 0.612
        },
        {
          "concept": "performance metrics",
          "relevance": 0.604
        },
        {
          "concept": "model interpretation",
          "relevance": 0.592
        },
        {
          "concept": "algorithm",
          "relevance": 0.58
        },
        {
          "concept": "comprehensive survey",
          "relevance": 0.557
        },
        {
          "concept": "evaluation model",
          "relevance": 0.546
        },
        {
          "concept": "prediction results",
          "relevance": 0.535
        },
        {
          "concept": "evaluation approach",
          "relevance": 0.528
        },
        {
          "concept": "learning",
          "relevance": 0.51
        },
        {
          "concept": "research efforts",
          "relevance": 0.49
        },
        {
          "concept": "network",
          "relevance": 0.46
        },
        {
          "concept": "trustworthiness",
          "relevance": 0.453
        },
        {
          "concept": "metrics",
          "relevance": 0.452
        },
        {
          "concept": "machine",
          "relevance": 0.45
        },
        {
          "concept": "task",
          "relevance": 0.447
        },
        {
          "concept": "robustness",
          "relevance": 0.443
        },
        {
          "concept": "library",
          "relevance": 0.438
        },
        {
          "concept": "interpretation tools",
          "relevance": 0.435
        },
        {
          "concept": "model",
          "relevance": 0.415
        },
        {
          "concept": "performance",
          "relevance": 0.411
        },
        {
          "concept": "taxonomy",
          "relevance": 0.4
        },
        {
          "concept": "decision",
          "relevance": 0.394
        },
        {
          "concept": "research",
          "relevance": 0.386
        },
        {
          "concept": "tools",
          "relevance": 0.374
        },
        {
          "concept": "connection",
          "relevance": 0.37
        },
        {
          "concept": "interpreting results",
          "relevance": 0.363
        },
        {
          "concept": "evaluation",
          "relevance": 0.362
        },
        {
          "concept": "design",
          "relevance": 0.361
        },
        {
          "concept": "prediction",
          "relevance": 0.351
        },
        {
          "concept": "interpretation",
          "relevance": 0.349
        },
        {
          "concept": "handling",
          "relevance": 0.346
        },
        {
          "concept": "efforts",
          "relevance": 0.331
        },
        {
          "concept": "results",
          "relevance": 0.314
        },
        {
          "concept": "perspective",
          "relevance": 0.307
        },
        {
          "concept": "people",
          "relevance": 0.304
        },
        {
          "concept": "nature",
          "relevance": 0.266
        },
        {
          "concept": "survey",
          "relevance": 0.245
        },
        {
          "concept": "years",
          "relevance": 0.194
        },
        {
          "concept": "factors",
          "relevance": 0.192
        },
        {
          "concept": "approach",
          "relevance": 0.147
        }
      ]
    },
    {
      "paperId": "pub.1139691916",
      "doi": "10.1038/s41586-021-03819-2",
      "title": "Highly accurate protein structure prediction with AlphaFold",
      "year": 2021,
      "citationCount": 33329,
      "fieldCitationRatio": 5666.48,
      "abstract": "Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1, 2, 3–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10, 11, 12, 13–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.",
      "reference_ids": [
        "pub.1039902343",
        "pub.1113529218",
        "pub.1017098509",
        "pub.1026577191",
        "pub.1105107305",
        "pub.1129913565",
        "pub.1129666117",
        "pub.1136931755",
        "pub.1137973267",
        "pub.1130863323",
        "pub.1002464874",
        "pub.1010491660",
        "pub.1139853407",
        "pub.1044743857",
        "pub.1124097668",
        "pub.1015084295",
        "pub.1093359587",
        "pub.1128944684",
        "pub.1062507831",
        "pub.1120339347",
        "pub.1020261308",
        "pub.1120005283",
        "pub.1020707112",
        "pub.1059981028",
        "pub.1132274683",
        "pub.1049629118",
        "pub.1044535889",
        "pub.1119798172",
        "pub.1127247391",
        "pub.1090894993",
        "pub.1004056762",
        "pub.1011732715",
        "pub.1121530095",
        "pub.1128226003",
        "pub.1132138203",
        "pub.1053316395",
        "pub.1121331236",
        "pub.1135711401",
        "pub.1138192175",
        "pub.1094368585",
        "pub.1132945021",
        "pub.1092237583",
        "pub.1002561157",
        "pub.1117479822",
        "pub.1011422668",
        "pub.1132678305",
        "pub.1138201349",
        "pub.1030443070",
        "pub.1023788906",
        "pub.1122056716",
        "pub.1107464011",
        "pub.1120724709",
        "pub.1125154578",
        "pub.1049361146",
        "pub.1121962449",
        "pub.1120335455",
        "pub.1106367462",
        "pub.1123789520",
        "pub.1130045217",
        "pub.1139365970",
        "pub.1121657304",
        "pub.1006999469",
        "pub.1136703630",
        "pub.1123471089",
        "pub.1053037263",
        "pub.1001034143",
        "pub.1027955415",
        "pub.1121005879",
        "pub.1023778044",
        "pub.1033216981",
        "pub.1134000390",
        "pub.1061743753",
        "pub.1033518395"
      ],
      "concepts_scores": [
        {
          "concept": "protein structure prediction",
          "relevance": 0.791
        },
        {
          "concept": "protein structure",
          "relevance": 0.748
        },
        {
          "concept": "structure prediction",
          "relevance": 0.718
        },
        {
          "concept": "Critical Assessment of protein Structure Prediction",
          "relevance": 0.712
        },
        {
          "concept": "Assessment of protein Structure Prediction",
          "relevance": 0.709
        },
        {
          "concept": "accurate protein structure prediction",
          "relevance": 0.704
        },
        {
          "concept": "multi-sequence alignment",
          "relevance": 0.684
        },
        {
          "concept": "structural bioinformatics",
          "relevance": 0.63
        },
        {
          "concept": "atomic accuracy",
          "relevance": 0.629
        },
        {
          "concept": "three-dimensional structure",
          "relevance": 0.618
        },
        {
          "concept": "unique proteins",
          "relevance": 0.61
        },
        {
          "concept": "homologous structures",
          "relevance": 0.608
        },
        {
          "concept": "AlphaFold",
          "relevance": 0.603
        },
        {
          "concept": "accurate computational approach",
          "relevance": 0.601
        },
        {
          "concept": "biological knowledge",
          "relevance": 0.601
        },
        {
          "concept": "protein",
          "relevance": 0.575
        },
        {
          "concept": "structural coverage",
          "relevance": 0.544
        },
        {
          "concept": "computational approach",
          "relevance": 0.52
        },
        {
          "concept": "experimental structures",
          "relevance": 0.506
        },
        {
          "concept": "computational methods",
          "relevance": 0.488
        },
        {
          "concept": "bioinformatics",
          "relevance": 0.481
        },
        {
          "concept": "amino",
          "relevance": 0.468
        },
        {
          "concept": "critical assessment",
          "relevance": 0.452
        },
        {
          "concept": "structure",
          "relevance": 0.379
        },
        {
          "concept": "machine learning approach",
          "relevance": 0.375
        },
        {
          "concept": "network-based model",
          "relevance": 0.369
        },
        {
          "concept": "alignment",
          "relevance": 0.363
        },
        {
          "concept": "fraction",
          "relevance": 0.329
        },
        {
          "concept": "function",
          "relevance": 0.327
        },
        {
          "concept": "redesigned version",
          "relevance": 0.314
        },
        {
          "concept": "components",
          "relevance": 0.308
        },
        {
          "concept": "prediction",
          "relevance": 0.303
        },
        {
          "concept": "approach",
          "relevance": 0.287
        },
        {
          "concept": "method",
          "relevance": 0.268
        },
        {
          "concept": "knowledge",
          "relevance": 0.267
        },
        {
          "concept": "High",
          "relevance": 0.25
        },
        {
          "concept": "predictable component",
          "relevance": 0.24
        },
        {
          "concept": "learning approach",
          "relevance": 0.236
        },
        {
          "concept": "neural network-based model",
          "relevance": 0.229
        },
        {
          "concept": "coverage",
          "relevance": 0.223
        },
        {
          "concept": "model",
          "relevance": 0.223
        },
        {
          "concept": "cases",
          "relevance": 0.221
        },
        {
          "concept": "years",
          "relevance": 0.22
        },
        {
          "concept": "life",
          "relevance": 0.215
        },
        {
          "concept": "gap",
          "relevance": 0.195
        },
        {
          "concept": "deep learning algorithms",
          "relevance": 0.191
        },
        {
          "concept": "accuracy",
          "relevance": 0.184
        },
        {
          "concept": "version",
          "relevance": 0.184
        },
        {
          "concept": "learning algorithms",
          "relevance": 0.177
        },
        {
          "concept": "months",
          "relevance": 0.173
        },
        {
          "concept": "criticism",
          "relevance": 0.15
        },
        {
          "concept": "design",
          "relevance": 0.146
        },
        {
          "concept": "algorithm",
          "relevance": 0.142
        },
        {
          "concept": "machine",
          "relevance": 0.141
        }
      ]
    },
    {
      "paperId": "pub.1130863323",
      "doi": "10.1038/s41586-020-2649-2",
      "title": "Array programming with NumPy",
      "year": 2020,
      "citationCount": 19840,
      "fieldCitationRatio": 9537.46,
      "abstract": "Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.",
      "reference_ids": [
        "pub.1061398153",
        "pub.1061382601",
        "pub.1061398162",
        "pub.1040656750",
        "pub.1105400113",
        "pub.1046599992",
        "pub.1083719304",
        "pub.1061398358",
        "pub.1011232850",
        "pub.1058083062",
        "pub.1013799887",
        "pub.1112927257",
        "pub.1061398157",
        "pub.1046502114",
        "pub.1058368556",
        "pub.1071473569",
        "pub.1061398159",
        "pub.1063164528",
        "pub.1104984064",
        "pub.1061398156",
        "pub.1094933204",
        "pub.1061398464",
        "pub.1124547608",
        "pub.1061398357",
        "pub.1056920695",
        "pub.1003579700",
        "pub.1014615184",
        "pub.1107762488",
        "pub.1125915376",
        "pub.1041692893",
        "pub.1093917619",
        "pub.1058084068",
        "pub.1061398048",
        "pub.1032133855",
        "pub.1084657683"
      ],
      "concepts_scores": [
        {
          "concept": "scientific Python ecosystem",
          "relevance": 0.658
        },
        {
          "concept": "numpy-like interface",
          "relevance": 0.649
        },
        {
          "concept": "higher-dimensional arrays",
          "relevance": 0.646
        },
        {
          "concept": "Program Library",
          "relevance": 0.614
        },
        {
          "concept": "interoperability layer",
          "relevance": 0.613
        },
        {
          "concept": "software stack",
          "relevance": 0.611
        },
        {
          "concept": "Python ecosystem",
          "relevance": 0.606
        },
        {
          "concept": "computational library",
          "relevance": 0.601
        },
        {
          "concept": "NumPy",
          "relevance": 0.591
        },
        {
          "concept": "array programs",
          "relevance": 0.585
        },
        {
          "concept": "Python language",
          "relevance": 0.585
        },
        {
          "concept": "array objects",
          "relevance": 0.585
        },
        {
          "concept": "flexible framework",
          "relevance": 0.559
        },
        {
          "concept": "expressive syntax",
          "relevance": 0.537
        },
        {
          "concept": "library",
          "relevance": 0.491
        },
        {
          "concept": "interoperability",
          "relevance": 0.473
        },
        {
          "concept": "analysis pipeline",
          "relevance": 0.47
        },
        {
          "concept": "black hole2",
          "relevance": 0.468
        },
        {
          "concept": "scientific data",
          "relevance": 0.455
        },
        {
          "concept": "Python",
          "relevance": 0.454
        },
        {
          "concept": "software",
          "relevance": 0.442
        },
        {
          "concept": "API",
          "relevance": 0.442
        },
        {
          "concept": "syntax",
          "relevance": 0.439
        },
        {
          "concept": "target audience",
          "relevance": 0.434
        },
        {
          "concept": "array",
          "relevance": 0.408
        },
        {
          "concept": "pipeline",
          "relevance": 0.401
        },
        {
          "concept": "framework",
          "relevance": 0.399
        },
        {
          "concept": "language",
          "relevance": 0.397
        },
        {
          "concept": "images",
          "relevance": 0.394
        },
        {
          "concept": "paradigm",
          "relevance": 0.394
        },
        {
          "concept": "geoscience",
          "relevance": 0.388
        },
        {
          "concept": "vector",
          "relevance": 0.387
        },
        {
          "concept": "interface",
          "relevance": 0.369
        },
        {
          "concept": "data",
          "relevance": 0.366
        },
        {
          "concept": "objective",
          "relevance": 0.365
        },
        {
          "concept": "industry analysis",
          "relevance": 0.365
        },
        {
          "concept": "engineering",
          "relevance": 0.362
        },
        {
          "concept": "astronomy",
          "relevance": 0.347
        },
        {
          "concept": "concept",
          "relevance": 0.347
        },
        {
          "concept": "special needs",
          "relevance": 0.345
        },
        {
          "concept": "project",
          "relevance": 0.343
        },
        {
          "concept": "program",
          "relevance": 0.342
        },
        {
          "concept": "stack",
          "relevance": 0.335
        },
        {
          "concept": "research",
          "relevance": 0.322
        },
        {
          "concept": "matrix",
          "relevance": 0.317
        },
        {
          "concept": "needs",
          "relevance": 0.317
        },
        {
          "concept": "science",
          "relevance": 0.314
        },
        {
          "concept": "discovery",
          "relevance": 0.311
        },
        {
          "concept": "materials science",
          "relevance": 0.297
        },
        {
          "concept": "audience",
          "relevance": 0.289
        },
        {
          "concept": "field",
          "relevance": 0.286
        },
        {
          "concept": "position",
          "relevance": 0.284
        },
        {
          "concept": "physics",
          "relevance": 0.275
        },
        {
          "concept": "ecosystem",
          "relevance": 0.275
        },
        {
          "concept": "layer",
          "relevance": 0.273
        },
        {
          "concept": "central position",
          "relevance": 0.268
        },
        {
          "concept": "analysis",
          "relevance": 0.251
        },
        {
          "concept": "waves1",
          "relevance": 0.248
        },
        {
          "concept": "psychology",
          "relevance": 0.238
        },
        {
          "concept": "biology",
          "relevance": 0.234
        },
        {
          "concept": "finance",
          "relevance": 0.229
        },
        {
          "concept": "economics",
          "relevance": 0.224
        },
        {
          "concept": "chemistry",
          "relevance": 0.207
        },
        {
          "concept": "materials",
          "relevance": 0.18
        }
      ]
    },
    {
      "paperId": "pub.1132274683",
      "doi": "10.1093/nar/gkaa1100",
      "title": "UniProt: the universal protein knowledgebase in 2021",
      "year": 2020,
      "citationCount": 6056,
      "fieldCitationRatio": 928.87,
      "abstract": "The aim of the UniProt Knowledgebase is to provide users with a comprehensive, high-quality and freely accessible set of protein sequences annotated with functional information. In this article, we describe significant updates that we have made over the last two years to the resource. The number of sequences in UniProtKB has risen to approximately 190 million, despite continued work to reduce sequence redundancy at the proteome level. We have adopted new methods of assessing proteome completeness and quality. We continue to extract detailed annotations from the literature to add to reviewed entries and supplement these in unreviewed entries with annotations provided by automated systems such as the newly implemented Association-Rule-Based Annotator (ARBA). We have developed a credit-based publication submission interface to allow the community to contribute publications and annotations to UniProt entries. We describe how UniProtKB responded to the COVID-19 pandemic through expert curation of relevant entries that were rapidly made available to the research community through a dedicated portal. UniProt resources are available under a CC-BY (4.0) license via the web at https://www.uniprot.org/.",
      "reference_ids": [
        "pub.1117745172",
        "pub.1106390704",
        "pub.1092391320",
        "pub.1121627726",
        "pub.1084178179",
        "pub.1123934460",
        "pub.1121691423",
        "pub.1121830871",
        "pub.1124128788",
        "pub.1129551610",
        "pub.1079396620",
        "pub.1053526431",
        "pub.1014059422",
        "pub.1029002744",
        "pub.1107772714",
        "pub.1121586176",
        "pub.1125857564",
        "pub.1022885794",
        "pub.1112583196",
        "pub.1099877235",
        "pub.1127429464",
        "pub.1121119180",
        "pub.1000697842",
        "pub.1050283464",
        "pub.1083699903",
        "pub.1122232830",
        "pub.1045704939",
        "pub.1112862254",
        "pub.1121860123",
        "pub.1123480872",
        "pub.1107714438",
        "pub.1084602545",
        "pub.1127675249",
        "pub.1020212128",
        "pub.1043574274",
        "pub.1105107305",
        "pub.1015833174",
        "pub.1107690601",
        "pub.1017333655",
        "pub.1035157869",
        "pub.1121946191",
        "pub.1107235473",
        "pub.1125697066",
        "pub.1114150484",
        "pub.1103972998",
        "pub.1121802687",
        "pub.1107872517",
        "pub.1022229472"
      ],
      "concepts_scores": [
        {
          "concept": "reduce sequence redundancy",
          "relevance": 0.637
        },
        {
          "concept": "UniProt resources",
          "relevance": 0.598
        },
        {
          "concept": "UniProt entries",
          "relevance": 0.592
        },
        {
          "concept": "UniProt Knowledgebase",
          "relevance": 0.591
        },
        {
          "concept": "sequence redundancy",
          "relevance": 0.591
        },
        {
          "concept": "protein sequences",
          "relevance": 0.59
        },
        {
          "concept": "proteome level",
          "relevance": 0.578
        },
        {
          "concept": "expert curation",
          "relevance": 0.561
        },
        {
          "concept": "UniProtKB",
          "relevance": 0.538
        },
        {
          "concept": "proteomics",
          "relevance": 0.518
        },
        {
          "concept": "annotation",
          "relevance": 0.517
        },
        {
          "concept": "functional information",
          "relevance": 0.498
        },
        {
          "concept": "sequence",
          "relevance": 0.497
        },
        {
          "concept": "automated system",
          "relevance": 0.489
        },
        {
          "concept": "research community",
          "relevance": 0.486
        },
        {
          "concept": "CC-BY",
          "relevance": 0.468
        },
        {
          "concept": "UniProt",
          "relevance": 0.461
        },
        {
          "concept": "reviewed entries",
          "relevance": 0.46
        },
        {
          "concept": "protein",
          "relevance": 0.431
        },
        {
          "concept": "relevant entries",
          "relevance": 0.423
        },
        {
          "concept": "knowledgebase",
          "relevance": 0.409
        },
        {
          "concept": "users",
          "relevance": 0.404
        },
        {
          "concept": "Web",
          "relevance": 0.385
        },
        {
          "concept": "redundancy",
          "relevance": 0.381
        },
        {
          "concept": "entry",
          "relevance": 0.381
        },
        {
          "concept": "high-quality",
          "relevance": 0.378
        },
        {
          "concept": "resources",
          "relevance": 0.376
        },
        {
          "concept": "community",
          "relevance": 0.369
        },
        {
          "concept": "update",
          "relevance": 0.367
        },
        {
          "concept": "portal",
          "relevance": 0.351
        },
        {
          "concept": "information",
          "relevance": 0.348
        },
        {
          "concept": "supplementation",
          "relevance": 0.322
        },
        {
          "concept": "license",
          "relevance": 0.322
        },
        {
          "concept": "system",
          "relevance": 0.315
        },
        {
          "concept": "method",
          "relevance": 0.304
        },
        {
          "concept": "levels",
          "relevance": 0.298
        },
        {
          "concept": "quality",
          "relevance": 0.296
        },
        {
          "concept": "COVID-19 pandemic",
          "relevance": 0.285
        },
        {
          "concept": "research",
          "relevance": 0.281
        },
        {
          "concept": "publications",
          "relevance": 0.272
        },
        {
          "concept": "completion",
          "relevance": 0.263
        },
        {
          "concept": "COVID-19",
          "relevance": 0.262
        },
        {
          "concept": "Arba",
          "relevance": 0.249
        },
        {
          "concept": "literature",
          "relevance": 0.24
        },
        {
          "concept": "years",
          "relevance": 0.206
        },
        {
          "concept": "pandemic",
          "relevance": 0.2
        }
      ]
    },
    {
      "paperId": "pub.1135553216",
      "doi": "10.1145/3446776",
      "title": "Understanding deep learning (still) requires rethinking generalization",
      "year": 2021,
      "citationCount": 2083,
      "fieldCitationRatio": 649.16,
      "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.",
      "reference_ids": [
        "pub.1019980111",
        "pub.1119806343",
        "pub.1010886364",
        "pub.1001763873",
        "pub.1037023029",
        "pub.1061100600",
        "pub.1023250347",
        "pub.1062999168",
        "pub.1128280227",
        "pub.1039823676",
        "pub.1091590035"
      ],
      "concepts_scores": [
        {
          "concept": "neural network",
          "relevance": 0.742
        },
        {
          "concept": "state-of-the-art convolutional networks",
          "relevance": 0.706
        },
        {
          "concept": "deep artificial neural networks",
          "relevance": 0.677
        },
        {
          "concept": "stochastic gradient method",
          "relevance": 0.64
        },
        {
          "concept": "convolutional network",
          "relevance": 0.622
        },
        {
          "concept": "image classification",
          "relevance": 0.62
        },
        {
          "concept": "generalization error",
          "relevance": 0.619
        },
        {
          "concept": "training data",
          "relevance": 0.616
        },
        {
          "concept": "deep learning",
          "relevance": 0.615
        },
        {
          "concept": "artificial neural network",
          "relevance": 0.613
        },
        {
          "concept": "explicit regularization",
          "relevance": 0.603
        },
        {
          "concept": "regularization technique",
          "relevance": 0.589
        },
        {
          "concept": "network",
          "relevance": 0.562
        },
        {
          "concept": "random noise",
          "relevance": 0.561
        },
        {
          "concept": "massive size",
          "relevance": 0.557
        },
        {
          "concept": "traditional approaches",
          "relevance": 0.536
        },
        {
          "concept": "random labeling",
          "relevance": 0.527
        },
        {
          "concept": "model family",
          "relevance": 0.518
        },
        {
          "concept": "gradient method",
          "relevance": 0.514
        },
        {
          "concept": "data points",
          "relevance": 0.506
        },
        {
          "concept": "systematic experiments",
          "relevance": 0.493
        },
        {
          "concept": "training",
          "relevance": 0.488
        },
        {
          "concept": "traditional models",
          "relevance": 0.486
        },
        {
          "concept": "original version",
          "relevance": 0.484
        },
        {
          "concept": "regularization",
          "relevance": 0.479
        },
        {
          "concept": "images",
          "relevance": 0.465
        },
        {
          "concept": "classification",
          "relevance": 0.437
        },
        {
          "concept": "learning",
          "relevance": 0.434
        },
        {
          "concept": "noise",
          "relevance": 0.429
        },
        {
          "concept": "performance",
          "relevance": 0.405
        },
        {
          "concept": "error",
          "relevance": 0.404
        },
        {
          "concept": "experimental findings",
          "relevance": 0.401
        },
        {
          "concept": "generalization",
          "relevance": 0.395
        },
        {
          "concept": "experiments",
          "relevance": 0.391
        },
        {
          "concept": "model",
          "relevance": 0.39
        },
        {
          "concept": "labeling",
          "relevance": 0.39
        },
        {
          "concept": "data",
          "relevance": 0.372
        },
        {
          "concept": "version",
          "relevance": 0.371
        },
        {
          "concept": "technique",
          "relevance": 0.362
        },
        {
          "concept": "method",
          "relevance": 0.356
        },
        {
          "concept": "theoretical constructs",
          "relevance": 0.329
        },
        {
          "concept": "point",
          "relevance": 0.324
        },
        {
          "concept": "construction",
          "relevance": 0.311
        },
        {
          "concept": "practice",
          "relevance": 0.302
        },
        {
          "concept": "parameters",
          "relevance": 0.294
        },
        {
          "concept": "field",
          "relevance": 0.291
        },
        {
          "concept": "test performance",
          "relevance": 0.283
        },
        {
          "concept": "gap",
          "relevance": 0.281
        },
        {
          "concept": "comparison",
          "relevance": 0.278
        },
        {
          "concept": "size",
          "relevance": 0.263
        },
        {
          "concept": "section",
          "relevance": 0.242
        },
        {
          "concept": "phenomenon",
          "relevance": 0.235
        },
        {
          "concept": "properties",
          "relevance": 0.232
        },
        {
          "concept": "Republic",
          "relevance": 0.192
        },
        {
          "concept": "findings",
          "relevance": 0.191
        },
        {
          "concept": "expression",
          "relevance": 0.159
        },
        {
          "concept": "family",
          "relevance": 0.144
        },
        {
          "concept": "approach",
          "relevance": 0.143
        }
      ]
    },
    {
      "paperId": "pub.1023250347",
      "doi": "10.1007/bf02551274",
      "title": "Approximation by superpositions of a sigmoidal function",
      "year": 1989,
      "citationCount": 11023,
      "fieldCitationRatio": NaN,
      "abstract": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.",
      "reference_ids": [
        "pub.1034169987",
        "pub.1040529426",
        "pub.1061162954",
        "pub.1061385413",
        "pub.1064408863",
        "pub.1073139110",
        "pub.1120682771",
        "pub.1086206762",
        "pub.1038881641",
        "pub.1061144285",
        "pub.1062855657",
        "pub.1035113948",
        "pub.1053187839",
        "pub.1089196220"
      ],
      "concepts_scores": [
        {
          "concept": "neural network",
          "relevance": 0.747
        },
        {
          "concept": "arbitrary decision regions",
          "relevance": 0.684
        },
        {
          "concept": "feedforward neural network",
          "relevance": 0.674
        },
        {
          "concept": "decision regions",
          "relevance": 0.619
        },
        {
          "concept": "artificial neural network",
          "relevance": 0.618
        },
        {
          "concept": "hidden layer",
          "relevance": 0.616
        },
        {
          "concept": "sigmoidal nonlinearity",
          "relevance": 0.594
        },
        {
          "concept": "univariate functions",
          "relevance": 0.577
        },
        {
          "concept": "sigmoid function",
          "relevance": 0.57
        },
        {
          "concept": "network",
          "relevance": 0.556
        },
        {
          "concept": "unit hypercube",
          "relevance": 0.55
        },
        {
          "concept": "finite linear combination",
          "relevance": 0.538
        },
        {
          "concept": "approximation properties",
          "relevance": 0.532
        },
        {
          "concept": "affine functions",
          "relevance": 0.518
        },
        {
          "concept": "hypercube",
          "relevance": 0.434
        },
        {
          "concept": "representation",
          "relevance": 0.434
        },
        {
          "concept": "approximation",
          "relevance": 0.404
        },
        {
          "concept": "function",
          "relevance": 0.368
        },
        {
          "concept": "real variability",
          "relevance": 0.335
        },
        {
          "concept": "mild conditions",
          "relevance": 0.324
        },
        {
          "concept": "results",
          "relevance": 0.312
        },
        {
          "concept": "nonlinearity",
          "relevance": 0.311
        },
        {
          "concept": "nonlinearities",
          "relevance": 0.308
        },
        {
          "concept": "superposition",
          "relevance": 0.283
        },
        {
          "concept": "layer",
          "relevance": 0.28
        },
        {
          "concept": "units",
          "relevance": 0.24
        },
        {
          "concept": "combination of composition",
          "relevance": 0.239
        },
        {
          "concept": "properties",
          "relevance": 0.233
        },
        {
          "concept": "variables",
          "relevance": 0.23
        },
        {
          "concept": "conditions",
          "relevance": 0.207
        },
        {
          "concept": "region",
          "relevance": 0.199
        },
        {
          "concept": "composition",
          "relevance": 0.151
        }
      ]
    },
    {
      "paperId": "pub.1019980111",
      "doi": "10.1007/3-540-44581-1_27",
      "title": "A Generalized Representer Theorem",
      "year": 2001,
      "citationCount": 1148,
      "fieldCitationRatio": NaN,
      "abstract": "Wahba’s classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.",
      "reference_ids": [
        "pub.1027312764",
        "pub.1050102216",
        "pub.1151918246",
        "pub.1001915196",
        "pub.1036184584",
        "pub.1064408465",
        "pub.1110614774",
        "pub.1098555835",
        "pub.1110614777",
        "pub.1011234020",
        "pub.1036379424",
        "pub.1064398840",
        "pub.1052917720",
        "pub.1004501447",
        "pub.1151918230",
        "pub.1151918232"
      ],
      "concepts_scores": [
        {
          "concept": "risk minimization problem",
          "relevance": 0.639
        },
        {
          "concept": "feature space",
          "relevance": 0.637
        },
        {
          "concept": "risk term",
          "relevance": 0.614
        },
        {
          "concept": "training examples",
          "relevance": 0.61
        },
        {
          "concept": "theorem states",
          "relevance": 0.606
        },
        {
          "concept": "larger class",
          "relevance": 0.593
        },
        {
          "concept": "minimization problem",
          "relevance": 0.584
        },
        {
          "concept": "quadratic regularization",
          "relevance": 0.584
        },
        {
          "concept": "kernel algorithm",
          "relevance": 0.574
        },
        {
          "concept": "optimal solution",
          "relevance": 0.546
        },
        {
          "concept": "regularization",
          "relevance": 0.512
        },
        {
          "concept": "examples",
          "relevance": 0.502
        },
        {
          "concept": "kernel",
          "relevance": 0.496
        },
        {
          "concept": "space",
          "relevance": 0.495
        },
        {
          "concept": "theorem",
          "relevance": 0.474
        },
        {
          "concept": "Wahba",
          "relevance": 0.453
        },
        {
          "concept": "problem",
          "relevance": 0.451
        },
        {
          "concept": "term",
          "relevance": 0.441
        },
        {
          "concept": "solution",
          "relevance": 0.435
        },
        {
          "concept": "features",
          "relevance": 0.399
        },
        {
          "concept": "training",
          "relevance": 0.392
        },
        {
          "concept": "algorithm",
          "relevance": 0.389
        },
        {
          "concept": "class",
          "relevance": 0.388
        },
        {
          "concept": "dimensionality",
          "relevance": 0.381
        },
        {
          "concept": "expansion",
          "relevance": 0.354
        },
        {
          "concept": "state",
          "relevance": 0.27
        },
        {
          "concept": "results",
          "relevance": 0.261
        },
        {
          "concept": "span",
          "relevance": 0.183
        },
        {
          "concept": "risk",
          "relevance": 0.139
        }
      ]
    }
  ],
  "evolution_links": [
    {
      "source": "pub.1157336399",
      "target": "pub.1151380649",
      "source_title": "Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence",
      "target_title": "High-Resolution Image Synthesis with Latent Diffusion Models"
    },
    {
      "source": "pub.1151380649",
      "target": "pub.1095689025",
      "source_title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "target_title": "ImageNet: A large-scale hierarchical image database"
    },
    {
      "source": "pub.1095689025",
      "target": "pub.1052687286",
      "source_title": "ImageNet: A large-scale hierarchical image database",
      "target_title": "Distinctive Image Features from Scale-Invariant Keypoints"
    },
    {
      "source": "pub.1095689025",
      "target": "pub.1027534025",
      "source_title": "ImageNet: A large-scale hierarchical image database",
      "target_title": "LabelMe: A Database and Web-Based Tool for Image Annotation"
    },
    {
      "source": "pub.1151380649",
      "target": "pub.1095850445",
      "source_title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "target_title": "Image-to-Image Translation with Conditional Adversarial Networks"
    },
    {
      "source": "pub.1095850445",
      "target": "pub.1061640964",
      "source_title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "target_title": "Image Quality Assessment: From Error Visibility to Structural Similarity"
    },
    {
      "source": "pub.1095850445",
      "target": "pub.1093626237",
      "source_title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "target_title": "Fully Convolutional Networks for Semantic Segmentation"
    },
    {
      "source": "pub.1157336399",
      "target": "pub.1150997559",
      "source_title": "Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence",
      "target_title": "Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond"
    },
    {
      "source": "pub.1150997559",
      "target": "pub.1139691916",
      "source_title": "Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond",
      "target_title": "Highly accurate protein structure prediction with AlphaFold"
    },
    {
      "source": "pub.1139691916",
      "target": "pub.1130863323",
      "source_title": "Highly accurate protein structure prediction with AlphaFold",
      "target_title": "Array programming with NumPy"
    },
    {
      "source": "pub.1139691916",
      "target": "pub.1132274683",
      "source_title": "Highly accurate protein structure prediction with AlphaFold",
      "target_title": "UniProt: the universal protein knowledgebase in 2021"
    },
    {
      "source": "pub.1150997559",
      "target": "pub.1135553216",
      "source_title": "Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond",
      "target_title": "Understanding deep learning (still) requires rethinking generalization"
    },
    {
      "source": "pub.1135553216",
      "target": "pub.1023250347",
      "source_title": "Understanding deep learning (still) requires rethinking generalization",
      "target_title": "Approximation by superpositions of a sigmoidal function"
    },
    {
      "source": "pub.1135553216",
      "target": "pub.1019980111",
      "source_title": "Understanding deep learning (still) requires rethinking generalization",
      "target_title": "A Generalized Representer Theorem"
    }
  ]
}