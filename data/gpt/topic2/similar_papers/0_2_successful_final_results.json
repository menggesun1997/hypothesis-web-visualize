{
  "before_idea": {
    "title": "Semantic-Enriched Self-Supervised Framework for Aphasia Subtyping",
    "Problem_Statement": "Clinical language disorder modeling suffers from limited annotated data and poor semantic integration, reducing LLM interpretability and diagnostic accuracy for disorders like aphasia.",
    "Motivation": "Targets internal gap of data scarcity and external gap around applying annotation frameworks from low-resource languages to improve semantic knowledge embedding within clinical language models, enhancing interpretability and efficacy.",
    "Proposed_Method": "Develop a self-supervised learning framework that incorporates semantic annotation strategies and cross-lingual transfer from low-resource language models. This includes an intermediate semantic representation layer trained with weak supervision from clinical domain ontologies to enrich syntactic and pragmatic features relevant to aphasia subtyping.",
    "Step_by_Step_Experiment_Plan": "1) Compile multilingual aphasia speech/text datasets with minimal annotations. 2) Annotate partial semantic frames using adapted frameworks from low-resource language research. 3) Pretrain transformer models with masked language and semantic frame prediction tasks. 4) Evaluate model performance on aphasia subtype classification and interpretability via probing tasks.",
    "Test_Case_Examples": "Input: Patient utterance with disfluent speech texts. Output: Aphasia subtype label and highlighting of semantic frame deviations correlated with diagnostic markers.",
    "Fallback_Plan": "If semantic enrichment yields marginal gains, incorporate longitudinal patient data and multimodal cues (e.g., speech melody) to complement text-based features."
  },
  "after_idea": {
    "ideas": [
      {
        "title": "Semantic-Enriched Self-Supervised Framework for Aphasia Subtyping with Explicit Semantic Layer and Multimodal Resilience",
        "Problem_Statement": "Clinical language disorder modeling for aphasia subtyping continues to be challenged by limited annotated datasets and insufficient integration of semantic and pragmatic clinical knowledge. Existing large language models (LLMs) trained with standard masked language model (MLM) objectives inadequately capture clinically relevant semantic distinctions, which reduces interpretability by clinicians and limits diagnostic accuracy. Improving semantic knowledge embedding into clinical language models, while ensuring feasibility and interpretability, remains an open problem.",
        "Motivation": "While competitive self-supervised models exist, their reliance on purely syntactic or surface-level language signals constrains clinical utility for aphasia subtyping. Our approach addresses this by explicitly incorporating semantic frame annotation adapted from low-resource language frameworks—providing clinically meaningful, interpretable intermediate representations. Furthermore, to enhance applicability to real-world diverse populations and clinical settings such as the University Clinics of Kinshasa, we introduce a modular architecture facilitating multimodal integration and robustness to annotation scarcity. This layered architecture and integration strategy represent a novel and practically impactful advance over prior work, enabling improved subtype classification alongside transparent semantic interpretation.",
        "Proposed_Method": "We propose a transformer-based architecture enhanced with a dedicated Semantic Enrichment Layer (SEL) situated between the embedding and transformer encoder stages. The SEL predicts semantic frames derived from a refined clinical semantic ontology tailored for aphasia, trained via weak supervision leveraging partial semantic annotations from domain experts at the University Clinics of Kinshasa and remote clinical collaborators. \n\nConcretely, the input token embeddings are first processed by the SEL, which uses a multi-head classification head to predict frame elements (e.g., Agents, Actions, Affected Entities) according to the clinical semantic frames schema. These frame predictions are then concatenated as enriched semantic embeddings and fed into the transformer encoder alongside original embeddings. \n\nThe pretraining objective combines standard masked language modeling (MLM) with a semantic frame prediction (SFP) loss applied to SEL outputs, allowing the model to simultaneously learn lexical context and clinically meaningful semantic distinctions. This dual-objective training explicitly biases the model to embed semantic knowledge complementary to syntactic signals.\n\nThis architecture differs from traditional MLM-only approaches by explicitly disentangling semantic frame predictions in a dedicated layer, enabling clearer diagnostic interpretation. Semantic frame outputs can be directly inspected and correlated with clinical markers for aphasia subtype, enhancing interpretability. \n\nData flow is as follows: patient utterances (text/speech transcripts) --> embedding layer --> SEL predicts partial semantic frames --> enriched embeddings concatenated with raw embeddings --> transformer encoder --> contextualized outputs used for downstream aphasia subtype classification. \n\nThe SEL also facilitates multimodal fusion by allowing integration of aligned acoustic-prosodic features (e.g., speech melody embeddings from speech analysis modules) concatenated at the SEL stage, enabling richer, multimodal semantic representations if needed.\n\nA detailed model architecture diagram and data flow schematic, including SEL integration points and objective functions, will be provided for reproducibility and clarity.",
        "Step_by_Step_Experiment_Plan": "1) Dataset Compilation and Annotation:\n - Aggregate multilingual aphasia speech and text transcript datasets, including populations from the University Clinics of Kinshasa to enhance global representation.\n - Collaborate with domain experts (speech therapists, neurologists) to annotate a substantial subset with partial clinical semantic frames, focusing on key frame elements relevant to aphasia diagnosis.\n - Establish annotation guidelines and quality control protocols including double annotation and adjudication to ensure consistency and reduce noise.\n\n2) Model Pretraining and SEL Development:\n - Implement the SEL module as a multi-head classifier predicting clinical semantic frame elements.\n - Pretrain the model with combined MLM and SFP objectives.\n - Experiment with different degrees of annotation availability, including semi-supervised weak supervision approaches to leverage unannotated data.\n\n3) Evaluation:\n - Quantitatively evaluate aphasia subtype classification accuracy against baseline MLM-only transformers.\n - Perform interpretability probing using specialized datasets targeting semantic and pragmatic phenomena relevant to aphasia, such as agent-patient role identification and pragmatic inference errors.\n - Use probing tasks adapted from clinical NLP benchmarks and ablation studies to measure SEL contribution.\n\n4) Fallback and Robustness:\n - If semantic enrichment alone yields limited gains, integrate multimodal acoustic features (e.g., speech melody embeddings extracted via open-source prosody analysis tools) at the SEL fusion point.\n - Retrain and evaluate multimodal models, assessing impact on robustness and classification.\n\n5) Reporting and Reproducibility:\n - Release detailed annotation guidelines, SEL architecture, and training scripts.\n - Provide a comprehensive error analysis correlating frame prediction errors with clinical diagnostic challenges.",
        "Test_Case_Examples": "Example Input: Patient utterance transcript exhibiting disfluent, agrammatic speech.\n\nOutput:\n - Aphasia subtype label: e.g., 'Broca's Aphasia'\n - Visualization of predicted semantic frame with highlighted frame elements and their predicted values, e.g., Agent='Patient', Action='Attempted Verb', Affected Entity='Object', showing deviations or omissions.\n - Confidence scores for subtype classification and frame element predictions.\n\nInterpretation:\n - Clinician can inspect frame prediction mismatches (e.g., missing Agents or incomplete Actions), which correspond to known clinical markers for the subtype.\n\nAdditional Example:\n - Patient utterance in Lingala, demonstrating framework's cross-lingual capacity through partial semantic annotations adapted from low-resource language strategies, evidencing broad applicability.",
        "Fallback_Plan": "If initial semantic enrichment via the SEL module fails to produce meaningful improvements, we will pivot to a multimodal integration approach. This involves complementing text-based features with speech acoustic-prosodic cues such as pitch, tempo, and intonation contours extracted via open-source speech analysis tools. These prosodic embeddings will be fused at the SEL stage as additional modality embeddings, enabling the model to capture speech melody disturbances typical in aphasia subtypes.\n\nThe multimodal training pipeline includes aligned audio-text data preprocessing, modal-specific encoders feeding into the SEL fusion layer, and joint optimization of all modalities under the MLM and semantic frame plus prosody prediction objectives. This strategy improves model robustness to annotation scarcity and noisy text data, ensures resilience in low-resource clinical settings, and expands the interpretability scope to include speech melody markers critical for accurate subtyping."
      }
    ]
  },
  "feedback_results": {
    "keywords_query": [
      "Semantic-Enriched Framework",
      "Self-Supervised Learning",
      "Aphasia Subtyping",
      "Data Scarcity",
      "Clinical Language Models",
      "Interpretability"
    ],
    "direct_cooccurrence_count": 82,
    "min_pmi_score_value": 3.376474293736211,
    "avg_pmi_score_value": 5.25595380544723,
    "novelty": "NOV-COMPETITIVE",
    "future_suggestions_categories": [
      "3215 Reproductive Medicine",
      "32 Biomedical and Clinical Sciences",
      "3202 Clinical Sciences"
    ],
    "future_suggestions_concepts": [
      "University Clinics of Kinshasa"
    ],
    "internal_review": {
      "critiques": [
        {
          "feedback_code": "SOU-MECHANISM",
          "feedback_content": "The proposed method introduces an intermediate semantic representation layer trained with weak supervision from clinical ontologies, merging syntactic and pragmatic features. However, the mechanism by which semantic enrichment concretely enhances aphasia subtype classification and model interpretability is not fully detailed. Clarify the architecture of this semantic layer, how semantic frames are integrated into the transformer pretraining objectives, and how these representations differ or improve over standard masked language model pretraining. Articulating the connection between semantic frame prediction and downstream diagnostic tasks would strengthen confidence in soundness and potential impact of the approach, reducing ambiguity about how semantic knowledge effectively complements clinical language disorder modeling rather than adding complexity without clear gains. This elaboration is essential to validate the core assumption that semantic enrichment will lead to improved interpretability and classification accuracy for aphasia subtypes, given existing competitive methods that already leverage self-supervised models on clinical data with various annotations. Without this clarity, there is risk the mechanism remains too abstract and under-specified for reproducibility and evaluation by reviewers or practitioners familiar with clinical NLP and speech disorders research. Please provide a diagram or algorithmic description illustrating data flow and semantic feature fusion in the model architecture to better ground this proposal's mechanism section in sound engineering principles and clinical relevance evidence. \n\nTarget section: Proposed_Method"
        },
        {
          "feedback_code": "FEA-EXPERIMENT",
          "feedback_content": "The step-by-step experimental plan lays out a reasonable roadmap of multilingual data compilation, partial semantic frame annotation, pretraining with dual masked language and semantic frame prediction objectives, and evaluation on subtype classification and interpretability probing. However, critical feasibility concerns remain regarding the scalability and validity of semantic annotation adapted from low-resource language frameworks to clinical aphasia datasets, which are limited and noisy. Given minimal existing annotations, the methodology for reliably annotating partial semantic frames needs concrete description—are domain experts involved, what quality control measures ensure annotation consistency, and how labor-intensive is this process? Without addressing these challenges, the plan risks infeasibility or suboptimal semantic annotations that could hinder model effectiveness. Furthermore, evaluation metrics for interpretability via probing tasks require specification: which probing datasets, linguistic phenomena, or semantic categories will be assessed? Lastly, the fallback plan involving multimodal cues like speech melody is promising but underdeveloped — clarifying how these modalities will be integrated in model architecture and training pipelines would improve practical resilience if semantic enrichment alone is insufficient. Strengthening these feasibility aspects will substantiate the experimental approach's viability and robustness to dataset and annotation constraints typical in clinical language disorder domains. \n\nTarget section: Step_by_Step_Experiment_Plan"
        }
      ]
    }
  }
}