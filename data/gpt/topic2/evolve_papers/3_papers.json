{
  "papers": [
    {
      "paperId": "pub.1170882028",
      "doi": "10.1007/s10994-024-06543-w",
      "title": "A survey on interpretable reinforcement learning",
      "year": 2024,
      "citationCount": 37,
      "fieldCitationRatio": NaN,
      "abstract": "Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as an intrinsic property of a model) and explainability (as a post-hoc operation) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions, notably related to the recent development of foundation models (e.g., large language models, RL from human feedback).",
      "reference_ids": [
        "pub.1128856123",
        "pub.1111918634",
        "pub.1136349979",
        "pub.1061717465",
        "pub.1123511716",
        "pub.1041237883",
        "pub.1099151330",
        "pub.1098662202",
        "pub.1140876140",
        "pub.1094884403",
        "pub.1166483990",
        "pub.1040632585",
        "pub.1112771687",
        "pub.1092261951",
        "pub.1096023874",
        "pub.1123328476",
        "pub.1100555394",
        "pub.1105690160",
        "pub.1107865769",
        "pub.1116885251",
        "pub.1122194839",
        "pub.1128853006",
        "pub.1095600500",
        "pub.1120932873",
        "pub.1007561122",
        "pub.1105579420",
        "pub.1149215034",
        "pub.1000719108",
        "pub.1148916557",
        "pub.1150865751",
        "pub.1099742746",
        "pub.1122136951",
        "pub.1148956311",
        "pub.1011365177",
        "pub.1042756296",
        "pub.1150866888",
        "pub.1052958504",
        "pub.1034392970",
        "pub.1008662143",
        "pub.1039307785",
        "pub.1000965123",
        "pub.1048089995",
        "pub.1130208127",
        "pub.1120776140",
        "pub.1008835835",
        "pub.1095928504",
        "pub.1151387241",
        "pub.1098653549",
        "pub.1004143803",
        "pub.1002567621",
        "pub.1039420619",
        "pub.1122290019",
        "pub.1019776797",
        "pub.1125109262",
        "pub.1128856491",
        "pub.1031107305",
        "pub.1095318469",
        "pub.1130211914",
        "pub.1105690171",
        "pub.1051758467",
        "pub.1030043153",
        "pub.1101183886",
        "pub.1044516014",
        "pub.1094849935",
        "pub.1136564546",
        "pub.1117331023",
        "pub.1128856766",
        "pub.1091209318",
        "pub.1105386822",
        "pub.1123669031",
        "pub.1112615565",
        "pub.1019948494",
        "pub.1130042379",
        "pub.1136277802",
        "pub.1101804800",
        "pub.1124427830",
        "pub.1050398439",
        "pub.1036286740",
        "pub.1094982049",
        "pub.1122857349",
        "pub.1084717270",
        "pub.1062665979",
        "pub.1123307665",
        "pub.1129120143",
        "pub.1132632737",
        "pub.1095811486",
        "pub.1111334708",
        "pub.1121455886",
        "pub.1090643968",
        "pub.1148917162",
        "pub.1111703270",
        "pub.1124308279",
        "pub.1128855984",
        "pub.1050059401",
        "pub.1124940529",
        "pub.1036188787",
        "pub.1013539307",
        "pub.1050695762",
        "pub.1130798225",
        "pub.1117925830",
        "pub.1128675718",
        "pub.1130302878",
        "pub.1034598982",
        "pub.1030517994",
        "pub.1151380649",
        "pub.1135125332",
        "pub.1120914916",
        "pub.1129723978",
        "pub.1120944858",
        "pub.1003987550",
        "pub.1128526715",
        "pub.1134080356",
        "pub.1105386937",
        "pub.1007321847",
        "pub.1150865587",
        "pub.1163376142",
        "pub.1037554913",
        "pub.1099106296",
        "pub.1107164163",
        "pub.1115989812",
        "pub.1111349486",
        "pub.1109496258",
        "pub.1119942144",
        "pub.1025479327",
        "pub.1117659233",
        "pub.1149215103",
        "pub.1128104578",
        "pub.1151386900",
        "pub.1114160691",
        "pub.1124426165",
        "pub.1093361297",
        "pub.1151387247",
        "pub.1148845242",
        "pub.1018866511"
      ],
      "concepts_scores": [
        {
          "concept": "reinforcement learning",
          "relevance": 0.788
        },
        {
          "concept": "sequential decision-making problems",
          "relevance": 0.695
        },
        {
          "concept": "deep reinforcement learning",
          "relevance": 0.685
        },
        {
          "concept": "decision-making problems",
          "relevance": 0.668
        },
        {
          "concept": "high-stakes domains",
          "relevance": 0.668
        },
        {
          "concept": "context of RL",
          "relevance": 0.666
        },
        {
          "concept": "machine learning approach",
          "relevance": 0.643
        },
        {
          "concept": "autonomous driving",
          "relevance": 0.625
        },
        {
          "concept": "learning approach",
          "relevance": 0.605
        },
        {
          "concept": "research area",
          "relevance": 0.562
        },
        {
          "concept": "research directions",
          "relevance": 0.554
        },
        {
          "concept": "learning",
          "relevance": 0.537
        },
        {
          "concept": "medical applications",
          "relevance": 0.509
        },
        {
          "concept": "explainability",
          "relevance": 0.482
        },
        {
          "concept": "deployment",
          "relevance": 0.467
        },
        {
          "concept": "decision-making",
          "relevance": 0.462
        },
        {
          "concept": "machine",
          "relevance": 0.451
        },
        {
          "concept": "scheme",
          "relevance": 0.448
        },
        {
          "concept": "reinforcement",
          "relevance": 0.422
        },
        {
          "concept": "input",
          "relevance": 0.418
        },
        {
          "concept": "model",
          "relevance": 0.396
        },
        {
          "concept": "context",
          "relevance": 0.393
        },
        {
          "concept": "domain",
          "relevance": 0.39
        },
        {
          "concept": "applications",
          "relevance": 0.388
        },
        {
          "concept": "research",
          "relevance": 0.334
        },
        {
          "concept": "overview",
          "relevance": 0.332
        },
        {
          "concept": "needs",
          "relevance": 0.328
        },
        {
          "concept": "interpretation",
          "relevance": 0.324
        },
        {
          "concept": "driving",
          "relevance": 0.317
        },
        {
          "concept": "direction",
          "relevance": 0.305
        },
        {
          "concept": "facets",
          "relevance": 0.286
        },
        {
          "concept": "survey",
          "relevance": 0.285
        },
        {
          "concept": "development",
          "relevance": 0.281
        },
        {
          "concept": "policy needs",
          "relevance": 0.272
        },
        {
          "concept": "area",
          "relevance": 0.262
        },
        {
          "concept": "approach",
          "relevance": 0.197
        },
        {
          "concept": "years",
          "relevance": 0.196
        },
        {
          "concept": "paper",
          "relevance": 0.191
        },
        {
          "concept": "problem",
          "relevance": 0.185
        }
      ]
    },
    {
      "paperId": "pub.1151380649",
      "doi": "10.1109/cvpr52688.2022.01042",
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "year": 2022,
      "citationCount": 10384,
      "fieldCitationRatio": 4077.61,
      "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
      "reference_ids": [
        "pub.1125160158",
        "pub.1095689025",
        "pub.1142386675",
        "pub.1094016389",
        "pub.1110720202",
        "pub.1123987931",
        "pub.1090555548",
        "pub.1095850445",
        "pub.1150931789",
        "pub.1017774818",
        "pub.1138871251",
        "pub.1150866620",
        "pub.1094476634",
        "pub.1123987722",
        "pub.1145901468",
        "pub.1129913318",
        "pub.1124467986",
        "pub.1128857248",
        "pub.1142373977",
        "pub.1129913308",
        "pub.1110720266",
        "pub.1145639545"
      ],
      "concepts_scores": [
        {
          "concept": "image synthesis",
          "relevance": 0.731
        },
        {
          "concept": "text-to-image synthesis",
          "relevance": 0.709
        },
        {
          "concept": "high-resolution image synthesis",
          "relevance": 0.688
        },
        {
          "concept": "cross-attention layer",
          "relevance": 0.687
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.686
        },
        {
          "concept": "image generation process",
          "relevance": 0.679
        },
        {
          "concept": "GPU days",
          "relevance": 0.641
        },
        {
          "concept": "pretrained autoencoder",
          "relevance": 0.641
        },
        {
          "concept": "image inpainting",
          "relevance": 0.639
        },
        {
          "concept": "bounding boxes",
          "relevance": 0.636
        },
        {
          "concept": "image formation process",
          "relevance": 0.635
        },
        {
          "concept": "latent space",
          "relevance": 0.633
        },
        {
          "concept": "visual fidelity",
          "relevance": 0.632
        },
        {
          "concept": "super-resolution",
          "relevance": 0.627
        },
        {
          "concept": "image generation",
          "relevance": 0.626
        },
        {
          "concept": "convolutional manner",
          "relevance": 0.625
        },
        {
          "concept": "complexity reduction",
          "relevance": 0.624
        },
        {
          "concept": "competitive performance",
          "relevance": 0.623
        },
        {
          "concept": "model architecture",
          "relevance": 0.622
        },
        {
          "concept": "computational resources",
          "relevance": 0.621
        },
        {
          "concept": "high-resolution synthesis",
          "relevance": 0.617
        },
        {
          "concept": "conditional input",
          "relevance": 0.605
        },
        {
          "concept": "computational requirements",
          "relevance": 0.605
        },
        {
          "concept": "pixel space",
          "relevance": 0.601
        },
        {
          "concept": "autoencoder",
          "relevance": 0.576
        },
        {
          "concept": "image data",
          "relevance": 0.573
        },
        {
          "concept": "generation process",
          "relevance": 0.531
        },
        {
          "concept": "images",
          "relevance": 0.509
        },
        {
          "concept": "inpainting",
          "relevance": 0.501
        },
        {
          "concept": "flexible generation",
          "relevance": 0.495
        },
        {
          "concept": "GPU",
          "relevance": 0.494
        },
        {
          "concept": "pixel",
          "relevance": 0.476
        },
        {
          "concept": "training",
          "relevance": 0.474
        },
        {
          "concept": "architecture",
          "relevance": 0.465
        },
        {
          "concept": "high-resolution",
          "relevance": 0.456
        },
        {
          "concept": "task",
          "relevance": 0.449
        },
        {
          "concept": "representation",
          "relevance": 0.439
        },
        {
          "concept": "retraining",
          "relevance": 0.434
        },
        {
          "concept": "DM training",
          "relevance": 0.433
        },
        {
          "concept": "model",
          "relevance": 0.428
        },
        {
          "concept": "optimization",
          "relevance": 0.427
        },
        {
          "concept": "inference",
          "relevance": 0.427
        },
        {
          "concept": "diffusion model",
          "relevance": 0.425
        },
        {
          "concept": "space",
          "relevance": 0.422
        },
        {
          "concept": "input",
          "relevance": 0.418
        },
        {
          "concept": "sequential evaluation",
          "relevance": 0.415
        },
        {
          "concept": "text",
          "relevance": 0.413
        },
        {
          "concept": "performance",
          "relevance": 0.412
        },
        {
          "concept": "requirements",
          "relevance": 0.41
        },
        {
          "concept": "flexibility",
          "relevance": 0.397
        },
        {
          "concept": "applications",
          "relevance": 0.389
        },
        {
          "concept": "resources",
          "relevance": 0.387
        },
        {
          "concept": "process",
          "relevance": 0.376
        },
        {
          "concept": "box",
          "relevance": 0.375
        },
        {
          "concept": "generation",
          "relevance": 0.372
        },
        {
          "concept": "fidelity",
          "relevance": 0.367
        },
        {
          "concept": "evaluation",
          "relevance": 0.363
        },
        {
          "concept": "quality",
          "relevance": 0.353
        },
        {
          "concept": "art",
          "relevance": 0.353
        },
        {
          "concept": "complex",
          "relevance": 0.343
        },
        {
          "concept": "state-of-the-art synthesis",
          "relevance": 0.342
        },
        {
          "concept": "data",
          "relevance": 0.328
        },
        {
          "concept": "manner",
          "relevance": 0.318
        },
        {
          "concept": "ART score",
          "relevance": 0.314
        },
        {
          "concept": "formulation",
          "relevance": 0.283
        },
        {
          "concept": "layer",
          "relevance": 0.283
        },
        {
          "concept": "formation process",
          "relevance": 0.282
        },
        {
          "concept": "preservation",
          "relevance": 0.276
        },
        {
          "concept": "diffusion",
          "relevance": 0.256
        },
        {
          "concept": "mechanism",
          "relevance": 0.24
        },
        {
          "concept": "synthesis",
          "relevance": 0.238
        },
        {
          "concept": "scores",
          "relevance": 0.222
        },
        {
          "concept": "reduction",
          "relevance": 0.205
        },
        {
          "concept": "days",
          "relevance": 0.11
        }
      ]
    },
    {
      "paperId": "pub.1095689025",
      "doi": "10.1109/cvpr.2009.5206848",
      "title": "ImageNet: A large-scale hierarchical image database",
      "year": 2009,
      "citationCount": 48560,
      "fieldCitationRatio": 11425.11,
      "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",
      "reference_ids": [
        "pub.1002200216",
        "pub.1002707761",
        "pub.1093839826",
        "pub.1094055038",
        "pub.1045945012",
        "pub.1093423587",
        "pub.1094700637",
        "pub.1093888066",
        "pub.1061743490",
        "pub.1022501172",
        "pub.1093301542",
        "pub.1017544873",
        "pub.1027534025",
        "pub.1038699157",
        "pub.1061743121",
        "pub.1093557144",
        "pub.1021652857",
        "pub.1052687286",
        "pub.1045723254"
      ],
      "concepts_scores": [
        {
          "concept": "semantic hierarchy of WordNet",
          "relevance": 0.695
        },
        {
          "concept": "hierarchical structure of ImageNet",
          "relevance": 0.695
        },
        {
          "concept": "explosion of image data",
          "relevance": 0.676
        },
        {
          "concept": "large-scale ontologies",
          "relevance": 0.673
        },
        {
          "concept": "computer vision community",
          "relevance": 0.673
        },
        {
          "concept": "synsets of WordNet",
          "relevance": 0.665
        },
        {
          "concept": "data collection scheme",
          "relevance": 0.658
        },
        {
          "concept": "large-scale databases",
          "relevance": 0.644
        },
        {
          "concept": "multimedia data",
          "relevance": 0.625
        },
        {
          "concept": "vision community",
          "relevance": 0.624
        },
        {
          "concept": "annotated images",
          "relevance": 0.619
        },
        {
          "concept": "image classification",
          "relevance": 0.618
        },
        {
          "concept": "semantic hierarchy",
          "relevance": 0.617
        },
        {
          "concept": "WordNet structure",
          "relevance": 0.616
        },
        {
          "concept": "image datasets",
          "relevance": 0.615
        },
        {
          "concept": "object clustering",
          "relevance": 0.613
        },
        {
          "concept": "ImageNet",
          "relevance": 0.611
        },
        {
          "concept": "object recognition",
          "relevance": 0.599
        },
        {
          "concept": "WordNet",
          "relevance": 0.593
        },
        {
          "concept": "Amazon Mechanical Turk",
          "relevance": 0.59
        },
        {
          "concept": "collection scheme",
          "relevance": 0.588
        },
        {
          "concept": "image data",
          "relevance": 0.56
        },
        {
          "concept": "synsets",
          "relevance": 0.556
        },
        {
          "concept": "Mechanical Turk",
          "relevance": 0.547
        },
        {
          "concept": "ontology of images",
          "relevance": 0.536
        },
        {
          "concept": "robust model",
          "relevance": 0.527
        },
        {
          "concept": "hierarchical structure",
          "relevance": 0.518
        },
        {
          "concept": "images",
          "relevance": 0.499
        },
        {
          "concept": "multimedia",
          "relevance": 0.477
        },
        {
          "concept": "Internet",
          "relevance": 0.468
        },
        {
          "concept": "algorithm",
          "relevance": 0.461
        },
        {
          "concept": "dataset",
          "relevance": 0.458
        },
        {
          "concept": "subtrees",
          "relevance": 0.457
        },
        {
          "concept": "database",
          "relevance": 0.455
        },
        {
          "concept": "computer",
          "relevance": 0.451
        },
        {
          "concept": "task",
          "relevance": 0.439
        },
        {
          "concept": "scheme",
          "relevance": 0.439
        },
        {
          "concept": "classification",
          "relevance": 0.436
        },
        {
          "concept": "recognition",
          "relevance": 0.422
        },
        {
          "concept": "accuracy",
          "relevance": 0.419
        },
        {
          "concept": "data",
          "relevance": 0.396
        },
        {
          "concept": "Amazon",
          "relevance": 0.387
        },
        {
          "concept": "applications",
          "relevance": 0.38
        },
        {
          "concept": "clusters",
          "relevance": 0.374
        },
        {
          "concept": "objective",
          "relevance": 0.37
        },
        {
          "concept": "Turks",
          "relevance": 0.355
        },
        {
          "concept": "model",
          "relevance": 0.335
        },
        {
          "concept": "diversity",
          "relevance": 0.335
        },
        {
          "concept": "backbone",
          "relevance": 0.333
        },
        {
          "concept": "research",
          "relevance": 0.327
        },
        {
          "concept": "explosion",
          "relevance": 0.321
        },
        {
          "concept": "opportunities",
          "relevance": 0.295
        },
        {
          "concept": "scale",
          "relevance": 0.28
        },
        {
          "concept": "average",
          "relevance": 0.273
        },
        {
          "concept": "community",
          "relevance": 0.26
        },
        {
          "concept": "structure",
          "relevance": 0.257
        },
        {
          "concept": "analysis",
          "relevance": 0.254
        },
        {
          "concept": "use",
          "relevance": 0.251
        },
        {
          "concept": "index",
          "relevance": 0.226
        },
        {
          "concept": "potential",
          "relevance": 0.215
        }
      ]
    },
    {
      "paperId": "pub.1052687286",
      "doi": "10.1023/b:visi.0000029664.99615.94",
      "title": "Distinctive Image Features from Scale-Invariant Keypoints",
      "year": 2004,
      "citationCount": 45113,
      "fieldCitationRatio": 8931.09,
      "abstract": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.",
      "reference_ids": [
        "pub.1040477036",
        "pub.1014228158",
        "pub.1061156202",
        "pub.1037494632",
        "pub.1003744571",
        "pub.1024856005",
        "pub.1094986307",
        "pub.1004259985",
        "pub.1022603165",
        "pub.1061156611",
        "pub.1099383030",
        "pub.1094775144",
        "pub.1099368997",
        "pub.1021081072",
        "pub.1094093086",
        "pub.1007590088",
        "pub.1034101072",
        "pub.1051377384",
        "pub.1035426628",
        "pub.1046596731",
        "pub.1027706028",
        "pub.1024177337",
        "pub.1061742025",
        "pub.1037481186",
        "pub.1093624919",
        "pub.1045823787",
        "pub.1061155653",
        "pub.1026320994",
        "pub.1094949846"
      ],
      "concepts_scores": [
        {
          "concept": "database of features",
          "relevance": 0.754
        },
        {
          "concept": "near real-time performance",
          "relevance": 0.667
        },
        {
          "concept": "scale-invariant keypoints",
          "relevance": 0.656
        },
        {
          "concept": "real-time performance",
          "relevance": 0.642
        },
        {
          "concept": "Distinctive imaging features",
          "relevance": 0.611
        },
        {
          "concept": "affine distortion",
          "relevance": 0.607
        },
        {
          "concept": "robust matching",
          "relevance": 0.599
        },
        {
          "concept": "Hough transform",
          "relevance": 0.586
        },
        {
          "concept": "object recognition",
          "relevance": 0.585
        },
        {
          "concept": "least-squares solution",
          "relevance": 0.572
        },
        {
          "concept": "image features",
          "relevance": 0.552
        },
        {
          "concept": "image scale",
          "relevance": 0.54
        },
        {
          "concept": "recognition",
          "relevance": 0.501
        },
        {
          "concept": "keypoints",
          "relevance": 0.476
        },
        {
          "concept": "images",
          "relevance": 0.475
        },
        {
          "concept": "features",
          "relevance": 0.474
        },
        {
          "concept": "scale invariance",
          "relevance": 0.474
        },
        {
          "concept": "matching",
          "relevance": 0.464
        },
        {
          "concept": "objective",
          "relevance": 0.45
        },
        {
          "concept": "scene",
          "relevance": 0.45
        },
        {
          "concept": "database",
          "relevance": 0.444
        },
        {
          "concept": "clutter",
          "relevance": 0.435
        },
        {
          "concept": "verification",
          "relevance": 0.421
        },
        {
          "concept": "noise",
          "relevance": 0.418
        },
        {
          "concept": "performance",
          "relevance": 0.394
        },
        {
          "concept": "viewpoint",
          "relevance": 0.38
        },
        {
          "concept": "illumination",
          "relevance": 0.38
        },
        {
          "concept": "probability",
          "relevance": 0.374
        },
        {
          "concept": "distortion",
          "relevance": 0.369
        },
        {
          "concept": "clusters",
          "relevance": 0.365
        },
        {
          "concept": "transformation",
          "relevance": 0.358
        },
        {
          "concept": "method",
          "relevance": 0.346
        },
        {
          "concept": "views",
          "relevance": 0.342
        },
        {
          "concept": "solution",
          "relevance": 0.337
        },
        {
          "concept": "parameters",
          "relevance": 0.335
        },
        {
          "concept": "occlusion",
          "relevance": 0.314
        },
        {
          "concept": "rotation",
          "relevance": 0.305
        },
        {
          "concept": "scale",
          "relevance": 0.241
        },
        {
          "concept": "changes",
          "relevance": 0.205
        }
      ]
    },
    {
      "paperId": "pub.1027534025",
      "doi": "10.1007/s11263-007-0090-8",
      "title": "LabelMe: A Database and Web-Based Tool for Image Annotation",
      "year": 2007,
      "citationCount": 3093,
      "fieldCitationRatio": 663.1,
      "abstract": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.",
      "reference_ids": [
        "pub.1094301320",
        "pub.1015899908",
        "pub.1021318481",
        "pub.1061742623",
        "pub.1061743121",
        "pub.1094132829",
        "pub.1094707806",
        "pub.1008598522",
        "pub.1094251884",
        "pub.1110625185",
        "pub.1043225769",
        "pub.1094870330",
        "pub.1019562355",
        "pub.1061743101",
        "pub.1095195005",
        "pub.1094136659",
        "pub.1095068040",
        "pub.1095164167",
        "pub.1093279315",
        "pub.1093955201",
        "pub.1093187020",
        "pub.1034831185",
        "pub.1094053049",
        "pub.1021652857",
        "pub.1063151975",
        "pub.1094847977",
        "pub.1032051655",
        "pub.1026428551",
        "pub.1091765347",
        "pub.1094700637",
        "pub.1034091054",
        "pub.1093305113"
      ],
      "concepts_scores": [
        {
          "concept": "collection of images",
          "relevance": 0.678
        },
        {
          "concept": "minimal user supervision",
          "relevance": 0.669
        },
        {
          "concept": "image annotation",
          "relevance": 0.639
        },
        {
          "concept": "object detection",
          "relevance": 0.636
        },
        {
          "concept": "supervised learning",
          "relevance": 0.634
        },
        {
          "concept": "recognition research",
          "relevance": 0.629
        },
        {
          "concept": "object parts",
          "relevance": 0.625
        },
        {
          "concept": "user supervision",
          "relevance": 0.624
        },
        {
          "concept": "depth order",
          "relevance": 0.621
        },
        {
          "concept": "object categories",
          "relevance": 0.619
        },
        {
          "concept": "object labels",
          "relevance": 0.619
        },
        {
          "concept": "art datasets",
          "relevance": 0.618
        },
        {
          "concept": "object recognition",
          "relevance": 0.613
        },
        {
          "concept": "multiple instances",
          "relevance": 0.612
        },
        {
          "concept": "annotation tool",
          "relevance": 0.596
        },
        {
          "concept": "web-based tool",
          "relevance": 0.581
        },
        {
          "concept": "dataset",
          "relevance": 0.58
        },
        {
          "concept": "annotation",
          "relevance": 0.557
        },
        {
          "concept": "quantitative evaluation",
          "relevance": 0.511
        },
        {
          "concept": "images",
          "relevance": 0.506
        },
        {
          "concept": "recognition",
          "relevance": 0.499
        },
        {
          "concept": "WordNet",
          "relevance": 0.498
        },
        {
          "concept": "scene",
          "relevance": 0.472
        },
        {
          "concept": "instances",
          "relevance": 0.471
        },
        {
          "concept": "automatically",
          "relevance": 0.471
        },
        {
          "concept": "objective",
          "relevance": 0.469
        },
        {
          "concept": "detection",
          "relevance": 0.46
        },
        {
          "concept": "labeling",
          "relevance": 0.46
        },
        {
          "concept": "Web",
          "relevance": 0.46
        },
        {
          "concept": "learning",
          "relevance": 0.442
        },
        {
          "concept": "tools",
          "relevance": 0.435
        },
        {
          "concept": "sharing",
          "relevance": 0.425
        },
        {
          "concept": "database",
          "relevance": 0.402
        },
        {
          "concept": "collection",
          "relevance": 0.366
        },
        {
          "concept": "supervision",
          "relevance": 0.366
        },
        {
          "concept": "evaluation",
          "relevance": 0.363
        },
        {
          "concept": "art",
          "relevance": 0.353
        },
        {
          "concept": "Abstract",
          "relevance": 0.346
        },
        {
          "concept": "research",
          "relevance": 0.335
        },
        {
          "concept": "data",
          "relevance": 0.328
        },
        {
          "concept": "categories",
          "relevance": 0.317
        },
        {
          "concept": "order",
          "relevance": 0.307
        },
        {
          "concept": "parts",
          "relevance": 0.298
        },
        {
          "concept": "content",
          "relevance": 0.258
        },
        {
          "concept": "depth",
          "relevance": 0.252
        }
      ]
    },
    {
      "paperId": "pub.1095850445",
      "doi": "10.1109/cvpr.2017.632",
      "title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "year": 2017,
      "citationCount": 17290,
      "fieldCitationRatio": 3485.46,
      "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
      "reference_ids": [
        "pub.1036703514",
        "pub.1017774818",
        "pub.1061164348",
        "pub.1026908314",
        "pub.1094854085",
        "pub.1095706293",
        "pub.1093406896",
        "pub.1094706336",
        "pub.1044720211",
        "pub.1016085920",
        "pub.1034474844",
        "pub.1095493673",
        "pub.1027979653",
        "pub.1094856798",
        "pub.1000850552",
        "pub.1000375585",
        "pub.1063151969",
        "pub.1046666876",
        "pub.1093626237",
        "pub.1035996172",
        "pub.1040802208",
        "pub.1094045097",
        "pub.1017576733",
        "pub.1051119970",
        "pub.1015291806",
        "pub.1093907742",
        "pub.1095367750",
        "pub.1004607132",
        "pub.1061640964",
        "pub.1093662716",
        "pub.1023031850",
        "pub.1009767488",
        "pub.1035588163"
      ],
      "concepts_scores": [
        {
          "concept": "conditional adversarial network",
          "relevance": 0.782
        },
        {
          "concept": "adversarial network",
          "relevance": 0.725
        },
        {
          "concept": "input image to output image",
          "relevance": 0.709
        },
        {
          "concept": "loss function",
          "relevance": 0.706
        },
        {
          "concept": "image-to-image translation problem",
          "relevance": 0.704
        },
        {
          "concept": "image-to-image translation",
          "relevance": 0.699
        },
        {
          "concept": "Image-to-image",
          "relevance": 0.678
        },
        {
          "concept": "general-purpose solution",
          "relevance": 0.666
        },
        {
          "concept": "hand-engineering",
          "relevance": 0.635
        },
        {
          "concept": "synthesized photos",
          "relevance": 0.629
        },
        {
          "concept": "hand-engineered",
          "relevance": 0.628
        },
        {
          "concept": "edge map",
          "relevance": 0.626
        },
        {
          "concept": "output image",
          "relevance": 0.623
        },
        {
          "concept": "label maps",
          "relevance": 0.623
        },
        {
          "concept": "color images",
          "relevance": 0.618
        },
        {
          "concept": "reconstructed object",
          "relevance": 0.599
        },
        {
          "concept": "Twitter users",
          "relevance": 0.599
        },
        {
          "concept": "mapping function",
          "relevance": 0.594
        },
        {
          "concept": "loss formulation",
          "relevance": 0.592
        },
        {
          "concept": "translation problems",
          "relevance": 0.561
        },
        {
          "concept": "network",
          "relevance": 0.556
        },
        {
          "concept": "maps",
          "relevance": 0.491
        },
        {
          "concept": "Pix2Pix",
          "relevance": 0.485
        },
        {
          "concept": "users",
          "relevance": 0.477
        },
        {
          "concept": "images",
          "relevance": 0.469
        },
        {
          "concept": "Twitter",
          "relevance": 0.468
        },
        {
          "concept": "software",
          "relevance": 0.454
        },
        {
          "concept": "task",
          "relevance": 0.444
        },
        {
          "concept": "input",
          "relevance": 0.414
        },
        {
          "concept": "labeling",
          "relevance": 0.393
        },
        {
          "concept": "photo",
          "relevance": 0.388
        },
        {
          "concept": "edge",
          "relevance": 0.379
        },
        {
          "concept": "objective",
          "relevance": 0.375
        },
        {
          "concept": "system",
          "relevance": 0.371
        },
        {
          "concept": "function",
          "relevance": 0.362
        },
        {
          "concept": "solution",
          "relevance": 0.349
        },
        {
          "concept": "color",
          "relevance": 0.345
        },
        {
          "concept": "experiments",
          "relevance": 0.34
        },
        {
          "concept": "translation",
          "relevance": 0.335
        },
        {
          "concept": "results",
          "relevance": 0.312
        },
        {
          "concept": "loss",
          "relevance": 0.285
        },
        {
          "concept": "formulation",
          "relevance": 0.28
        },
        {
          "concept": "artistic experience",
          "relevance": 0.269
        },
        {
          "concept": "community",
          "relevance": 0.263
        },
        {
          "concept": "conditions",
          "relevance": 0.206
        },
        {
          "concept": "problem",
          "relevance": 0.155
        },
        {
          "concept": "release",
          "relevance": 0.153
        }
      ]
    },
    {
      "paperId": "pub.1061640964",
      "doi": "10.1109/tip.2003.819861",
      "title": "Image Quality Assessment: From Error Visibility to Structural Similarity",
      "year": 2004,
      "citationCount": 44667,
      "fieldCitationRatio": 8842.79,
      "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.",
      "reference_ids": [
        "pub.1061098596",
        "pub.1109699835",
        "pub.1051590917",
        "pub.1061157146",
        "pub.1038074861",
        "pub.1110622471",
        "pub.1042918725",
        "pub.1002450358",
        "pub.1065157151",
        "pub.1061251565",
        "pub.1021151071",
        "pub.1094280748",
        "pub.1010524390",
        "pub.1007353974",
        "pub.1093619469",
        "pub.1061647489",
        "pub.1065210132",
        "pub.1061228473",
        "pub.1061222004",
        "pub.1011624274",
        "pub.1088779547",
        "pub.1005576318",
        "pub.1014836974",
        "pub.1095102111",
        "pub.1016979910",
        "pub.1065158325",
        "pub.1052170656",
        "pub.1065159598",
        "pub.1037873940",
        "pub.1037099914",
        "pub.1020793995",
        "pub.1004119149",
        "pub.1009205613",
        "pub.1061137112",
        "pub.1026562287",
        "pub.1061240373",
        "pub.1094779682",
        "pub.1061240018",
        "pub.1093467274",
        "pub.1061239913",
        "pub.1006831752",
        "pub.1061240395",
        "pub.1003534189",
        "pub.1010313466",
        "pub.1032128907",
        "pub.1061239618",
        "pub.1095644828",
        "pub.1024091291",
        "pub.1020705229"
      ],
      "concepts_scores": [
        {
          "concept": "degradation of structural information",
          "relevance": 0.712
        },
        {
          "concept": "perceptual image quality",
          "relevance": 0.689
        },
        {
          "concept": "visibility of errors",
          "relevance": 0.683
        },
        {
          "concept": "structural similarity index",
          "relevance": 0.683
        },
        {
          "concept": "human visual perception",
          "relevance": 0.681
        },
        {
          "concept": "database of images",
          "relevance": 0.68
        },
        {
          "concept": "human visual system",
          "relevance": 0.679
        },
        {
          "concept": "error visibility",
          "relevance": 0.63
        },
        {
          "concept": "distorted images",
          "relevance": 0.62
        },
        {
          "concept": "structural information",
          "relevance": 0.608
        },
        {
          "concept": "visual system",
          "relevance": 0.579
        },
        {
          "concept": "similarity index",
          "relevance": 0.577
        },
        {
          "concept": "image quality",
          "relevance": 0.57
        },
        {
          "concept": "visual perception",
          "relevance": 0.564
        },
        {
          "concept": "quality assessment",
          "relevance": 0.519
        },
        {
          "concept": "JPEG2000",
          "relevance": 0.505
        },
        {
          "concept": "JPEG",
          "relevance": 0.503
        },
        {
          "concept": "structural similarity",
          "relevance": 0.499
        },
        {
          "concept": "images",
          "relevance": 0.499
        },
        {
          "concept": "information",
          "relevance": 0.482
        },
        {
          "concept": "error",
          "relevance": 0.477
        },
        {
          "concept": "scene",
          "relevance": 0.473
        },
        {
          "concept": "visibility",
          "relevance": 0.462
        },
        {
          "concept": "framework",
          "relevance": 0.415
        },
        {
          "concept": "quality",
          "relevance": 0.409
        },
        {
          "concept": "database",
          "relevance": 0.402
        },
        {
          "concept": "complementary framework",
          "relevance": 0.401
        },
        {
          "concept": "examples",
          "relevance": 0.382
        },
        {
          "concept": "similarity",
          "relevance": 0.376
        },
        {
          "concept": "system",
          "relevance": 0.376
        },
        {
          "concept": "method",
          "relevance": 0.363
        },
        {
          "concept": "concept",
          "relevance": 0.361
        },
        {
          "concept": "properties",
          "relevance": 0.329
        },
        {
          "concept": "degradation",
          "relevance": 0.322
        },
        {
          "concept": "structure",
          "relevance": 0.302
        },
        {
          "concept": "perception",
          "relevance": 0.294
        },
        {
          "concept": "comparison",
          "relevance": 0.283
        },
        {
          "concept": "rate",
          "relevance": 0.257
        },
        {
          "concept": "index",
          "relevance": 0.232
        },
        {
          "concept": "assessment",
          "relevance": 0.225
        },
        {
          "concept": "differences",
          "relevance": 0.153
        }
      ]
    },
    {
      "paperId": "pub.1093626237",
      "doi": "10.1109/cvpr.2015.7298965",
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "year": 2015,
      "citationCount": 32637,
      "fieldCitationRatio": 6415.44,
      "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20]), the VGG net [1], and GoogLeNet [2]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IV on 2012), NYVDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",
      "reference_ids": [
        "pub.1061641212",
        "pub.1061743868",
        "pub.1033986161",
        "pub.1003867987",
        "pub.1094083003",
        "pub.1053469442",
        "pub.1095134254",
        "pub.1024540204",
        "pub.1030406568",
        "pub.1095686079",
        "pub.1008345178",
        "pub.1052031051",
        "pub.1016764525",
        "pub.1094727707",
        "pub.1093843992",
        "pub.1094291017",
        "pub.1032233097",
        "pub.1006936750",
        "pub.1015397249",
        "pub.1003201959",
        "pub.1003742061",
        "pub.1093500653"
      ],
      "concepts_scores": [
        {
          "concept": "convolutional network",
          "relevance": 0.779
        },
        {
          "concept": "semantic segmentation",
          "relevance": 0.726
        },
        {
          "concept": "yield hierarchies of features",
          "relevance": 0.7
        },
        {
          "concept": "state-of-the-art segmentation",
          "relevance": 0.698
        },
        {
          "concept": "trained end-to-end",
          "relevance": 0.697
        },
        {
          "concept": "input of arbitrary size",
          "relevance": 0.696
        },
        {
          "concept": "correspondingly-sized output",
          "relevance": 0.682
        },
        {
          "concept": "dense prediction tasks",
          "relevance": 0.682
        },
        {
          "concept": "hierarchies of features",
          "relevance": 0.677
        },
        {
          "concept": "pixels-to-pixels",
          "relevance": 0.676
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.676
        },
        {
          "concept": "fully convolutional network",
          "relevance": 0.671
        },
        {
          "concept": "end-to-end",
          "relevance": 0.663
        },
        {
          "concept": "PASCAL VOC",
          "relevance": 0.631
        },
        {
          "concept": "learned representations",
          "relevance": 0.629
        },
        {
          "concept": "appearance information",
          "relevance": 0.629
        },
        {
          "concept": "classification network",
          "relevance": 0.626
        },
        {
          "concept": "VGG-Net",
          "relevance": 0.626
        },
        {
          "concept": "SIFT flow",
          "relevance": 0.626
        },
        {
          "concept": "segmentation task",
          "relevance": 0.623
        },
        {
          "concept": "semantic information",
          "relevance": 0.619
        },
        {
          "concept": "prediction task",
          "relevance": 0.615
        },
        {
          "concept": "efficient inference",
          "relevance": 0.614
        },
        {
          "concept": "visual model",
          "relevance": 0.593
        },
        {
          "concept": "network",
          "relevance": 0.569
        },
        {
          "concept": "arbitrary size",
          "relevance": 0.562
        },
        {
          "concept": "task",
          "relevance": 0.513
        },
        {
          "concept": "coarse layer",
          "relevance": 0.495
        },
        {
          "concept": "VGG",
          "relevance": 0.491
        },
        {
          "concept": "inference",
          "relevance": 0.487
        },
        {
          "concept": "semantics",
          "relevance": 0.478
        },
        {
          "concept": "segments",
          "relevance": 0.474
        },
        {
          "concept": "information",
          "relevance": 0.474
        },
        {
          "concept": "SIFT",
          "relevance": 0.473
        },
        {
          "concept": "architecture",
          "relevance": 0.459
        },
        {
          "concept": "classification",
          "relevance": 0.439
        },
        {
          "concept": "learning",
          "relevance": 0.436
        },
        {
          "concept": "representation",
          "relevance": 0.432
        },
        {
          "concept": "nets",
          "relevance": 0.413
        },
        {
          "concept": "input",
          "relevance": 0.412
        },
        {
          "concept": "fine layer",
          "relevance": 0.404
        },
        {
          "concept": "images",
          "relevance": 0.403
        },
        {
          "concept": "Fully",
          "relevance": 0.396
        },
        {
          "concept": "model",
          "relevance": 0.391
        },
        {
          "concept": "features",
          "relevance": 0.391
        },
        {
          "concept": "output",
          "relevance": 0.387
        },
        {
          "concept": "connection",
          "relevance": 0.367
        },
        {
          "concept": "space",
          "relevance": 0.359
        },
        {
          "concept": "layer",
          "relevance": 0.323
        },
        {
          "concept": "VOC",
          "relevance": 0.319
        },
        {
          "concept": "spatially",
          "relevance": 0.318
        },
        {
          "concept": "size",
          "relevance": 0.264
        },
        {
          "concept": "appearance",
          "relevance": 0.253
        },
        {
          "concept": "flow",
          "relevance": 0.208
        }
      ]
    },
    {
      "paperId": "pub.1148956311",
      "doi": "10.1609/aaai.v32i1.11694",
      "title": "Deep Reinforcement Learning That Matters",
      "year": 2018,
      "citationCount": 1058,
      "fieldCitationRatio": 235.43,
      "abstract": "In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.",
      "reference_ids": [
        "pub.1127185937",
        "pub.1030392094",
        "pub.1029295303"
      ],
      "concepts_scores": [
        {
          "concept": "state-of-the-art",
          "relevance": 0.681
        },
        {
          "concept": "reinforcement learning",
          "relevance": 0.63
        },
        {
          "concept": "state-of-the-art deep RL methods",
          "relevance": 0.618
        },
        {
          "concept": "deep reinforcement learning",
          "relevance": 0.589
        },
        {
          "concept": "deep RL methods",
          "relevance": 0.589
        },
        {
          "concept": "deep RL",
          "relevance": 0.546
        },
        {
          "concept": "RL methods",
          "relevance": 0.545
        },
        {
          "concept": "benchmark environments",
          "relevance": 0.539
        },
        {
          "concept": "non-determinism",
          "relevance": 0.531
        },
        {
          "concept": "significant metrics",
          "relevance": 0.51
        },
        {
          "concept": "wasted effort",
          "relevance": 0.491
        },
        {
          "concept": "novel method",
          "relevance": 0.49
        },
        {
          "concept": "metrics",
          "relevance": 0.451
        },
        {
          "concept": "investigate challenges",
          "relevance": 0.435
        },
        {
          "concept": "spur discussion",
          "relevance": 0.414
        },
        {
          "concept": "non-reproducibility",
          "relevance": 0.38
        },
        {
          "concept": "learning",
          "relevance": 0.379
        },
        {
          "concept": "method",
          "relevance": 0.378
        },
        {
          "concept": "domain",
          "relevance": 0.335
        },
        {
          "concept": "results",
          "relevance": 0.335
        },
        {
          "concept": "environment",
          "relevance": 0.331
        },
        {
          "concept": "improvement",
          "relevance": 0.328
        },
        {
          "concept": "significant progress",
          "relevance": 0.326
        },
        {
          "concept": "technique",
          "relevance": 0.316
        },
        {
          "concept": "challenges",
          "relevance": 0.303
        },
        {
          "concept": "standards",
          "relevance": 0.286
        },
        {
          "concept": "efforts",
          "relevance": 0.286
        },
        {
          "concept": "reproducible results",
          "relevance": 0.28
        },
        {
          "concept": "experimental reports",
          "relevance": 0.266
        },
        {
          "concept": "experimental techniques",
          "relevance": 0.257
        },
        {
          "concept": "variance",
          "relevance": 0.23
        },
        {
          "concept": "discussion",
          "relevance": 0.229
        },
        {
          "concept": "progression",
          "relevance": 0.227
        },
        {
          "concept": "procedure",
          "relevance": 0.22
        },
        {
          "concept": "baseline",
          "relevance": 0.218
        },
        {
          "concept": "guidelines",
          "relevance": 0.214
        },
        {
          "concept": "variables",
          "relevance": 0.2
        },
        {
          "concept": "reproducibility",
          "relevance": 0.19
        },
        {
          "concept": "reports",
          "relevance": 0.184
        },
        {
          "concept": "years",
          "relevance": 0.18
        },
        {
          "concept": "significance",
          "relevance": 0.168
        },
        {
          "concept": "problem",
          "relevance": 0.139
        }
      ]
    },
    {
      "paperId": "pub.1029295303",
      "doi": "10.1007/978-3-540-24775-3_3",
      "title": "Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms",
      "year": 2004,
      "citationCount": 355,
      "fieldCitationRatio": 61.21,
      "abstract": "Empirical research in learning algorithms for classification tasks generally requires the use of significance tests. The quality of a test is typically judged on Type I error (how often the test indicates a difference when it should not) and Type II error (how often it indicates no difference when it should). In this paper we argue that the replicability of a test is also of importance. We say that a test has low replicability if its outcome strongly depends on the particular random partitioning of the data that is used to perform it. We present empirical measures of replicability and use them to compare the performance of several popular tests in a realistic setting involving standard learning algorithms and benchmark datasets. Based on our results we give recommendations on which test to use.",
      "reference_ids": [
        "pub.1044842029",
        "pub.1019095106",
        "pub.1030680500",
        "pub.1053132543"
      ],
      "concepts_scores": [
        {
          "concept": "learning algorithms",
          "relevance": 0.672
        },
        {
          "concept": "standard learning algorithms",
          "relevance": 0.607
        },
        {
          "concept": "compare learning algorithms",
          "relevance": 0.604
        },
        {
          "concept": "benchmark datasets",
          "relevance": 0.562
        },
        {
          "concept": "classification task",
          "relevance": 0.56
        },
        {
          "concept": "algorithm",
          "relevance": 0.506
        },
        {
          "concept": "random partitions",
          "relevance": 0.485
        },
        {
          "concept": "measures of replication",
          "relevance": 0.449
        },
        {
          "concept": "dataset",
          "relevance": 0.414
        },
        {
          "concept": "task",
          "relevance": 0.397
        },
        {
          "concept": "classification",
          "relevance": 0.394
        },
        {
          "concept": "learning",
          "relevance": 0.391
        },
        {
          "concept": "significance tests",
          "relevance": 0.389
        },
        {
          "concept": "empirical measures",
          "relevance": 0.388
        },
        {
          "concept": "performance",
          "relevance": 0.364
        },
        {
          "concept": "error",
          "relevance": 0.363
        },
        {
          "concept": "empirical research",
          "relevance": 0.355
        },
        {
          "concept": "partitioning",
          "relevance": 0.35
        },
        {
          "concept": "type II error",
          "relevance": 0.346
        },
        {
          "concept": "type I error",
          "relevance": 0.321
        },
        {
          "concept": "II errors",
          "relevance": 0.316
        },
        {
          "concept": "quality",
          "relevance": 0.312
        },
        {
          "concept": "research",
          "relevance": 0.295
        },
        {
          "concept": "data",
          "relevance": 0.289
        },
        {
          "concept": "test",
          "relevance": 0.284
        },
        {
          "concept": "recommendations",
          "relevance": 0.282
        },
        {
          "concept": "results",
          "relevance": 0.279
        },
        {
          "concept": "type",
          "relevance": 0.221
        },
        {
          "concept": "replication",
          "relevance": 0.212
        },
        {
          "concept": "outcomes",
          "relevance": 0.203
        },
        {
          "concept": "significance",
          "relevance": 0.176
        }
      ]
    },
    {
      "paperId": "pub.1030680500",
      "doi": "10.1023/a:1007465528199",
      "title": "Bayesian Network Classifiers",
      "year": 1997,
      "citationCount": 3829,
      "fieldCitationRatio": NaN,
      "abstract": "Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection.",
      "reference_ids": [
        "pub.1011622945",
        "pub.1069975366",
        "pub.1021506833",
        "pub.1059418581",
        "pub.1064409646",
        "pub.1027065619",
        "pub.1026070931",
        "pub.1049953235",
        "pub.1046316965",
        "pub.1052966378",
        "pub.1052870567",
        "pub.1018373874",
        "pub.1026774640",
        "pub.1034203065",
        "pub.1093241761",
        "pub.1014832254",
        "pub.1024076823",
        "pub.1050546771",
        "pub.1035524560",
        "pub.1040065388",
        "pub.1041000917",
        "pub.1026539583",
        "pub.1013962799",
        "pub.1008536307",
        "pub.1006921805",
        "pub.1003003508",
        "pub.1098665985",
        "pub.1061213478",
        "pub.1061646459",
        "pub.1006764400"
      ],
      "concepts_scores": [
        {
          "concept": "Tree Augmented Naive Bayes",
          "relevance": 0.781
        },
        {
          "concept": "Naive Bayes",
          "relevance": 0.773
        },
        {
          "concept": "Bayesian classifier",
          "relevance": 0.717
        },
        {
          "concept": "state-of-the-art classifiers",
          "relevance": 0.703
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.679
        },
        {
          "concept": "naive Bayesian classifier",
          "relevance": 0.672
        },
        {
          "concept": "representations of probability distributions",
          "relevance": 0.67
        },
        {
          "concept": "Irvine repository",
          "relevance": 0.631
        },
        {
          "concept": "supervised learning",
          "relevance": 0.628
        },
        {
          "concept": "induced classifier",
          "relevance": 0.626
        },
        {
          "concept": "wrapper method",
          "relevance": 0.62
        },
        {
          "concept": "classifier",
          "relevance": 0.606
        },
        {
          "concept": "computational simplicity",
          "relevance": 0.571
        },
        {
          "concept": "C4.5",
          "relevance": 0.566
        },
        {
          "concept": "evaluation approach",
          "relevance": 0.525
        },
        {
          "concept": "probability distribution",
          "relevance": 0.524
        },
        {
          "concept": "wrapper",
          "relevance": 0.477
        },
        {
          "concept": "restrictive assumptions",
          "relevance": 0.47
        },
        {
          "concept": "repository",
          "relevance": 0.466
        },
        {
          "concept": "network",
          "relevance": 0.458
        },
        {
          "concept": "robustness",
          "relevance": 0.44
        },
        {
          "concept": "learning",
          "relevance": 0.438
        },
        {
          "concept": "Irvine",
          "relevance": 0.437
        },
        {
          "concept": "representation",
          "relevance": 0.434
        },
        {
          "concept": "method",
          "relevance": 0.416
        },
        {
          "concept": "University of California",
          "relevance": 0.41
        },
        {
          "concept": "Bay",
          "relevance": 0.401
        },
        {
          "concept": "features",
          "relevance": 0.393
        },
        {
          "concept": "simplicity",
          "relevance": 0.391
        },
        {
          "concept": "trees",
          "relevance": 0.369
        },
        {
          "concept": "selection",
          "relevance": 0.34
        },
        {
          "concept": "assumptions",
          "relevance": 0.333
        },
        {
          "concept": "independence",
          "relevance": 0.327
        },
        {
          "concept": "data",
          "relevance": 0.324
        },
        {
          "concept": "time",
          "relevance": 0.31
        },
        {
          "concept": "University",
          "relevance": 0.29
        },
        {
          "concept": "statements",
          "relevance": 0.286
        },
        {
          "concept": "theory",
          "relevance": 0.272
        },
        {
          "concept": "distribution",
          "relevance": 0.26
        },
        {
          "concept": "problem",
          "relevance": 0.182
        },
        {
          "concept": "approach",
          "relevance": 0.18
        },
        {
          "concept": "California",
          "relevance": 0.16
        }
      ]
    },
    {
      "paperId": "pub.1053132543",
      "doi": "10.1162/089976698300017197",
      "title": "Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms",
      "year": 1998,
      "citationCount": 2648,
      "fieldCitationRatio": NaN,
      "abstract": "This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 x 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5 x 2 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, McNemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 x 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.",
      "reference_ids": [
        "pub.1009692954",
        "pub.1045206911",
        "pub.1012041080",
        "pub.1109705929",
        "pub.1025075228",
        "pub.1002929950"
      ],
      "concepts_scores": [
        {
          "concept": "learning algorithms",
          "relevance": 0.514
        },
        {
          "concept": "type I error",
          "relevance": 0.503
        },
        {
          "concept": "low type I error",
          "relevance": 0.492
        },
        {
          "concept": "t-test",
          "relevance": 0.473
        },
        {
          "concept": "McNemar test",
          "relevance": 0.471
        },
        {
          "concept": "choice of training set",
          "relevance": 0.465
        },
        {
          "concept": "classification learning algorithms",
          "relevance": 0.461
        },
        {
          "concept": "random train-test splits",
          "relevance": 0.444
        },
        {
          "concept": "train-test split",
          "relevance": 0.436
        },
        {
          "concept": "cross-validation",
          "relevance": 0.427
        },
        {
          "concept": "statistical tests",
          "relevance": 0.422
        },
        {
          "concept": "paired-differences t test",
          "relevance": 0.421
        },
        {
          "concept": "training set",
          "relevance": 0.409
        },
        {
          "concept": "computational cost",
          "relevance": 0.407
        },
        {
          "concept": "learning tasks",
          "relevance": 0.398
        },
        {
          "concept": "algorithm",
          "relevance": 0.396
        },
        {
          "concept": "probability of type I error",
          "relevance": 0.388
        },
        {
          "concept": "paired-difference",
          "relevance": 0.38
        },
        {
          "concept": "McNemar",
          "relevance": 0.373
        },
        {
          "concept": "learning",
          "relevance": 0.363
        },
        {
          "concept": "No difference",
          "relevance": 0.336
        },
        {
          "concept": "elevated probability",
          "relevance": 0.336
        },
        {
          "concept": "approximate statistical test",
          "relevance": 0.334
        },
        {
          "concept": "type",
          "relevance": 0.322
        },
        {
          "concept": "probability",
          "relevance": 0.307
        },
        {
          "concept": "task",
          "relevance": 0.303
        },
        {
          "concept": "iteration",
          "relevance": 0.302
        },
        {
          "concept": "test",
          "relevance": 0.297
        },
        {
          "concept": "measured variation",
          "relevance": 0.297
        },
        {
          "concept": "sets",
          "relevance": 0.274
        },
        {
          "concept": "variation",
          "relevance": 0.273
        },
        {
          "concept": "proportion",
          "relevance": 0.268
        },
        {
          "concept": "cost",
          "relevance": 0.253
        },
        {
          "concept": "differences",
          "relevance": 0.25
        },
        {
          "concept": "approximation",
          "relevance": 0.238
        },
        {
          "concept": "choice",
          "relevance": 0.238
        },
        {
          "concept": "power",
          "relevance": 0.234
        },
        {
          "concept": "experiments",
          "relevance": 0.232
        },
        {
          "concept": "article",
          "relevance": 0.229
        },
        {
          "concept": "situation",
          "relevance": 0.226
        },
        {
          "concept": "splitting",
          "relevance": 0.196
        }
      ]
    },
    {
      "paperId": "pub.1030392094",
      "doi": "10.1145/1015330.1015338",
      "title": "Estimating replicability of classifier learning experiments",
      "year": 2004,
      "citationCount": 32,
      "fieldCitationRatio": 5.52,
      "abstract": "Replicability of machine learning experiments measures how likely it is that the outcome of one experiment is repeated when performed with a different randomization of the data. In this paper, we present an estimator of replicability of an experiment that is efficient. More precisely, the estimator is unbiased and has lowest variance in the class of estimators formed by a linear combination of outcomes of experiments on a given data set.We gathered empirical data for comparing experiments consisting of different sampling schemes and hypothesis tests. Both factors are shown to have an impact on replicability of experiments. The data suggests that sign tests should not be used due to low replicability. Ranked sum tests show better performance, but the combination of a sorted runs sampling scheme with a t-test gives the most desirable performance judged on Type I and II error and replicability.",
      "reference_ids": NaN,
      "concepts_scores": [
        {
          "concept": "sampling scheme",
          "relevance": 0.509
        },
        {
          "concept": "comparative experiments",
          "relevance": 0.478
        },
        {
          "concept": "replication of experiments",
          "relevance": 0.441
        },
        {
          "concept": "hypothesis testing",
          "relevance": 0.432
        },
        {
          "concept": "low variance",
          "relevance": 0.43
        },
        {
          "concept": "outcome of experiments",
          "relevance": 0.43
        },
        {
          "concept": "linear combination",
          "relevance": 0.426
        },
        {
          "concept": "scheme",
          "relevance": 0.423
        },
        {
          "concept": "estimation",
          "relevance": 0.416
        },
        {
          "concept": "sign test",
          "relevance": 0.403
        },
        {
          "concept": "estimates of replicability",
          "relevance": 0.401
        },
        {
          "concept": "empirical data",
          "relevance": 0.39
        },
        {
          "concept": "performance",
          "relevance": 0.388
        },
        {
          "concept": "learning experience",
          "relevance": 0.359
        },
        {
          "concept": "experiments",
          "relevance": 0.349
        },
        {
          "concept": "randomization",
          "relevance": 0.339
        },
        {
          "concept": "data",
          "relevance": 0.33
        },
        {
          "concept": "impact",
          "relevance": 0.327
        },
        {
          "concept": "hypothesis",
          "relevance": 0.316
        },
        {
          "concept": "variance",
          "relevance": 0.314
        },
        {
          "concept": "type I and",
          "relevance": 0.291
        },
        {
          "concept": "test",
          "relevance": 0.259
        },
        {
          "concept": "t-test",
          "relevance": 0.24
        },
        {
          "concept": "outcomes",
          "relevance": 0.237
        },
        {
          "concept": "experience measures",
          "relevance": 0.234
        },
        {
          "concept": "factors",
          "relevance": 0.226
        },
        {
          "concept": "combination",
          "relevance": 0.225
        },
        {
          "concept": "measurements",
          "relevance": 0.224
        },
        {
          "concept": "I and",
          "relevance": 0.216
        },
        {
          "concept": "replication",
          "relevance": 0.213
        },
        {
          "concept": "low replication",
          "relevance": 0.21
        },
        {
          "concept": "samples",
          "relevance": 0.192
        },
        {
          "concept": "signs",
          "relevance": 0.189
        },
        {
          "concept": "rank sum test",
          "relevance": 0.185
        },
        {
          "concept": "sum test",
          "relevance": 0.181
        },
        {
          "concept": "type",
          "relevance": 0.175
        }
      ]
    }
  ],
  "evolution_links": [
    {
      "source": "pub.1170882028",
      "target": "pub.1151380649",
      "source_title": "A survey on interpretable reinforcement learning",
      "target_title": "High-Resolution Image Synthesis with Latent Diffusion Models"
    },
    {
      "source": "pub.1151380649",
      "target": "pub.1095689025",
      "source_title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "target_title": "ImageNet: A large-scale hierarchical image database"
    },
    {
      "source": "pub.1095689025",
      "target": "pub.1052687286",
      "source_title": "ImageNet: A large-scale hierarchical image database",
      "target_title": "Distinctive Image Features from Scale-Invariant Keypoints"
    },
    {
      "source": "pub.1095689025",
      "target": "pub.1027534025",
      "source_title": "ImageNet: A large-scale hierarchical image database",
      "target_title": "LabelMe: A Database and Web-Based Tool for Image Annotation"
    },
    {
      "source": "pub.1151380649",
      "target": "pub.1095850445",
      "source_title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "target_title": "Image-to-Image Translation with Conditional Adversarial Networks"
    },
    {
      "source": "pub.1095850445",
      "target": "pub.1061640964",
      "source_title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "target_title": "Image Quality Assessment: From Error Visibility to Structural Similarity"
    },
    {
      "source": "pub.1095850445",
      "target": "pub.1093626237",
      "source_title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "target_title": "Fully Convolutional Networks for Semantic Segmentation"
    },
    {
      "source": "pub.1170882028",
      "target": "pub.1148956311",
      "source_title": "A survey on interpretable reinforcement learning",
      "target_title": "Deep Reinforcement Learning That Matters"
    },
    {
      "source": "pub.1148956311",
      "target": "pub.1029295303",
      "source_title": "Deep Reinforcement Learning That Matters",
      "target_title": "Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms"
    },
    {
      "source": "pub.1029295303",
      "target": "pub.1030680500",
      "source_title": "Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms",
      "target_title": "Bayesian Network Classifiers"
    },
    {
      "source": "pub.1029295303",
      "target": "pub.1053132543",
      "source_title": "Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms",
      "target_title": "Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms"
    },
    {
      "source": "pub.1148956311",
      "target": "pub.1030392094",
      "source_title": "Deep Reinforcement Learning That Matters",
      "target_title": "Estimating replicability of classifier learning experiments"
    }
  ]
}