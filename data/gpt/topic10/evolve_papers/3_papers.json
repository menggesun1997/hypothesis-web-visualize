{
  "papers": [
    {
      "paperId": "pub.1157026832",
      "doi": "10.1016/j.media.2023.102802",
      "title": "Transformers in medical imaging: A survey",
      "year": 2023,
      "citationCount": 762,
      "fieldCitationRatio": NaN,
      "abstract": "Following unprecedented success on the natural language tasks, Transformers have been successfully applied to several computer vision problems, achieving state-of-the-art results and prompting researchers to reconsider the supremacy of convolutional neural networks (CNNs) as de facto operators. Capitalizing on these advances in computer vision, the medical imaging field has also witnessed growing interest for Transformers that can capture global context compared to CNNs with local receptive fields. Inspired from this transition, in this survey, we attempt to provide a comprehensive review of the applications of Transformers in medical imaging covering various aspects, ranging from recently proposed architectural designs to unsolved issues. Specifically, we survey the use of Transformers in medical image segmentation, detection, classification, restoration, synthesis, registration, clinical report generation, and other tasks. In particular, for each of these applications, we develop taxonomy, identify application-specific challenges as well as provide insights to solve them, and highlight recent trends. Further, we provide a critical discussion of the field's current state as a whole, including the identification of key challenges, open problems, and outlining promising future directions. We hope this survey will ignite further interest in the community and provide researchers with an up-to-date reference regarding applications of Transformer models in medical imaging. Finally, to cope with the rapid development in this field, we intend to regularly update the relevant latest papers and their open-source implementations at https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging.",
      "reference_ids": [
        "pub.1044365820",
        "pub.1141518696",
        "pub.1103216235",
        "pub.1131076570",
        "pub.1149652590",
        "pub.1141326877",
        "pub.1039831711",
        "pub.1111331308",
        "pub.1091683583",
        "pub.1145901767",
        "pub.1145639403",
        "pub.1136891762",
        "pub.1111907881",
        "pub.1143608087",
        "pub.1121624201",
        "pub.1141395027",
        "pub.1061423477",
        "pub.1140814555",
        "pub.1112586522",
        "pub.1132138203",
        "pub.1144759463",
        "pub.1133875407",
        "pub.1145901944",
        "pub.1146554213",
        "pub.1145901173",
        "pub.1095852454",
        "pub.1141302126",
        "pub.1099710246",
        "pub.1121964315",
        "pub.1141302032",
        "pub.1145901710",
        "pub.1095843442",
        "pub.1061696613",
        "pub.1152036583",
        "pub.1124346722",
        "pub.1104284031",
        "pub.1016812232",
        "pub.1149601994",
        "pub.1149471416",
        "pub.1150860500",
        "pub.1138840605",
        "pub.1061531397",
        "pub.1125156087",
        "pub.1111749023",
        "pub.1136719748",
        "pub.1110720608",
        "pub.1122780174",
        "pub.1127318620",
        "pub.1141326855",
        "pub.1104047828",
        "pub.1149214609",
        "pub.1014346769",
        "pub.1000017712",
        "pub.1084206837",
        "pub.1143755014",
        "pub.1141301943",
        "pub.1135756834",
        "pub.1142372360",
        "pub.1141326797",
        "pub.1146180808",
        "pub.1129757314",
        "pub.1145901979",
        "pub.1128120458",
        "pub.1141302115",
        "pub.1143046041",
        "pub.1069322891",
        "pub.1154350475",
        "pub.1133883747",
        "pub.1143493199",
        "pub.1084228312",
        "pub.1140598192",
        "pub.1154178484",
        "pub.1090904008",
        "pub.1141302093",
        "pub.1023640338",
        "pub.1138871364",
        "pub.1051365551",
        "pub.1141395013",
        "pub.1141081634",
        "pub.1149471423",
        "pub.1120714616",
        "pub.1144245159",
        "pub.1078797520",
        "pub.1131398959",
        "pub.1152408431",
        "pub.1141886506",
        "pub.1141263358",
        "pub.1150931785",
        "pub.1143535931",
        "pub.1042138685",
        "pub.1122629279",
        "pub.1085056202",
        "pub.1141036538",
        "pub.1138955846",
        "pub.1110583306",
        "pub.1141968567",
        "pub.1107646053",
        "pub.1141326865",
        "pub.1144244805",
        "pub.1085994135",
        "pub.1139947508",
        "pub.1135739812",
        "pub.1099653569",
        "pub.1123692889",
        "pub.1141302009",
        "pub.1084908686",
        "pub.1141326851",
        "pub.1113059233",
        "pub.1132028086",
        "pub.1142501013",
        "pub.1149214543",
        "pub.1143949033",
        "pub.1090899803",
        "pub.1141301966",
        "pub.1117659567",
        "pub.1126983463",
        "pub.1127381783",
        "pub.1110923455",
        "pub.1122290240",
        "pub.1121575042",
        "pub.1095850445",
        "pub.1140363536",
        "pub.1142380266",
        "pub.1136744364",
        "pub.1150998877",
        "pub.1134517486",
        "pub.1085304410",
        "pub.1142365944",
        "pub.1148248639",
        "pub.1012146698",
        "pub.1027628596",
        "pub.1144149584",
        "pub.1030964537",
        "pub.1140116757",
        "pub.1148730879",
        "pub.1137440398",
        "pub.1135351858",
        "pub.1121839199",
        "pub.1124405276",
        "pub.1110720947",
        "pub.1142695784",
        "pub.1133515395",
        "pub.1123353636",
        "pub.1106148962",
        "pub.1138297304",
        "pub.1113419194",
        "pub.1128647298",
        "pub.1112925802",
        "pub.1142759719",
        "pub.1128347069",
        "pub.1139216682",
        "pub.1158077385",
        "pub.1111948780",
        "pub.1149652601",
        "pub.1141490071",
        "pub.1110721051",
        "pub.1101302717",
        "pub.1147456387",
        "pub.1141301975",
        "pub.1142385965",
        "pub.1144245054",
        "pub.1147192531",
        "pub.1129507106",
        "pub.1100060309",
        "pub.1141302097",
        "pub.1072988504",
        "pub.1138941772",
        "pub.1132652361",
        "pub.1141302064",
        "pub.1145639555",
        "pub.1130212635",
        "pub.1129941156",
        "pub.1143054437",
        "pub.1141301953",
        "pub.1107669637",
        "pub.1093359587",
        "pub.1149471422",
        "pub.1151380774",
        "pub.1141302046",
        "pub.1145901339",
        "pub.1061696747",
        "pub.1149471417",
        "pub.1143041575",
        "pub.1140596350",
        "pub.1120940405",
        "pub.1134452217",
        "pub.1145901669",
        "pub.1145704643",
        "pub.1127614343",
        "pub.1133176873",
        "pub.1132270339",
        "pub.1141301954",
        "pub.1059743951",
        "pub.1016771773",
        "pub.1146286863",
        "pub.1149214584",
        "pub.1135786328",
        "pub.1086050371",
        "pub.1149471425",
        "pub.1135354008",
        "pub.1125125609",
        "pub.1142366534",
        "pub.1131395532",
        "pub.1095837190",
        "pub.1100748311",
        "pub.1099239594",
        "pub.1143724932",
        "pub.1101199861",
        "pub.1141327016",
        "pub.1092272938",
        "pub.1118170204",
        "pub.1150769730",
        "pub.1111517553",
        "pub.1151381613",
        "pub.1041282192",
        "pub.1120624953",
        "pub.1061696449",
        "pub.1095795137",
        "pub.1092192335",
        "pub.1125894270",
        "pub.1121063041",
        "pub.1128713610",
        "pub.1138044416",
        "pub.1143052300",
        "pub.1108068789",
        "pub.1129975971",
        "pub.1123988062",
        "pub.1150356858",
        "pub.1148404291",
        "pub.1141302149",
        "pub.1145901053",
        "pub.1111273884",
        "pub.1091962600",
        "pub.1145902197",
        "pub.1131270415",
        "pub.1135634620",
        "pub.1112112099",
        "pub.1008345178",
        "pub.1091111450",
        "pub.1061170901",
        "pub.1141326853",
        "pub.1085753273",
        "pub.1004995399",
        "pub.1127252117",
        "pub.1141326976",
        "pub.1141793907",
        "pub.1078754059",
        "pub.1129913554",
        "pub.1033000174",
        "pub.1123988554",
        "pub.1142140188",
        "pub.1095839207",
        "pub.1139947259",
        "pub.1151380786",
        "pub.1095689025",
        "pub.1144734789",
        "pub.1141302118",
        "pub.1151032805",
        "pub.1141395030",
        "pub.1100316721",
        "pub.1104166569",
        "pub.1112145402",
        "pub.1137742786",
        "pub.1127247101",
        "pub.1123267294",
        "pub.1141326981",
        "pub.1101763573",
        "pub.1043676105",
        "pub.1124156666",
        "pub.1122024915",
        "pub.1142428103",
        "pub.1121734354",
        "pub.1149379029",
        "pub.1141302016",
        "pub.1128708772",
        "pub.1061157543",
        "pub.1141326759",
        "pub.1140871626",
        "pub.1145901391",
        "pub.1123670452",
        "pub.1115977982",
        "pub.1127954048",
        "pub.1132499687",
        "pub.1131504681",
        "pub.1142383969",
        "pub.1107065495",
        "pub.1145902000",
        "pub.1125159756",
        "pub.1061641266",
        "pub.1145043474",
        "pub.1141326944",
        "pub.1144481194",
        "pub.1151380320",
        "pub.1142371280",
        "pub.1017774818",
        "pub.1100060663",
        "pub.1100060679",
        "pub.1146815745",
        "pub.1141326803",
        "pub.1148286503",
        "pub.1079018910",
        "pub.1124960155",
        "pub.1095839635",
        "pub.1133174989",
        "pub.1141301944",
        "pub.1129913738",
        "pub.1111751419"
      ],
      "concepts_scores": [
        {
          "concept": "convolutional neural network",
          "relevance": 0.81
        },
        {
          "concept": "medical images",
          "relevance": 0.729
        },
        {
          "concept": "state-of-the-art results",
          "relevance": 0.695
        },
        {
          "concept": "natural language tasks",
          "relevance": 0.674
        },
        {
          "concept": "computer vision problems",
          "relevance": 0.674
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.67
        },
        {
          "concept": "medical image segmentation",
          "relevance": 0.67
        },
        {
          "concept": "local receptive fields",
          "relevance": 0.666
        },
        {
          "concept": "open-source implementation",
          "relevance": 0.658
        },
        {
          "concept": "medical imaging field",
          "relevance": 0.657
        },
        {
          "concept": "application of transformations",
          "relevance": 0.627
        },
        {
          "concept": "computer vision",
          "relevance": 0.621
        },
        {
          "concept": "image segmentation",
          "relevance": 0.611
        },
        {
          "concept": "neural network",
          "relevance": 0.607
        },
        {
          "concept": "vision problems",
          "relevance": 0.601
        },
        {
          "concept": "report generation",
          "relevance": 0.591
        },
        {
          "concept": "receptive fields",
          "relevance": 0.561
        },
        {
          "concept": "architectural design",
          "relevance": 0.558
        },
        {
          "concept": "transformation model",
          "relevance": 0.556
        },
        {
          "concept": "open problems",
          "relevance": 0.542
        },
        {
          "concept": "imaging field",
          "relevance": 0.537
        },
        {
          "concept": "computer",
          "relevance": 0.523
        },
        {
          "concept": "current state",
          "relevance": 0.517
        },
        {
          "concept": "task",
          "relevance": 0.509
        },
        {
          "concept": "images",
          "relevance": 0.486
        },
        {
          "concept": "field's current state",
          "relevance": 0.469
        },
        {
          "concept": "applications",
          "relevance": 0.462
        },
        {
          "concept": "transformation",
          "relevance": 0.457
        },
        {
          "concept": "network",
          "relevance": 0.452
        },
        {
          "concept": "classification",
          "relevance": 0.436
        },
        {
          "concept": "language tasks",
          "relevance": 0.434
        },
        {
          "concept": "implementation",
          "relevance": 0.416
        },
        {
          "concept": "vision",
          "relevance": 0.414
        },
        {
          "concept": "taxonomy",
          "relevance": 0.393
        },
        {
          "concept": "detection",
          "relevance": 0.388
        },
        {
          "concept": "segments",
          "relevance": 0.38
        },
        {
          "concept": "registration",
          "relevance": 0.38
        },
        {
          "concept": "research",
          "relevance": 0.379
        },
        {
          "concept": "operation",
          "relevance": 0.369
        },
        {
          "concept": "comprehensive review",
          "relevance": 0.368
        },
        {
          "concept": "issues",
          "relevance": 0.363
        },
        {
          "concept": "design",
          "relevance": 0.354
        },
        {
          "concept": "field",
          "relevance": 0.353
        },
        {
          "concept": "model",
          "relevance": 0.335
        },
        {
          "concept": "generation",
          "relevance": 0.314
        },
        {
          "concept": "supremacy",
          "relevance": 0.31
        },
        {
          "concept": "success",
          "relevance": 0.309
        },
        {
          "concept": "results",
          "relevance": 0.309
        },
        {
          "concept": "identification",
          "relevance": 0.3
        },
        {
          "concept": "direction",
          "relevance": 0.299
        },
        {
          "concept": "survey",
          "relevance": 0.293
        },
        {
          "concept": "development",
          "relevance": 0.276
        },
        {
          "concept": "discussion",
          "relevance": 0.261
        },
        {
          "concept": "community",
          "relevance": 0.26
        },
        {
          "concept": "transition",
          "relevance": 0.254
        },
        {
          "concept": "trends",
          "relevance": 0.252
        },
        {
          "concept": "whole",
          "relevance": 0.247
        },
        {
          "concept": "review",
          "relevance": 0.237
        },
        {
          "concept": "restoration",
          "relevance": 0.232
        },
        {
          "concept": "state",
          "relevance": 0.229
        },
        {
          "concept": "problem",
          "relevance": 0.208
        },
        {
          "concept": "paper",
          "relevance": 0.163
        },
        {
          "concept": "synthesis",
          "relevance": 0.159
        }
      ]
    },
    {
      "paperId": "pub.1151380774",
      "doi": "10.1109/cvpr52688.2022.01167",
      "title": "A ConvNet for the 2020s",
      "year": 2022,
      "citationCount": 5223,
      "fieldCitationRatio": 2088.02,
      "abstract": "The “Roaring 20s” of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually “modernize” a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.",
      "reference_ids": [
        "pub.1095689025",
        "pub.1107463261",
        "pub.1145901668",
        "pub.1061179979",
        "pub.1128647309",
        "pub.1132137236",
        "pub.1095420134",
        "pub.1110720947",
        "pub.1145901002",
        "pub.1085642448",
        "pub.1123987771",
        "pub.1094455081",
        "pub.1094291017",
        "pub.1110720778",
        "pub.1056860333",
        "pub.1142378417",
        "pub.1145901816",
        "pub.1145901976",
        "pub.1125161350",
        "pub.1145901979",
        "pub.1129913539",
        "pub.1100060307",
        "pub.1095850372",
        "pub.1110720879",
        "pub.1061156724",
        "pub.1094727707",
        "pub.1107502659",
        "pub.1119749418",
        "pub.1095837190",
        "pub.1009767488",
        "pub.1125159756",
        "pub.1151381159",
        "pub.1151380780",
        "pub.1062844374",
        "pub.1045321436",
        "pub.1099341273",
        "pub.1039042955",
        "pub.1095843442",
        "pub.1110720608",
        "pub.1093497718",
        "pub.1125162660",
        "pub.1061745117",
        "pub.1129724168",
        "pub.1003844693",
        "pub.1110448859",
        "pub.1145901003",
        "pub.1008345178",
        "pub.1093359587",
        "pub.1095573598",
        "pub.1142380802"
      ],
      "concepts_scores": [
        {
          "concept": "vision tasks",
          "relevance": 0.699
        },
        {
          "concept": "vision transformer",
          "relevance": 0.695
        },
        {
          "concept": "state-of-the-art image classification models",
          "relevance": 0.668
        },
        {
          "concept": "ImageNet top-1 accuracy",
          "relevance": 0.662
        },
        {
          "concept": "ADE20K segmentation",
          "relevance": 0.652
        },
        {
          "concept": "computer vision tasks",
          "relevance": 0.651
        },
        {
          "concept": "top-1 accuracy",
          "relevance": 0.648
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.647
        },
        {
          "concept": "superiority of Transformer",
          "relevance": 0.647
        },
        {
          "concept": "image classification model",
          "relevance": 0.645
        },
        {
          "concept": "vanilla ViT",
          "relevance": 0.605
        },
        {
          "concept": "vision backbones",
          "relevance": 0.605
        },
        {
          "concept": "COCO detection",
          "relevance": 0.603
        },
        {
          "concept": "semantic segmentation",
          "relevance": 0.6
        },
        {
          "concept": "object detection",
          "relevance": 0.599
        },
        {
          "concept": "Swin Transformer",
          "relevance": 0.599
        },
        {
          "concept": "hierarchical transformation",
          "relevance": 0.589
        },
        {
          "concept": "classification model",
          "relevance": 0.578
        },
        {
          "concept": "ConvNets",
          "relevance": 0.574
        },
        {
          "concept": "visual recognition",
          "relevance": 0.564
        },
        {
          "concept": "design space",
          "relevance": 0.511
        },
        {
          "concept": "task",
          "relevance": 0.491
        },
        {
          "concept": "performance differences",
          "relevance": 0.483
        },
        {
          "concept": "face difficulties",
          "relevance": 0.479
        },
        {
          "concept": "ImageNet",
          "relevance": 0.473
        },
        {
          "concept": "Swin",
          "relevance": 0.47
        },
        {
          "concept": "ConvNeXt",
          "relevance": 0.469
        },
        {
          "concept": "ResNet",
          "relevance": 0.468
        },
        {
          "concept": "accuracy",
          "relevance": 0.468
        },
        {
          "concept": "vision",
          "relevance": 0.463
        },
        {
          "concept": "scalability",
          "relevance": 0.46
        },
        {
          "concept": "K-segments",
          "relevance": 0.46
        },
        {
          "concept": "convolution",
          "relevance": 0.454
        },
        {
          "concept": "performance",
          "relevance": 0.45
        },
        {
          "concept": "transformation",
          "relevance": 0.442
        },
        {
          "concept": "computer",
          "relevance": 0.435
        },
        {
          "concept": "COCO",
          "relevance": 0.435
        },
        {
          "concept": "detection",
          "relevance": 0.433
        },
        {
          "concept": "vanilla",
          "relevance": 0.432
        },
        {
          "concept": "priors",
          "relevance": 0.427
        },
        {
          "concept": "recognition",
          "relevance": 0.407
        },
        {
          "concept": "design",
          "relevance": 0.396
        },
        {
          "concept": "Vit",
          "relevance": 0.388
        },
        {
          "concept": "model",
          "relevance": 0.374
        },
        {
          "concept": "simplicity",
          "relevance": 0.372
        },
        {
          "concept": "segments",
          "relevance": 0.367
        },
        {
          "concept": "modulation",
          "relevance": 0.365
        },
        {
          "concept": "objective",
          "relevance": 0.357
        },
        {
          "concept": "space",
          "relevance": 0.344
        },
        {
          "concept": "exploration",
          "relevance": 0.34
        },
        {
          "concept": "efficiency",
          "relevance": 0.339
        },
        {
          "concept": "intrinsic superiority",
          "relevance": 0.335
        },
        {
          "concept": "backbone",
          "relevance": 0.321
        },
        {
          "concept": "difficulties",
          "relevance": 0.306
        },
        {
          "concept": "roars",
          "relevance": 0.278
        },
        {
          "concept": "limitations",
          "relevance": 0.278
        },
        {
          "concept": "components",
          "relevance": 0.274
        },
        {
          "concept": "introduction",
          "relevance": 0.253
        },
        {
          "concept": "modernization",
          "relevance": 0.235
        },
        {
          "concept": "bias",
          "relevance": 0.218
        },
        {
          "concept": "outcomes",
          "relevance": 0.212
        },
        {
          "concept": "effect",
          "relevance": 0.194
        },
        {
          "concept": "differences",
          "relevance": 0.183
        },
        {
          "concept": "family",
          "relevance": 0.18
        }
      ]
    },
    {
      "paperId": "pub.1145901979",
      "doi": "10.1109/iccv48922.2021.00986",
      "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "year": 2021,
      "citationCount": 23470,
      "fieldCitationRatio": 7314.36,
      "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",
      "reference_ids": [
        "pub.1042379868",
        "pub.1123987989",
        "pub.1107463261",
        "pub.1123987804",
        "pub.1129913473",
        "pub.1132404102",
        "pub.1093359587",
        "pub.1037811822",
        "pub.1132652361",
        "pub.1045321436",
        "pub.1095843442",
        "pub.1094291017",
        "pub.1096897141",
        "pub.1061179979",
        "pub.1125159756",
        "pub.1110720511",
        "pub.1142378417",
        "pub.1100060679",
        "pub.1095852454",
        "pub.1125155892",
        "pub.1110720512",
        "pub.1107454784",
        "pub.1129913504",
        "pub.1142371099",
        "pub.1123988431",
        "pub.1129913539",
        "pub.1132137236",
        "pub.1017774818",
        "pub.1142385772",
        "pub.1110720947",
        "pub.1145901054",
        "pub.1129913310",
        "pub.1145901053",
        "pub.1126062535",
        "pub.1110720778",
        "pub.1062844374",
        "pub.1100060607",
        "pub.1125458068",
        "pub.1132270339",
        "pub.1110448859",
        "pub.1125154425",
        "pub.1095850372",
        "pub.1107454614",
        "pub.1100060307",
        "pub.1017885690",
        "pub.1142371280",
        "pub.1142392935",
        "pub.1095689025"
      ],
      "concepts_scores": [
        {
          "concept": "vision transformer",
          "relevance": 0.718
        },
        {
          "concept": "Swin Transformer",
          "relevance": 0.717
        },
        {
          "concept": "hierarchical vision transformer",
          "relevance": 0.674
        },
        {
          "concept": "self-attention computation",
          "relevance": 0.674
        },
        {
          "concept": "cross-window connections",
          "relevance": 0.674
        },
        {
          "concept": "dense prediction tasks",
          "relevance": 0.674
        },
        {
          "concept": "transformer-based models",
          "relevance": 0.667
        },
        {
          "concept": "ADE20K",
          "relevance": 0.625
        },
        {
          "concept": "vision backbones",
          "relevance": 0.625
        },
        {
          "concept": "vision tasks",
          "relevance": 0.623
        },
        {
          "concept": "mask AP",
          "relevance": 0.621
        },
        {
          "concept": "semantic segmentation",
          "relevance": 0.62
        },
        {
          "concept": "computer vision",
          "relevance": 0.619
        },
        {
          "concept": "object detection",
          "relevance": 0.619
        },
        {
          "concept": "image classification",
          "relevance": 0.615
        },
        {
          "concept": "visual entities",
          "relevance": 0.614
        },
        {
          "concept": "resolution of pixels",
          "relevance": 0.611
        },
        {
          "concept": "computational complexity",
          "relevance": 0.609
        },
        {
          "concept": "hierarchical transformation",
          "relevance": 0.608
        },
        {
          "concept": "prediction task",
          "relevance": 0.608
        },
        {
          "concept": "local window",
          "relevance": 0.606
        },
        {
          "concept": "image size",
          "relevance": 0.602
        },
        {
          "concept": "window scheme",
          "relevance": 0.601
        },
        {
          "concept": "shifted windows",
          "relevance": 0.598
        },
        {
          "concept": "hierarchical architecture",
          "relevance": 0.56
        },
        {
          "concept": "hierarchical design",
          "relevance": 0.538
        },
        {
          "concept": "architecture",
          "relevance": 0.525
        },
        {
          "concept": "computer",
          "relevance": 0.521
        },
        {
          "concept": "vision",
          "relevance": 0.514
        },
        {
          "concept": "task",
          "relevance": 0.507
        },
        {
          "concept": "mIoU",
          "relevance": 0.486
        },
        {
          "concept": "Swin",
          "relevance": 0.485
        },
        {
          "concept": "images",
          "relevance": 0.485
        },
        {
          "concept": "pixel",
          "relevance": 0.463
        },
        {
          "concept": "transformation",
          "relevance": 0.456
        },
        {
          "concept": "COCO",
          "relevance": 0.45
        },
        {
          "concept": "code",
          "relevance": 0.445
        },
        {
          "concept": "scheme",
          "relevance": 0.438
        },
        {
          "concept": "window",
          "relevance": 0.436
        },
        {
          "concept": "classification",
          "relevance": 0.434
        },
        {
          "concept": "representation",
          "relevance": 0.427
        },
        {
          "concept": "hierarchically",
          "relevance": 0.415
        },
        {
          "concept": "language",
          "relevance": 0.402
        },
        {
          "concept": "performance",
          "relevance": 0.402
        },
        {
          "concept": "flexibility",
          "relevance": 0.387
        },
        {
          "concept": "detection",
          "relevance": 0.387
        },
        {
          "concept": "mask",
          "relevance": 0.387
        },
        {
          "concept": "model",
          "relevance": 0.387
        },
        {
          "concept": "backbone",
          "relevance": 0.384
        },
        {
          "concept": "domain",
          "relevance": 0.381
        },
        {
          "concept": "segments",
          "relevance": 0.379
        },
        {
          "concept": "words",
          "relevance": 0.375
        },
        {
          "concept": "objective",
          "relevance": 0.369
        },
        {
          "concept": "entities",
          "relevance": 0.364
        },
        {
          "concept": "connection",
          "relevance": 0.363
        },
        {
          "concept": "design",
          "relevance": 0.353
        },
        {
          "concept": "efficiency",
          "relevance": 0.35
        },
        {
          "concept": "challenges",
          "relevance": 0.344
        },
        {
          "concept": "quality",
          "relevance": 0.344
        },
        {
          "concept": "complex",
          "relevance": 0.335
        },
        {
          "concept": "resolution",
          "relevance": 0.31
        },
        {
          "concept": "size",
          "relevance": 0.261
        },
        {
          "concept": "scale",
          "relevance": 0.233
        },
        {
          "concept": "variation",
          "relevance": 0.221
        },
        {
          "concept": "margin",
          "relevance": 0.218
        },
        {
          "concept": "potential",
          "relevance": 0.215
        },
        {
          "concept": "differences",
          "relevance": 0.185
        }
      ]
    },
    {
      "paperId": "pub.1132270339",
      "doi": "10.1007/978-3-030-58452-8_13",
      "title": "End-to-End Object Detection with Transformers",
      "year": 2020,
      "citationCount": 13281,
      "fieldCitationRatio": 3743.11,
      "abstract": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
      "reference_ids": [
        "pub.1100060572",
        "pub.1095738839",
        "pub.1125160668",
        "pub.1120932896",
        "pub.1110720947",
        "pub.1122290650",
        "pub.1095474558",
        "pub.1095839166",
        "pub.1123988134",
        "pub.1093359587",
        "pub.1017177111",
        "pub.1053318791",
        "pub.1095811486",
        "pub.1045321436",
        "pub.1123988380",
        "pub.1122988422",
        "pub.1123988441",
        "pub.1093838265",
        "pub.1125159756",
        "pub.1100060307",
        "pub.1129913473",
        "pub.1094073194",
        "pub.1100060607",
        "pub.1100060309",
        "pub.1125161029",
        "pub.1032778056",
        "pub.1095851355",
        "pub.1126961449",
        "pub.1110720778",
        "pub.1061745117",
        "pub.1095852454",
        "pub.1110720512"
      ],
      "concepts_scores": [
        {
          "concept": "object detection",
          "relevance": 0.726
        },
        {
          "concept": "end-to-end object detection",
          "relevance": 0.708
        },
        {
          "concept": "non-maximum suppression procedure",
          "relevance": 0.701
        },
        {
          "concept": "COCO object detection dataset",
          "relevance": 0.701
        },
        {
          "concept": "transformer encoder-decoder architecture",
          "relevance": 0.698
        },
        {
          "concept": "hand-designed components",
          "relevance": 0.683
        },
        {
          "concept": "forces unique predictions",
          "relevance": 0.683
        },
        {
          "concept": "learned object queries",
          "relevance": 0.683
        },
        {
          "concept": "object detection datasets",
          "relevance": 0.681
        },
        {
          "concept": "global image context",
          "relevance": 0.68
        },
        {
          "concept": "encoder-decoder architecture",
          "relevance": 0.678
        },
        {
          "concept": "run-time performance",
          "relevance": 0.673
        },
        {
          "concept": "end-to-end",
          "relevance": 0.664
        },
        {
          "concept": "anchor generation",
          "relevance": 0.631
        },
        {
          "concept": "object queries",
          "relevance": 0.631
        },
        {
          "concept": "panoptic segmentation",
          "relevance": 0.631
        },
        {
          "concept": "competitive baselines",
          "relevance": 0.631
        },
        {
          "concept": "detection dataset",
          "relevance": 0.63
        },
        {
          "concept": "pretrained models",
          "relevance": 0.625
        },
        {
          "concept": "training code",
          "relevance": 0.623
        },
        {
          "concept": "image context",
          "relevance": 0.619
        },
        {
          "concept": "detection pipeline",
          "relevance": 0.613
        },
        {
          "concept": "prediction problem",
          "relevance": 0.596
        },
        {
          "concept": "bipartite matching",
          "relevance": 0.586
        },
        {
          "concept": "detectable transformation",
          "relevance": 0.574
        },
        {
          "concept": "DETR",
          "relevance": 0.521
        },
        {
          "concept": "modern detectors",
          "relevance": 0.517
        },
        {
          "concept": "query",
          "relevance": 0.489
        },
        {
          "concept": "detection",
          "relevance": 0.484
        },
        {
          "concept": "dataset",
          "relevance": 0.462
        },
        {
          "concept": "architecture",
          "relevance": 0.459
        },
        {
          "concept": "COCO",
          "relevance": 0.455
        },
        {
          "concept": "code",
          "relevance": 0.451
        },
        {
          "concept": "transformation",
          "relevance": 0.45
        },
        {
          "concept": "suppression procedure",
          "relevance": 0.447
        },
        {
          "concept": "task",
          "relevance": 0.443
        },
        {
          "concept": "prediction",
          "relevance": 0.424
        },
        {
          "concept": "accuracy",
          "relevance": 0.423
        },
        {
          "concept": "matching",
          "relevance": 0.414
        },
        {
          "concept": "pipeline",
          "relevance": 0.411
        },
        {
          "concept": "framework",
          "relevance": 0.409
        },
        {
          "concept": "performance",
          "relevance": 0.407
        },
        {
          "concept": "training",
          "relevance": 0.404
        },
        {
          "concept": "model",
          "relevance": 0.392
        },
        {
          "concept": "segments",
          "relevance": 0.384
        },
        {
          "concept": "objective",
          "relevance": 0.374
        },
        {
          "concept": "detector",
          "relevance": 0.36
        },
        {
          "concept": "method",
          "relevance": 0.358
        },
        {
          "concept": "knowledge",
          "relevance": 0.355
        },
        {
          "concept": "reasons",
          "relevance": 0.348
        },
        {
          "concept": "context",
          "relevance": 0.336
        },
        {
          "concept": "anchor",
          "relevance": 0.318
        },
        {
          "concept": "generation",
          "relevance": 0.317
        },
        {
          "concept": "unique prediction",
          "relevance": 0.29
        },
        {
          "concept": "baseline",
          "relevance": 0.29
        },
        {
          "concept": "components",
          "relevance": 0.287
        },
        {
          "concept": "relations",
          "relevance": 0.276
        },
        {
          "concept": "procedure",
          "relevance": 0.253
        },
        {
          "concept": "ingredients",
          "relevance": 0.179
        },
        {
          "concept": "problem",
          "relevance": 0.17
        }
      ]
    },
    {
      "paperId": "pub.1145901054",
      "doi": "10.1109/iccv48922.2021.00061",
      "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
      "year": 2021,
      "citationCount": 3748,
      "fieldCitationRatio": 1168.05,
      "abstract": "Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope that PVT could, serre as an alternative and useful backbone for pixel-level predictions and facilitate future research.",
      "reference_ids": [
        "pub.1061745111",
        "pub.1061745117",
        "pub.1142372225",
        "pub.1142392935",
        "pub.1095850372",
        "pub.1095332913",
        "pub.1014796149",
        "pub.1094291017",
        "pub.1017177111",
        "pub.1131395532",
        "pub.1123988434",
        "pub.1110720778",
        "pub.1120729618",
        "pub.1105386507",
        "pub.1128647309",
        "pub.1093359587",
        "pub.1123987538",
        "pub.1145901461",
        "pub.1129913504",
        "pub.1138524243",
        "pub.1093626237",
        "pub.1107454614",
        "pub.1100060307",
        "pub.1125154578",
        "pub.1113830167",
        "pub.1123988134",
        "pub.1095852454",
        "pub.1132270339",
        "pub.1132572459",
        "pub.1093497718",
        "pub.1137750781",
        "pub.1110720879",
        "pub.1085642448",
        "pub.1093192734",
        "pub.1009767488",
        "pub.1095689025",
        "pub.1148955875",
        "pub.1125161350",
        "pub.1017774818",
        "pub.1144406263",
        "pub.1085304410",
        "pub.1126862968",
        "pub.1095839207",
        "pub.1125160668",
        "pub.1045321436",
        "pub.1132653601",
        "pub.1141301947",
        "pub.1145901053",
        "pub.1146314706",
        "pub.1110720947",
        "pub.1123987495",
        "pub.1117799804",
        "pub.1125159756",
        "pub.1095843442",
        "pub.1140363594",
        "pub.1129913716",
        "pub.1100060309"
      ],
      "concepts_scores": [
        {
          "concept": "Pyramid Vision Transformer",
          "relevance": 0.85
        },
        {
          "concept": "convolutional neural network",
          "relevance": 0.834
        },
        {
          "concept": "dense prediction tasks",
          "relevance": 0.792
        },
        {
          "concept": "dense prediction",
          "relevance": 0.73
        },
        {
          "concept": "prediction task",
          "relevance": 0.716
        },
        {
          "concept": "convolutional neural network backbone",
          "relevance": 0.7
        },
        {
          "concept": "Vision Transformer (ViT",
          "relevance": 0.696
        },
        {
          "concept": "pixel-level predictions",
          "relevance": 0.673
        },
        {
          "concept": "low resolution output",
          "relevance": 0.661
        },
        {
          "concept": "vision tasks",
          "relevance": 0.633
        },
        {
          "concept": "COCO dataset",
          "relevance": 0.632
        },
        {
          "concept": "downstream tasks",
          "relevance": 0.631
        },
        {
          "concept": "computer vision",
          "relevance": 0.629
        },
        {
          "concept": "vision transformer",
          "relevance": 0.629
        },
        {
          "concept": "object detection",
          "relevance": 0.629
        },
        {
          "concept": "semantic segmentation",
          "relevance": 0.629
        },
        {
          "concept": "image classification",
          "relevance": 0.625
        },
        {
          "concept": "memory cost",
          "relevance": 0.619
        },
        {
          "concept": "dense partitions",
          "relevance": 0.615
        },
        {
          "concept": "neural network",
          "relevance": 0.614
        },
        {
          "concept": "output resolution",
          "relevance": 0.606
        },
        {
          "concept": "extensive experiments",
          "relevance": 0.579
        },
        {
          "concept": "convolution",
          "relevance": 0.551
        },
        {
          "concept": "task",
          "relevance": 0.55
        },
        {
          "concept": "computer",
          "relevance": 0.529
        },
        {
          "concept": "vision",
          "relevance": 0.486
        },
        {
          "concept": "images",
          "relevance": 0.469
        },
        {
          "concept": "pyramid",
          "relevance": 0.467
        },
        {
          "concept": "instances",
          "relevance": 0.466
        },
        {
          "concept": "dataset",
          "relevance": 0.464
        },
        {
          "concept": "network",
          "relevance": 0.457
        },
        {
          "concept": "COCO",
          "relevance": 0.457
        },
        {
          "concept": "output",
          "relevance": 0.45
        },
        {
          "concept": "classification",
          "relevance": 0.441
        },
        {
          "concept": "transformation",
          "relevance": 0.43
        },
        {
          "concept": "prediction",
          "relevance": 0.425
        },
        {
          "concept": "AP",
          "relevance": 0.423
        },
        {
          "concept": "backbone",
          "relevance": 0.417
        },
        {
          "concept": "performance",
          "relevance": 0.408
        },
        {
          "concept": "Vit",
          "relevance": 0.407
        },
        {
          "concept": "memory",
          "relevance": 0.394
        },
        {
          "concept": "partitioning",
          "relevance": 0.392
        },
        {
          "concept": "detection",
          "relevance": 0.392
        },
        {
          "concept": "segments",
          "relevance": 0.385
        },
        {
          "concept": "objective",
          "relevance": 0.375
        },
        {
          "concept": "cost",
          "relevance": 0.371
        },
        {
          "concept": "art",
          "relevance": 0.349
        },
        {
          "concept": "versatility",
          "relevance": 0.346
        },
        {
          "concept": "experiments",
          "relevance": 0.34
        },
        {
          "concept": "research",
          "relevance": 0.331
        },
        {
          "concept": "difficulties",
          "relevance": 0.321
        },
        {
          "concept": "resolution",
          "relevance": 0.315
        },
        {
          "concept": "success",
          "relevance": 0.313
        },
        {
          "concept": "parameters",
          "relevance": 0.296
        },
        {
          "concept": "densely",
          "relevance": 0.295
        },
        {
          "concept": "replacement",
          "relevance": 0.208
        }
      ]
    },
    {
      "paperId": "pub.1110720947",
      "doi": "10.1109/cvpr.2018.00813",
      "title": "Non-local Neural Networks",
      "year": 2018,
      "citationCount": 9726,
      "fieldCitationRatio": 2244.59,
      "abstract": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.",
      "reference_ids": [
        "pub.1038140272",
        "pub.1094644814",
        "pub.1018367015",
        "pub.1061744395",
        "pub.1095367750",
        "pub.1095991069",
        "pub.1100060555",
        "pub.1095805528",
        "pub.1008345178",
        "pub.1093345646",
        "pub.1093828312",
        "pub.1061717465",
        "pub.1095852454",
        "pub.1003201030",
        "pub.1095851879",
        "pub.1092541367",
        "pub.1061745117",
        "pub.1094250503",
        "pub.1095836985",
        "pub.1078707473",
        "pub.1094679398",
        "pub.1100060547",
        "pub.1095367828",
        "pub.1093254042",
        "pub.1094854085",
        "pub.1094614899",
        "pub.1104413659",
        "pub.1045321436",
        "pub.1093537334",
        "pub.1095851738",
        "pub.1025118676",
        "pub.1009469917",
        "pub.1093538034",
        "pub.1095850372",
        "pub.1093359587",
        "pub.1009767488",
        "pub.1095850500"
      ],
      "concepts_scores": [
        {
          "concept": "non-local operators",
          "relevance": 0.723
        },
        {
          "concept": "task of video classification",
          "relevance": 0.718
        },
        {
          "concept": "non-local means method",
          "relevance": 0.715
        },
        {
          "concept": "computer vision architectures",
          "relevance": 0.695
        },
        {
          "concept": "static image recognition",
          "relevance": 0.69
        },
        {
          "concept": "suite of tasks",
          "relevance": 0.68
        },
        {
          "concept": "Charades dataset",
          "relevance": 0.649
        },
        {
          "concept": "object detection/segmentation",
          "relevance": 0.649
        },
        {
          "concept": "video classification",
          "relevance": 0.647
        },
        {
          "concept": "vision architecture",
          "relevance": 0.644
        },
        {
          "concept": "computer vision",
          "relevance": 0.643
        },
        {
          "concept": "image recognition",
          "relevance": 0.638
        },
        {
          "concept": "local neighborhood",
          "relevance": 0.611
        },
        {
          "concept": "competition winners",
          "relevance": 0.601
        },
        {
          "concept": "weighted sum",
          "relevance": 0.57
        },
        {
          "concept": "recurrent operation",
          "relevance": 0.56
        },
        {
          "concept": "mean method",
          "relevance": 0.547
        },
        {
          "concept": "computer",
          "relevance": 0.54
        },
        {
          "concept": "task",
          "relevance": 0.526
        },
        {
          "concept": "building blocks",
          "relevance": 0.515
        },
        {
          "concept": "detection/segmentation",
          "relevance": 0.495
        },
        {
          "concept": "dataset",
          "relevance": 0.474
        },
        {
          "concept": "architecture",
          "relevance": 0.471
        },
        {
          "concept": "COCO",
          "relevance": 0.467
        },
        {
          "concept": "Charades",
          "relevance": 0.465
        },
        {
          "concept": "operation",
          "relevance": 0.463
        },
        {
          "concept": "code",
          "relevance": 0.462
        },
        {
          "concept": "classification",
          "relevance": 0.451
        },
        {
          "concept": "block",
          "relevance": 0.441
        },
        {
          "concept": "recognition",
          "relevance": 0.436
        },
        {
          "concept": "non-local model",
          "relevance": 0.433
        },
        {
          "concept": "vision",
          "relevance": 0.429
        },
        {
          "concept": "features",
          "relevance": 0.401
        },
        {
          "concept": "model",
          "relevance": 0.401
        },
        {
          "concept": "whistles",
          "relevance": 0.387
        },
        {
          "concept": "objective",
          "relevance": 0.383
        },
        {
          "concept": "neighborhood",
          "relevance": 0.38
        },
        {
          "concept": "sum",
          "relevance": 0.378
        },
        {
          "concept": "winners",
          "relevance": 0.378
        },
        {
          "concept": "method",
          "relevance": 0.367
        },
        {
          "concept": "building",
          "relevance": 0.364
        },
        {
          "concept": "estimation",
          "relevance": 0.356
        },
        {
          "concept": "nonlocal model",
          "relevance": 0.352
        },
        {
          "concept": "position",
          "relevance": 0.298
        },
        {
          "concept": "Bell",
          "relevance": 0.255
        },
        {
          "concept": "kinetics",
          "relevance": 0.244
        },
        {
          "concept": "response",
          "relevance": 0.17
        }
      ]
    },
    {
      "paperId": "pub.1095852454",
      "doi": "10.1109/cvpr.2017.106",
      "title": "Feature Pyramid Networks for Object Detection",
      "year": 2017,
      "citationCount": 22424,
      "fieldCitationRatio": 4520.41,
      "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",
      "reference_ids": [
        "pub.1045321436",
        "pub.1093626237",
        "pub.1017177111",
        "pub.1093997066",
        "pub.1052687286",
        "pub.1001688760",
        "pub.1094046638",
        "pub.1017774818",
        "pub.1094224567",
        "pub.1014346769",
        "pub.1008345178",
        "pub.1030874879",
        "pub.1061744812",
        "pub.1049647714",
        "pub.1009767488",
        "pub.1061745117",
        "pub.1030406568",
        "pub.1093700510",
        "pub.1093359587",
        "pub.1033900312",
        "pub.1085642448",
        "pub.1094727707",
        "pub.1096897144",
        "pub.1061743745",
        "pub.1096897042",
        "pub.1047850584",
        "pub.1095646840",
        "pub.1061744626",
        "pub.1061745147",
        "pub.1096897141",
        "pub.1095379009",
        "pub.1095573598",
        "pub.1095686079",
        "pub.1056860333"
      ],
      "concepts_scores": [
        {
          "concept": "feature pyramid network",
          "relevance": 0.841
        },
        {
          "concept": "feature pyramid",
          "relevance": 0.732
        },
        {
          "concept": "pyramid network",
          "relevance": 0.729
        },
        {
          "concept": "object detection",
          "relevance": 0.727
        },
        {
          "concept": "multi-scale object detection",
          "relevance": 0.7
        },
        {
          "concept": "deep learning object detector",
          "relevance": 0.689
        },
        {
          "concept": "Faster R-CNN system",
          "relevance": 0.687
        },
        {
          "concept": "COCO detection benchmark",
          "relevance": 0.683
        },
        {
          "concept": "single-model entries",
          "relevance": 0.683
        },
        {
          "concept": "semantic feature maps",
          "relevance": 0.68
        },
        {
          "concept": "learning object detectors",
          "relevance": 0.677
        },
        {
          "concept": "single-model results",
          "relevance": 0.662
        },
        {
          "concept": "top-down architecture",
          "relevance": 0.651
        },
        {
          "concept": "detection benchmarks",
          "relevance": 0.631
        },
        {
          "concept": "pyramid representation",
          "relevance": 0.63
        },
        {
          "concept": "object detectors",
          "relevance": 0.629
        },
        {
          "concept": "feature maps",
          "relevance": 0.628
        },
        {
          "concept": "recognition system",
          "relevance": 0.622
        },
        {
          "concept": "marginal extra cost",
          "relevance": 0.619
        },
        {
          "concept": "detect objects",
          "relevance": 0.618
        },
        {
          "concept": "pyramidal hierarchy",
          "relevance": 0.599
        },
        {
          "concept": "multi-scale",
          "relevance": 0.579
        },
        {
          "concept": "lateral connections",
          "relevance": 0.558
        },
        {
          "concept": "architecture",
          "relevance": 0.532
        },
        {
          "concept": "network",
          "relevance": 0.529
        },
        {
          "concept": "COCO",
          "relevance": 0.528
        },
        {
          "concept": "accurate solutions",
          "relevance": 0.516
        },
        {
          "concept": "pyramid",
          "relevance": 0.49
        },
        {
          "concept": "GPU",
          "relevance": 0.488
        },
        {
          "concept": "FPS",
          "relevance": 0.481
        },
        {
          "concept": "features",
          "relevance": 0.477
        },
        {
          "concept": "extra costs",
          "relevance": 0.462
        },
        {
          "concept": "detection",
          "relevance": 0.454
        },
        {
          "concept": "benchmarks",
          "relevance": 0.452
        },
        {
          "concept": "code",
          "relevance": 0.451
        },
        {
          "concept": "objective",
          "relevance": 0.433
        },
        {
          "concept": "representation",
          "relevance": 0.433
        },
        {
          "concept": "system",
          "relevance": 0.429
        },
        {
          "concept": "recognition",
          "relevance": 0.426
        },
        {
          "concept": "method",
          "relevance": 0.415
        },
        {
          "concept": "memory",
          "relevance": 0.393
        },
        {
          "concept": "maps",
          "relevance": 0.393
        },
        {
          "concept": "applications",
          "relevance": 0.384
        },
        {
          "concept": "whistles",
          "relevance": 0.378
        },
        {
          "concept": "cost",
          "relevance": 0.37
        },
        {
          "concept": "winners",
          "relevance": 0.369
        },
        {
          "concept": "connection",
          "relevance": 0.367
        },
        {
          "concept": "detector",
          "relevance": 0.36
        },
        {
          "concept": "solution",
          "relevance": 0.349
        },
        {
          "concept": "improvement",
          "relevance": 0.326
        },
        {
          "concept": "results",
          "relevance": 0.312
        },
        {
          "concept": "components",
          "relevance": 0.287
        },
        {
          "concept": "entry",
          "relevance": 0.283
        },
        {
          "concept": "scale",
          "relevance": 0.273
        },
        {
          "concept": "Bell",
          "relevance": 0.249
        }
      ]
    },
    {
      "paperId": "pub.1093828312",
      "doi": "10.1109/iccv.2015.123",
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
      "year": 2015,
      "citationCount": 16259,
      "fieldCitationRatio": 3196.02,
      "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first11reported in Feb. 2015. to surpass the reported human-level performance (5.1%, [26]) on this dataset. reported in Feb. 2015.",
      "reference_ids": [
        "pub.1032233097",
        "pub.1033986161",
        "pub.1033229571",
        "pub.1052031051",
        "pub.1093456265",
        "pub.1093968466",
        "pub.1094727707",
        "pub.1052782426",
        "pub.1095573598",
        "pub.1094291017",
        "pub.1099426737",
        "pub.1094646180",
        "pub.1009767488",
        "pub.1008345178",
        "pub.1014796149",
        "pub.1095689025",
        "pub.1030406568"
      ],
      "concepts_scores": [
        {
          "concept": "human-level performance",
          "relevance": 0.761
        },
        {
          "concept": "neural network",
          "relevance": 0.691
        },
        {
          "concept": "parametric rectified linear unit",
          "relevance": 0.679
        },
        {
          "concept": "robust initialization method",
          "relevance": 0.664
        },
        {
          "concept": "rectified linear unit",
          "relevance": 0.658
        },
        {
          "concept": "ImageNet classification",
          "relevance": 0.616
        },
        {
          "concept": "rectifying nonlinearities",
          "relevance": 0.616
        },
        {
          "concept": "classification datasets",
          "relevance": 0.613
        },
        {
          "concept": "image classification",
          "relevance": 0.607
        },
        {
          "concept": "network architecture",
          "relevance": 0.606
        },
        {
          "concept": "linear unit",
          "relevance": 0.602
        },
        {
          "concept": "overfitting risk",
          "relevance": 0.591
        },
        {
          "concept": "computational cost",
          "relevance": 0.579
        },
        {
          "concept": "initialization method",
          "relevance": 0.578
        },
        {
          "concept": "test error",
          "relevance": 0.562
        },
        {
          "concept": "ImageNet",
          "relevance": 0.558
        },
        {
          "concept": "PReLU",
          "relevance": 0.557
        },
        {
          "concept": "network",
          "relevance": 0.54
        },
        {
          "concept": "classification",
          "relevance": 0.521
        },
        {
          "concept": "dataset",
          "relevance": 0.521
        },
        {
          "concept": "ILSVRC",
          "relevance": 0.483
        },
        {
          "concept": "advanced initiatives",
          "relevance": 0.478
        },
        {
          "concept": "overfitting",
          "relevance": 0.47
        },
        {
          "concept": "performance",
          "relevance": 0.459
        },
        {
          "concept": "Deep",
          "relevance": 0.458
        },
        {
          "concept": "architecture",
          "relevance": 0.447
        },
        {
          "concept": "active units",
          "relevance": 0.413
        },
        {
          "concept": "method",
          "relevance": 0.403
        },
        {
          "concept": "error",
          "relevance": 0.395
        },
        {
          "concept": "images",
          "relevance": 0.393
        },
        {
          "concept": "scratch",
          "relevance": 0.383
        },
        {
          "concept": "model",
          "relevance": 0.381
        },
        {
          "concept": "rectifier",
          "relevance": 0.378
        },
        {
          "concept": "cost",
          "relevance": 0.36
        },
        {
          "concept": "knowledge",
          "relevance": 0.346
        },
        {
          "concept": "parametrization",
          "relevance": 0.319
        },
        {
          "concept": "improvement",
          "relevance": 0.317
        },
        {
          "concept": "nonlinearities",
          "relevance": 0.299
        },
        {
          "concept": "units",
          "relevance": 0.284
        },
        {
          "concept": "test",
          "relevance": 0.247
        },
        {
          "concept": "initiation",
          "relevance": 0.222
        },
        {
          "concept": "risk",
          "relevance": 0.171
        },
        {
          "concept": "activity",
          "relevance": 0.162
        }
      ]
    },
    {
      "paperId": "pub.1149652590",
      "doi": "10.1007/978-3-031-08999-2_22",
      "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images",
      "year": 2022,
      "citationCount": 1385,
      "fieldCitationRatio": 537.56,
      "abstract": "Semantic segmentation of brain tumors is a fundamental medical image analysis task involving multiple MRI imaging modalities that can assist clinicians in diagnosing the patient and successively studying the progression of the malignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs) approaches have become the de facto standard for 3D medical image segmentation. The popular “U-shaped” network architecture has achieved state-of-the-art performance benchmarks on different 2D and 3D semantic segmentation tasks and across various imaging modalities. However, due to the limited kernel size of convolution layers in FCNNs, their performance of modeling long-range information is sub-optimal, and this can lead to deficiencies in the segmentation of tumors with variable sizes. On the other hand, transformer models have demonstrated excellent capabilities in capturing such long-range information in multiple domains, including natural language processing and computer vision. Inspired by the success of vision transformers and their variants, we propose a novel segmentation model termed Swin UNEt TRansformers (Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi-modal input data is projected into a 1D sequence of embedding and used as an input to a hierarchical Swin transformer as the encoder. The swin transformer encoder extracts features at five different resolutions by utilizing shifted windows for computing self-attention and is connected to an FCNN-based decoder at each resolution via skip connections. We have participated in BraTS 2021 segmentation challenge, and our proposed model ranks among the top-performing approaches in the validation phase.Code: https://monai.io/research/swin-unetr.",
      "reference_ids": [
        "pub.1093838265",
        "pub.1111721806",
        "pub.1083403939",
        "pub.1151381613",
        "pub.1142371280",
        "pub.1112944279",
        "pub.1127695040",
        "pub.1013664571",
        "pub.1021243191",
        "pub.1107392317",
        "pub.1072247153",
        "pub.1101085211",
        "pub.1141326803",
        "pub.1084908686",
        "pub.1127700597",
        "pub.1111721788",
        "pub.1093626237",
        "pub.1136659892",
        "pub.1061696449",
        "pub.1121613379",
        "pub.1027450786",
        "pub.1145639403",
        "pub.1145901944",
        "pub.1141301944",
        "pub.1145901979",
        "pub.1001430701",
        "pub.1091487988"
      ],
      "concepts_scores": [
        {
          "concept": "semantic segmentation of brain tumors",
          "relevance": 0.809
        },
        {
          "concept": "segmentation of brain tumors",
          "relevance": 0.786
        },
        {
          "concept": "semantic segmentation",
          "relevance": 0.779
        },
        {
          "concept": "Swin Transformer",
          "relevance": 0.742
        },
        {
          "concept": "kernel sizes of convolutional layers",
          "relevance": 0.723
        },
        {
          "concept": "brain tumor semantic segmentation",
          "relevance": 0.716
        },
        {
          "concept": "medical image analysis tasks",
          "relevance": 0.713
        },
        {
          "concept": "success of vision transformers",
          "relevance": 0.712
        },
        {
          "concept": "state-of-the-art performance benchmarks",
          "relevance": 0.711
        },
        {
          "concept": "multi-modal input data",
          "relevance": 0.706
        },
        {
          "concept": "sequence to sequence prediction problems",
          "relevance": 0.704
        },
        {
          "concept": "computing self-attention",
          "relevance": 0.697
        },
        {
          "concept": "semantic segmentation task",
          "relevance": 0.695
        },
        {
          "concept": "image analysis tasks",
          "relevance": 0.691
        },
        {
          "concept": "medical image segmentation",
          "relevance": 0.691
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.691
        },
        {
          "concept": "encoder extracts features",
          "relevance": 0.689
        },
        {
          "concept": "novel segmentation model",
          "relevance": 0.687
        },
        {
          "concept": "convolutional neural network",
          "relevance": 0.686
        },
        {
          "concept": "natural language processing",
          "relevance": 0.685
        },
        {
          "concept": "sequence prediction problem",
          "relevance": 0.685
        },
        {
          "concept": "sequence of embeddings",
          "relevance": 0.668
        },
        {
          "concept": "multiple MRI imaging modalities",
          "relevance": 0.662
        },
        {
          "concept": "long-range information",
          "relevance": 0.645
        },
        {
          "concept": "convolutional layers",
          "relevance": 0.641
        },
        {
          "concept": "vision transformer",
          "relevance": 0.641
        },
        {
          "concept": "self-attention",
          "relevance": 0.641
        },
        {
          "concept": "computer vision",
          "relevance": 0.64
        },
        {
          "concept": "segmentation task",
          "relevance": 0.637
        },
        {
          "concept": "analysis tasks",
          "relevance": 0.636
        },
        {
          "concept": "network architecture",
          "relevance": 0.636
        },
        {
          "concept": "extract features",
          "relevance": 0.634
        },
        {
          "concept": "image segmentation",
          "relevance": 0.63
        },
        {
          "concept": "segmentation of tumors",
          "relevance": 0.627
        },
        {
          "concept": "neural network",
          "relevance": 0.626
        },
        {
          "concept": "Swin",
          "relevance": 0.625
        },
        {
          "concept": "language processing",
          "relevance": 0.622
        },
        {
          "concept": "shifted windows",
          "relevance": 0.619
        },
        {
          "concept": "segmentation model",
          "relevance": 0.616
        },
        {
          "concept": "Segmentation Challenge",
          "relevance": 0.616
        },
        {
          "concept": "prediction problem",
          "relevance": 0.609
        },
        {
          "concept": "kernel size",
          "relevance": 0.602
        },
        {
          "concept": "performance benchmarks",
          "relevance": 0.595
        },
        {
          "concept": "MRI imaging modalities",
          "relevance": 0.59
        },
        {
          "concept": "input data",
          "relevance": 0.581
        },
        {
          "concept": "FCNN",
          "relevance": 0.579
        },
        {
          "concept": "transformation model",
          "relevance": 0.573
        },
        {
          "concept": "task",
          "relevance": 0.551
        },
        {
          "concept": "multiple domains",
          "relevance": 0.546
        },
        {
          "concept": "network",
          "relevance": 0.539
        },
        {
          "concept": "sub-optimal",
          "relevance": 0.538
        },
        {
          "concept": "model ranking",
          "relevance": 0.538
        },
        {
          "concept": "MRI images",
          "relevance": 0.515
        },
        {
          "concept": "validation phase",
          "relevance": 0.501
        },
        {
          "concept": "decoding",
          "relevance": 0.498
        },
        {
          "concept": "semantics",
          "relevance": 0.489
        },
        {
          "concept": "UNETR",
          "relevance": 0.489
        },
        {
          "concept": "convolution",
          "relevance": 0.485
        },
        {
          "concept": "segments",
          "relevance": 0.485
        },
        {
          "concept": "information",
          "relevance": 0.484
        },
        {
          "concept": "excellent capability",
          "relevance": 0.483
        },
        {
          "concept": "encoding",
          "relevance": 0.482
        },
        {
          "concept": "BraTS",
          "relevance": 0.481
        },
        {
          "concept": "images",
          "relevance": 0.477
        },
        {
          "concept": "embedding",
          "relevance": 0.476
        },
        {
          "concept": "transformation",
          "relevance": 0.471
        },
        {
          "concept": "architecture",
          "relevance": 0.469
        },
        {
          "concept": "computer",
          "relevance": 0.465
        },
        {
          "concept": "benchmarks",
          "relevance": 0.461
        },
        {
          "concept": "kernel",
          "relevance": 0.453
        },
        {
          "concept": "variable size",
          "relevance": 0.44
        },
        {
          "concept": "vision",
          "relevance": 0.427
        },
        {
          "concept": "input",
          "relevance": 0.422
        },
        {
          "concept": "model",
          "relevance": 0.42
        },
        {
          "concept": "capability",
          "relevance": 0.419
        },
        {
          "concept": "performance",
          "relevance": 0.416
        },
        {
          "concept": "rank",
          "relevance": 0.409
        },
        {
          "concept": "features",
          "relevance": 0.4
        },
        {
          "concept": "imaging modalities",
          "relevance": 0.394
        },
        {
          "concept": "domain",
          "relevance": 0.394
        },
        {
          "concept": "entities",
          "relevance": 0.377
        },
        {
          "concept": "connection",
          "relevance": 0.375
        },
        {
          "concept": "resolution",
          "relevance": 0.372
        },
        {
          "concept": "window",
          "relevance": 0.364
        },
        {
          "concept": "challenges",
          "relevance": 0.356
        },
        {
          "concept": "validity",
          "relevance": 0.336
        },
        {
          "concept": "standards",
          "relevance": 0.335
        },
        {
          "concept": "modalities",
          "relevance": 0.331
        },
        {
          "concept": "data",
          "relevance": 0.33
        },
        {
          "concept": "process",
          "relevance": 0.327
        },
        {
          "concept": "brain tumors",
          "relevance": 0.321
        },
        {
          "concept": "success",
          "relevance": 0.318
        },
        {
          "concept": "variants",
          "relevance": 0.306
        },
        {
          "concept": "malignant entities",
          "relevance": 0.29
        },
        {
          "concept": "layer",
          "relevance": 0.285
        },
        {
          "concept": "sequence",
          "relevance": 0.279
        },
        {
          "concept": "tumor",
          "relevance": 0.273
        },
        {
          "concept": "size",
          "relevance": 0.27
        },
        {
          "concept": "patients",
          "relevance": 0.222
        },
        {
          "concept": "MRI",
          "relevance": 0.217
        },
        {
          "concept": "phase",
          "relevance": 0.215
        },
        {
          "concept": "clinicians",
          "relevance": 0.21
        },
        {
          "concept": "progression",
          "relevance": 0.206
        },
        {
          "concept": "deficiency",
          "relevance": 0.203
        },
        {
          "concept": "years",
          "relevance": 0.196
        },
        {
          "concept": "problem",
          "relevance": 0.169
        }
      ]
    },
    {
      "paperId": "pub.1093626237",
      "doi": "10.1109/cvpr.2015.7298965",
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "year": 2015,
      "citationCount": 32637,
      "fieldCitationRatio": 6415.44,
      "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20]), the VGG net [1], and GoogLeNet [2]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IV on 2012), NYVDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",
      "reference_ids": [
        "pub.1061641212",
        "pub.1061743868",
        "pub.1033986161",
        "pub.1003867987",
        "pub.1094083003",
        "pub.1053469442",
        "pub.1095134254",
        "pub.1024540204",
        "pub.1030406568",
        "pub.1095686079",
        "pub.1008345178",
        "pub.1052031051",
        "pub.1016764525",
        "pub.1094727707",
        "pub.1093843992",
        "pub.1094291017",
        "pub.1032233097",
        "pub.1006936750",
        "pub.1015397249",
        "pub.1003201959",
        "pub.1003742061",
        "pub.1093500653"
      ],
      "concepts_scores": [
        {
          "concept": "convolutional network",
          "relevance": 0.779
        },
        {
          "concept": "semantic segmentation",
          "relevance": 0.726
        },
        {
          "concept": "yield hierarchies of features",
          "relevance": 0.7
        },
        {
          "concept": "state-of-the-art segmentation",
          "relevance": 0.698
        },
        {
          "concept": "trained end-to-end",
          "relevance": 0.697
        },
        {
          "concept": "input of arbitrary size",
          "relevance": 0.696
        },
        {
          "concept": "correspondingly-sized output",
          "relevance": 0.682
        },
        {
          "concept": "dense prediction tasks",
          "relevance": 0.682
        },
        {
          "concept": "hierarchies of features",
          "relevance": 0.677
        },
        {
          "concept": "pixels-to-pixels",
          "relevance": 0.676
        },
        {
          "concept": "state-of-the-art",
          "relevance": 0.676
        },
        {
          "concept": "fully convolutional network",
          "relevance": 0.671
        },
        {
          "concept": "end-to-end",
          "relevance": 0.663
        },
        {
          "concept": "PASCAL VOC",
          "relevance": 0.631
        },
        {
          "concept": "learned representations",
          "relevance": 0.629
        },
        {
          "concept": "appearance information",
          "relevance": 0.629
        },
        {
          "concept": "classification network",
          "relevance": 0.626
        },
        {
          "concept": "VGG-Net",
          "relevance": 0.626
        },
        {
          "concept": "SIFT flow",
          "relevance": 0.626
        },
        {
          "concept": "segmentation task",
          "relevance": 0.623
        },
        {
          "concept": "semantic information",
          "relevance": 0.619
        },
        {
          "concept": "prediction task",
          "relevance": 0.615
        },
        {
          "concept": "efficient inference",
          "relevance": 0.614
        },
        {
          "concept": "visual model",
          "relevance": 0.593
        },
        {
          "concept": "network",
          "relevance": 0.569
        },
        {
          "concept": "arbitrary size",
          "relevance": 0.562
        },
        {
          "concept": "task",
          "relevance": 0.513
        },
        {
          "concept": "coarse layer",
          "relevance": 0.495
        },
        {
          "concept": "VGG",
          "relevance": 0.491
        },
        {
          "concept": "inference",
          "relevance": 0.487
        },
        {
          "concept": "semantics",
          "relevance": 0.478
        },
        {
          "concept": "segments",
          "relevance": 0.474
        },
        {
          "concept": "information",
          "relevance": 0.474
        },
        {
          "concept": "SIFT",
          "relevance": 0.473
        },
        {
          "concept": "architecture",
          "relevance": 0.459
        },
        {
          "concept": "classification",
          "relevance": 0.439
        },
        {
          "concept": "learning",
          "relevance": 0.436
        },
        {
          "concept": "representation",
          "relevance": 0.432
        },
        {
          "concept": "nets",
          "relevance": 0.413
        },
        {
          "concept": "input",
          "relevance": 0.412
        },
        {
          "concept": "fine layer",
          "relevance": 0.404
        },
        {
          "concept": "images",
          "relevance": 0.403
        },
        {
          "concept": "Fully",
          "relevance": 0.396
        },
        {
          "concept": "model",
          "relevance": 0.391
        },
        {
          "concept": "features",
          "relevance": 0.391
        },
        {
          "concept": "output",
          "relevance": 0.387
        },
        {
          "concept": "connection",
          "relevance": 0.367
        },
        {
          "concept": "space",
          "relevance": 0.359
        },
        {
          "concept": "layer",
          "relevance": 0.323
        },
        {
          "concept": "VOC",
          "relevance": 0.319
        },
        {
          "concept": "spatially",
          "relevance": 0.318
        },
        {
          "concept": "size",
          "relevance": 0.264
        },
        {
          "concept": "appearance",
          "relevance": 0.253
        },
        {
          "concept": "flow",
          "relevance": 0.208
        }
      ]
    },
    {
      "paperId": "pub.1094291017",
      "doi": "10.1109/cvpr.2015.7298594",
      "title": "Going Deeper with Convolutions",
      "year": 2015,
      "citationCount": 41698,
      "fieldCitationRatio": 8196.56,
      "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
      "reference_ids": [
        "pub.1061743347",
        "pub.1061179979",
        "pub.1062855239",
        "pub.1032233097",
        "pub.1094869203",
        "pub.1008345178",
        "pub.1025436137",
        "pub.1093519792",
        "pub.1095738839",
        "pub.1094727707",
        "pub.1062844374"
      ],
      "concepts_scores": [
        {
          "concept": "deep convolutional neural network architecture",
          "relevance": 0.646
        },
        {
          "concept": "convolutional neural network architecture",
          "relevance": 0.642
        },
        {
          "concept": "neural network architecture",
          "relevance": 0.625
        },
        {
          "concept": "context of classification",
          "relevance": 0.619
        },
        {
          "concept": "Recognition Challenge",
          "relevance": 0.582
        },
        {
          "concept": "deep networks",
          "relevance": 0.582
        },
        {
          "concept": "network architecture",
          "relevance": 0.576
        },
        {
          "concept": "computational resources",
          "relevance": 0.567
        },
        {
          "concept": "architectural decisions",
          "relevance": 0.564
        },
        {
          "concept": "Hebbian principle",
          "relevance": 0.551
        },
        {
          "concept": "multi-scale processes",
          "relevance": 0.519
        },
        {
          "concept": "network",
          "relevance": 0.514
        },
        {
          "concept": "architecture",
          "relevance": 0.492
        },
        {
          "concept": "classification",
          "relevance": 0.471
        },
        {
          "concept": "convolution",
          "relevance": 0.44
        },
        {
          "concept": "improve utilization",
          "relevance": 0.433
        },
        {
          "concept": "detection",
          "relevance": 0.42
        },
        {
          "concept": "quality",
          "relevance": 0.373
        },
        {
          "concept": "intuition",
          "relevance": 0.368
        },
        {
          "concept": "decision",
          "relevance": 0.362
        },
        {
          "concept": "resources",
          "relevance": 0.354
        },
        {
          "concept": "submission",
          "relevance": 0.354
        },
        {
          "concept": "design",
          "relevance": 0.331
        },
        {
          "concept": "challenges",
          "relevance": 0.323
        },
        {
          "concept": "context",
          "relevance": 0.311
        },
        {
          "concept": "utilization",
          "relevance": 0.31
        },
        {
          "concept": "principles",
          "relevance": 0.298
        },
        {
          "concept": "process",
          "relevance": 0.297
        },
        {
          "concept": "layer",
          "relevance": 0.259
        },
        {
          "concept": "inception",
          "relevance": 0.244
        },
        {
          "concept": "depth",
          "relevance": 0.23
        },
        {
          "concept": "width",
          "relevance": 0.199
        },
        {
          "concept": "constant",
          "relevance": 0.177
        }
      ]
    },
    {
      "paperId": "pub.1094727707",
      "doi": "10.1109/cvpr.2014.81",
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "year": 2014,
      "citationCount": 27463,
      "fieldCitationRatio": 6371.85,
      "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
      "reference_ids": [
        "pub.1093500653",
        "pub.1095689025",
        "pub.1094310450",
        "pub.1093204525",
        "pub.1095432752",
        "pub.1093883984",
        "pub.1011653915",
        "pub.1095420134",
        "pub.1002182143",
        "pub.1091744995",
        "pub.1093948019",
        "pub.1061743745",
        "pub.1003742061",
        "pub.1014968475",
        "pub.1052687286",
        "pub.1033900312",
        "pub.1016635886",
        "pub.1061744113",
        "pub.1061744374",
        "pub.1094492451",
        "pub.1056860333",
        "pub.1091605025",
        "pub.1061156724",
        "pub.1061179979",
        "pub.1093771760",
        "pub.1014796149",
        "pub.1027572846",
        "pub.1093997066",
        "pub.1008345178",
        "pub.1093987027",
        "pub.1094136360"
      ],
      "concepts_scores": [
        {
          "concept": "convolutional neural network",
          "relevance": 0.831
        },
        {
          "concept": "Mean Average Precision",
          "relevance": 0.821
        },
        {
          "concept": "region proposals",
          "relevance": 0.727
        },
        {
          "concept": "multiple low-level image features",
          "relevance": 0.707
        },
        {
          "concept": "high-capacity convolutional neural networks",
          "relevance": 0.707
        },
        {
          "concept": "domain-specific fine-tuning",
          "relevance": 0.701
        },
        {
          "concept": "improves mean average precision",
          "relevance": 0.698
        },
        {
          "concept": "low-level image features",
          "relevance": 0.697
        },
        {
          "concept": "convolutional neural network features",
          "relevance": 0.697
        },
        {
          "concept": "bottom-up region proposals",
          "relevance": 0.694
        },
        {
          "concept": "scalable detection algorithm",
          "relevance": 0.682
        },
        {
          "concept": "PASCAL VOC dataset",
          "relevance": 0.681
        },
        {
          "concept": "supervised pre-training",
          "relevance": 0.68
        },
        {
          "concept": "accurate object detection",
          "relevance": 0.679
        },
        {
          "concept": "object detection performance",
          "relevance": 0.679
        },
        {
          "concept": "labeled training data",
          "relevance": 0.677
        },
        {
          "concept": "significant performance boost",
          "relevance": 0.666
        },
        {
          "concept": "VOC dataset",
          "relevance": 0.631
        },
        {
          "concept": "auxiliary task",
          "relevance": 0.628
        },
        {
          "concept": "object detection",
          "relevance": 0.627
        },
        {
          "concept": "semantic segmentation",
          "relevance": 0.627
        },
        {
          "concept": "segment objects",
          "relevance": 0.622
        },
        {
          "concept": "R-CNN",
          "relevance": 0.622
        },
        {
          "concept": "feature hierarchy",
          "relevance": 0.62
        },
        {
          "concept": "average precision",
          "relevance": 0.618
        },
        {
          "concept": "training data",
          "relevance": 0.618
        },
        {
          "concept": "neural network",
          "relevance": 0.613
        },
        {
          "concept": "detection algorithm",
          "relevance": 0.612
        },
        {
          "concept": "pre-training",
          "relevance": 0.609
        },
        {
          "concept": "performance boost",
          "relevance": 0.607
        },
        {
          "concept": "detection performance",
          "relevance": 0.605
        },
        {
          "concept": "ensemble system",
          "relevance": 0.586
        },
        {
          "concept": "fine-tuning",
          "relevance": 0.58
        },
        {
          "concept": "image features",
          "relevance": 0.57
        },
        {
          "concept": "algorithm",
          "relevance": 0.464
        },
        {
          "concept": "dataset",
          "relevance": 0.462
        },
        {
          "concept": "proposal",
          "relevance": 0.462
        },
        {
          "concept": "network",
          "relevance": 0.456
        },
        {
          "concept": "features",
          "relevance": 0.453
        },
        {
          "concept": "segments",
          "relevance": 0.444
        },
        {
          "concept": "task",
          "relevance": 0.443
        },
        {
          "concept": "objective",
          "relevance": 0.432
        },
        {
          "concept": "method",
          "relevance": 0.414
        },
        {
          "concept": "performance",
          "relevance": 0.407
        },
        {
          "concept": "hierarchy",
          "relevance": 0.399
        },
        {
          "concept": "detection",
          "relevance": 0.391
        },
        {
          "concept": "precision",
          "relevance": 0.386
        },
        {
          "concept": "system",
          "relevance": 0.37
        },
        {
          "concept": "boost",
          "relevance": 0.351
        },
        {
          "concept": "data",
          "relevance": 0.323
        },
        {
          "concept": "VOC",
          "relevance": 0.319
        },
        {
          "concept": "region",
          "relevance": 0.229
        },
        {
          "concept": "years",
          "relevance": 0.192
        }
      ]
    }
  ],
  "evolution_links": [
    {
      "source": "pub.1157026832",
      "target": "pub.1151380774",
      "source_title": "Transformers in medical imaging: A survey",
      "target_title": "A ConvNet for the 2020s"
    },
    {
      "source": "pub.1151380774",
      "target": "pub.1145901979",
      "source_title": "A ConvNet for the 2020s",
      "target_title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
    },
    {
      "source": "pub.1145901979",
      "target": "pub.1132270339",
      "source_title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "target_title": "End-to-End Object Detection with Transformers"
    },
    {
      "source": "pub.1145901979",
      "target": "pub.1145901054",
      "source_title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "target_title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"
    },
    {
      "source": "pub.1151380774",
      "target": "pub.1110720947",
      "source_title": "A ConvNet for the 2020s",
      "target_title": "Non-local Neural Networks"
    },
    {
      "source": "pub.1110720947",
      "target": "pub.1095852454",
      "source_title": "Non-local Neural Networks",
      "target_title": "Feature Pyramid Networks for Object Detection"
    },
    {
      "source": "pub.1110720947",
      "target": "pub.1093828312",
      "source_title": "Non-local Neural Networks",
      "target_title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
    },
    {
      "source": "pub.1157026832",
      "target": "pub.1149652590",
      "source_title": "Transformers in medical imaging: A survey",
      "target_title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images"
    },
    {
      "source": "pub.1149652590",
      "target": "pub.1093626237",
      "source_title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images",
      "target_title": "Fully Convolutional Networks for Semantic Segmentation"
    },
    {
      "source": "pub.1093626237",
      "target": "pub.1094291017",
      "source_title": "Fully Convolutional Networks for Semantic Segmentation",
      "target_title": "Going Deeper with Convolutions"
    },
    {
      "source": "pub.1093626237",
      "target": "pub.1094727707",
      "source_title": "Fully Convolutional Networks for Semantic Segmentation",
      "target_title": "Rich feature hierarchies for accurate object detection and semantic segmentation"
    }
  ]
}