{
  "before_idea": {
    "title": "Bridging Gaze Tracking and LLM Dialogue in Collaborative Social Robotics for Hypothesis Exploration",
    "Problem_Statement": "Limited integration of nonverbal cues like gaze tracking with LLM-based dialogue for enriched human-robot collaboration in psychological research.",
    "Motivation": "Fills external gap between software implementation and practical robotics using human-centered computing advances, enriching interaction fidelity for hypothesis testing (Opportunity 3).",
    "Proposed_Method": "Combine state-of-the-art gaze tracking hardware/software with LLM-driven dialogue in a collaborative robot platform, allowing the robot to interpret gaze patterns as contextual cues adjusting conversational strategies and experimental task presentations dynamically.",
    "Step_by_Step_Experiment_Plan": "1. Develop calibration and gaze signal processing pipelines. 2. Train LLM dialogue models conditioned on gaze-derived context embeddings. 3. Deploy in lab tasks requiring mutual attention shifts and reasoning. 4. Evaluate interaction naturalness, hypothesis generation rates, and user satisfaction.",
    "Test_Case_Examples": "Input: Participant gazes away from robot during question about emotional state. Output: Robot adapts dialogue to re-engage participant or infer discomfort, refining questioning accordingly.",
    "Fallback_Plan": "If gaze tracking signal is noisy, utilize alternative physiological signals (e.g., pupil dilation) or default to conversational cues. If LLMs cannot integrate gaze context, apply reinforcement learning for behavior adaptation."
  },
  "after_idea": {
    "ideas": [
      {
        "title": "Bridging Gaze Tracking and LLM Dialogue in Collaborative Social Robotics for Hypothesis Exploration",
        "Problem_Statement": "Current human-robot collaboration frameworks inadequately integrate real-time nonverbal cues such as gaze within LLM-driven dialogue systems, limiting the naturalness and contextual responsiveness needed for effective psychological hypothesis exploration. The challenge lies in reliably extracting, representing, and fusing gaze-derived context embeddings with dialogue to create human-friendly, adaptive robot conversational behavior in multi-party and dynamic attention-shift environments.",
        "Motivation": "While multimodal human-robot interaction research has explored individual modalities like gaze or dialogue, the convergence of fine-grained gaze tracking and Large Language Models (LLMs) for dynamic conversational adaptation remains underdeveloped. Our novel focus is on engineering technically sound and interpretable multimodal fusion mechanisms that tightly integrate gaze context into LLM dialogue generation, surpassing existing approaches by enabling real-time, attention-aware conversational policies. This advances human-computer interaction theory and human-robot interaction research by bridging perception and language modules within socially engaging, collaborative robotics platforms, fostering richer user self-disclosure and hypothesis generation—addressing a recognized gap with competitive innovation.",
        "Proposed_Method": "We propose a modular architecture combining state-of-the-art gaze tracking hardware with a multi-component software pipeline that extracts gaze features, computes gaze-derived context embeddings, and integrates these embeddings into LLM-based dialogue generation via multi-level fusion. Specifically, gaze signals (fixations, saccades, gaze direction) are preprocessed into temporal feature vectors and encoded by a transformer-based gaze encoder trained on human activity recognition and mutual attention datasets. These embeddings interface with the LLM using two synergistic fusion strategies: (1) embedding augmentation through prompt conditioning with a gaze context token summarizing current attentional state, and (2) an adaptive attention mechanism inside a custom LLM fine-tuning step, enabling dynamic weighting of gaze-informed context during token generation. Dialogue policy adaptation is further refined using reinforcement learning, where gaze-aware dialogue outcomes optimize engagement and user satisfaction metrics. This approach explicitly fuses multimodal signals at both input-prompt and deep model attention levels, differentiating it from existing integration techniques. The method is designed for scalability to multi-party human-robot interaction and supports real-time responsive behaviors, promoting human-friendly robot interactions that encourage transparent self-disclosure within controlled laboratory tasks and beyond.",
        "Step_by_Step_Experiment_Plan": "1. Develop and validate a gaze signal processing pipeline: collect gaze data from a diverse participant cohort (n=30, mixed demographics) performing validated interactive lab tasks involving mutual attention shifts (e.g., emotion recognition, problem-solving dialogues). Process raw gaze input into fixation and saccade features and train a transformer-based gaze encoder.\n2. Fine-tune the LLM (e.g., GPT architecture) integrating gaze-driven context tokens and adaptive attention layers through supervised learning on multi-modal annotated dialogue datasets.\n3. Implement a reinforcement learning loop where gaze-informed dialogue policies optimize predefined quantitative metrics.\n4. Conduct controlled human-robot interaction experiments with participants performing hypothesis exploration tasks requiring verbal and nonverbal engagement.\n5. Evaluate using quantitative metrics: interaction naturalness (measured via engagement time, gaze synchrony indices, and response latency), hypothesis generation rates (number of novel hypotheses generated per session), and user satisfaction using standardized questionnaires (e.g., SUS, NASA-TLX adapted for HRI).\n6. Address system challenges such as gaze noise and latency by benchmarking fallback mechanisms (physiological signals and solely dialogue-context RL policies), analyzing performance trade-offs.\n7. Perform statistical analysis to validate robustness, generalizability, and efficacy across demographics and task types.",
        "Test_Case_Examples": "Input: Participant averts gaze while discussing emotional discomfort—gaze encoder generates context embedding indicating potential disengagement.\nOutput: The LLM-enhanced dialogue system dynamically adjusts conversation, prompting empathetic or clarifying questions, re-engaging participant, or inferring discomfort to refine questioning strategy.\n\nInput: During multi-party task, gaze synchrony between robot and human shifts rapidly.\nOutput: Robot adapts dialogue timing and content, leveraging gaze-contextual cues to maintain engagement and support transparent user self-disclosure, fostering collaborative hypothesis generation.",
        "Fallback_Plan": "If gaze tracking signal quality degrades due to hardware or environmental noise, the system automatically switches to alternative physiological signals such as pupil dilation rates or blink frequency for attentional state estimation, recalibrating context embeddings accordingly. If real-time integration of gaze embeddings within the LLM proves infeasible, we will implement a reinforcement learning-based dialogue policy module that uses gaze cues as reward signals without explicit embedding fusion. This fallback maintains adaptive behavior through behavioral policy learning rather than direct language model conditioning, ensuring continuity of human-friendly interaction and hypothesis exploration capabilities."
      }
    ]
  },
  "feedback_results": {
    "keywords_query": [
      "Gaze Tracking",
      "LLM Dialogue",
      "Collaborative Social Robotics",
      "Hypothesis Exploration",
      "Human-Robot Collaboration",
      "Nonverbal Cues"
    ],
    "direct_cooccurrence_count": 396,
    "min_pmi_score_value": 4.374050036293934,
    "avg_pmi_score_value": 6.094879148580642,
    "novelty": "NOV-COMPETITIVE",
    "future_suggestions_categories": [
      "46 Information and Computing Sciences",
      "4608 Human-Centred Computing",
      "4602 Artificial Intelligence"
    ],
    "future_suggestions_concepts": [
      "human-robot interaction",
      "human-computer interaction",
      "Human-Computer",
      "field of human-robot interaction",
      "users' self-disclosure",
      "self-disclosure",
      "human-friendly robot",
      "human-friendly",
      "human-computer interaction theory",
      "human-robot interaction research",
      "human-robot interaction experiments",
      "multi-party human-robot interaction",
      "human activity recognition",
      "online user study",
      "management of information",
      "human interface"
    ],
    "internal_review": {
      "critiques": [
        {
          "feedback_code": "SOU-MECHANISM",
          "feedback_content": "While the idea to integrate gaze tracking signals into LLM-driven dialogue for adaptive robot interactions is promising, the mechanism for how gaze-derived context embeddings will be effectively extracted, represented, and integrated into the LLM's dialogue generation process is not sufficiently detailed. Clarify the technical approach to fuse multimodal inputs (gaze data plus dialogue context), including any model architectures or interfaces bridging perception and language components. This will strengthen the proposal’s soundness and help evaluate the practical innovation behind the combined system design, especially given the competitive research landscape in human-robot multimodal interaction. Consider specifying whether gaze embeddings modify LLM prompt conditioning, attention mechanisms, or act as inputs for behavior policy learning (e.g., reinforcement learning). This detail is needed to validate the core technical novelty and feasibility assumptions embedded in the Proposed_Method section."
        },
        {
          "feedback_code": "FEA-EXPERIMENT",
          "feedback_content": "The Step_by_Step_Experiment_Plan outlines a logical progression but lacks concrete details regarding the complexity and controllability of experimental tasks, as well as metrics for evaluation beyond qualitative descriptors. To enhance feasibility and rigor, explicitly define the lab task scenarios and mutual attention shifts expected, the datasets or participant demographics involved, and quantitative measures for interaction naturalness (e.g., engagement time, gaze synchrony), hypothesis generation rates, and user satisfaction (e.g., standardized surveys). Also, discuss potential challenges like gaze tracking noise, real-time system latency, and how fallback mechanisms will be quantitatively evaluated. Without these clarifications, it remains uncertain if the experiments can reliably validate the proposed system within reasonable resource constraints."
        }
      ]
    }
  }
}